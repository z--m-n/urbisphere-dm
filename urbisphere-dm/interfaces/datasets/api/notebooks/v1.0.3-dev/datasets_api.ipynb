{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ffd984-c556-4236-a5a8-c0b4d3cd0afe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import glob\n",
    "import itertools\n",
    "import json\n",
    "import os\n",
    "import pprint as pp\n",
    "import shutil\n",
    "import sys\n",
    "import urllib\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path as Path\n",
    "from typing import Sequence\n",
    "\n",
    "import netCDF4 as nc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import simplejson\n",
    "import xarray as xr\n",
    "import zarr\n",
    "from mergedeep import merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd02713-fe30-4e86-b8a3-32ef52a523bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ[\"USE_PYGEOS\"] = \"0\"\n",
    "import aiohttp\n",
    "import fsspec\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c417251-d96b-4731-9e6a-70824a6c6fbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from fastapi import FastAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1364e6d4-70e4-48be-92a8-c83c9e438b5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from fastapi import APIRouter, Depends, HTTPException\n",
    "from fastapi import Path as apiPath\n",
    "from fastapi.responses import HTMLResponse, JSONResponse\n",
    "from fastapi.staticfiles import StaticFiles\n",
    "from requests import HTTPError\n",
    "from xpublish import Dependencies, Plugin, Rest, hookimpl\n",
    "\n",
    "# from xpublish.utils.api import JSONResponse, HTMLResponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed38f77-3822-478b-87a4-42fe85f68138",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# time to live cache, wrapper function\n",
    "import cachetools.func\n",
    "\n",
    "\n",
    "def list_to_tuple(function):\n",
    "    def wrapper(*args):\n",
    "        args = [tuple(x) if isinstance(x, list) else x for x in args]\n",
    "        result = function(*args)\n",
    "        result = tuple(result) if isinstance(result, list) else result\n",
    "        return result\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "def dataset_to_tuple(function):\n",
    "    def wrapper(*args):\n",
    "        args = tuple([*args])\n",
    "        result = function(*args)\n",
    "        result = tuple(result) if isinstance(result, list) else result\n",
    "        return result\n",
    "\n",
    "    return wrapper    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2eca6c-0e3c-4d61-9ee2-27246360c87d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# $ jupyter nbconvert --to script [YOUR_NOTEBOOK].ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e393b15-6654-4351-b41b-9c3a25aefd69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rebase_path(path_base=\"urbisphere-dm\", path_root=None):\n",
    "    \"\"\"return abs path of a higher level directory\"\"\"\n",
    "    from pathlib import Path\n",
    "\n",
    "    path_root = Path(\"__file__\").parent.resolve() if not path_root else path_root\n",
    "    path_parts = lambda p: p[0 : (p.index(path_base) + 1 if path_base in p else len(p))]\n",
    "    return str(Path(*[n for n in path_parts(Path(path_root).parts)]))\n",
    "\n",
    "\n",
    "sys.path.append(os.path.join(rebase_path(), \"interfaces/metadb/notebooks/\"))\n",
    "sys.path.append(os.path.join(rebase_path(), \"interfaces/metadb/src/\"))\n",
    "\n",
    "from ipynb.fs.defs.metadb_publications import metadb_publication_query\n",
    "from ipynb.fs.full.metadb_query import metadb_query as metadb_query_subset_table\n",
    "from metadb_attributes import metadb_combine_globalattrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb719272-e821-45c6-91d8-46d143e7fe64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class StopExecution(Exception):\n",
    "    def _render_traceback_(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "def get_dictlist_permutations(input_subset):\n",
    "    \"\"\"Expand a dict of (strs|lists) into all possible permutations.\"\"\"\n",
    "    import itertools\n",
    "\n",
    "    keys, values = zip(*input_subset.items())\n",
    "    values = [v if isinstance(v, list) else [v] for v in values]\n",
    "    permutations_dicts = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "    return permutations_dicts\n",
    "\n",
    "\n",
    "def get_dictlist_flatten(input_subset, joinstr=\"+\"):\n",
    "    \"\"\"Flatten dict of (strs|lists) into dict of (strs).\"\"\"\n",
    "    import itertools\n",
    "\n",
    "    keys, values = zip(*input_subset.items())\n",
    "    values = [joinstr.join(v) if isinstance(v, list) else (v) for v in values]\n",
    "    return dict(zip(keys, values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e081de58-4594-4dd0-aa35-ee2798b4a4b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_creation_time(d=datetime.utcnow()):\n",
    "    d_str = d.replace(microsecond=0, tzinfo=timezone.utc).isoformat()\n",
    "    return d_str\n",
    "\n",
    "\n",
    "def get_gattrs(gattrs, sep=\";\\n\"):\n",
    "    ga = gattrs[0]\n",
    "    gs = {}\n",
    "    if len(ga) > 1:\n",
    "        for g in gattrs[1:]:\n",
    "            bo = False\n",
    "            for k in gs.keys():\n",
    "                if k in g and not bo:\n",
    "                    bo = any(re.compile(m).match(gs[k]) for m in g[k])\n",
    "                    if bo:\n",
    "                        for k, v in g.items():\n",
    "                            if k in ga:\n",
    "                                ga[k] = v\n",
    "\n",
    "    ct = get_creation_time()\n",
    "    gd = {\n",
    "        \"version_id\": version[\"id\"],\n",
    "        \"version_time\": version[\"time\"],\n",
    "        \"version_date\": version[\"time\"],\n",
    "        \"creation_time\": ct,\n",
    "        \"creation_date\": ct[0:10],\n",
    "    }\n",
    "    ga = {k: sep.join(v) if isinstance(v, list) else v for k, v in ga.items()}\n",
    "    ga = {k: v.format(**gd) for k, v in ga.items()}\n",
    "    return ga\n",
    "\n",
    "\n",
    "def get_gattrs_pub(gattrs):\n",
    "    publication_gattrs = {}\n",
    "    for ga in gattrs:\n",
    "        if \"production_profile\" in ga:\n",
    "            pga = metadb_publication_query(\n",
    "                ga[\"production_profile\"],\n",
    "                publication_name=\"datasets_api\",\n",
    "            )\n",
    "            pga = {k: v for k, v in pga.items() if v != \"\"}\n",
    "            publication_gattrs = {**publication_gattrs, **pga}\n",
    "    return publication_gattrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdfb851-38d2-4727-a136-3bcd811a0da0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_config(ioconf_file, ioconf_name=None, version_dict=None):\n",
    "    \"\"\"Read configuration file and extract dict that matches version['id'].\"\"\"\n",
    "    import toml\n",
    "    from mergedeep import merge\n",
    "\n",
    "    def read_config(ioconfig_file):\n",
    "        if os.path.exists(ioconfig_file):\n",
    "            with open(ioconfig_file) as f:\n",
    "                ioconf = toml.load(f)\n",
    "            return ioconf\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    # read TOML config file\n",
    "    ioconf = read_config(ioconfig_file)\n",
    "\n",
    "    if isinstance(version_dict, dict):\n",
    "        # lookup of settings\n",
    "        if ioconfig_name:\n",
    "            group = ioconfig_name\n",
    "        else:\n",
    "            group = Path(ioconfig_file).stem\n",
    "\n",
    "        if group in ioconf:\n",
    "            conf_list = [\n",
    "                d\n",
    "                for d in ioconf[group]\n",
    "                if version_dict[\"id\"].startswith(\n",
    "                    d[\"version\"][\"id\"] if \"version\" in d else \"v\"\n",
    "                )\n",
    "            ]\n",
    "            config = merge(*conf_list)\n",
    "        else:\n",
    "            config = {}\n",
    "\n",
    "        return config\n",
    "    else:\n",
    "        return ioconf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392cc759-db80-4976-8e30-cef57da34f21",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6165e552-5cd8-441c-b4dd-d4e057b1721c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_query_range(query_from, query_period, query_to=None):\n",
    "    if not query_to and query_period:\n",
    "        query_to = pd.to_datetime(query_from) + pd.tseries.frequencies.to_offset(\n",
    "            query_period\n",
    "        )\n",
    "        if any(query_period.endswith(n) for n in [\"M\", \"Y\"]):\n",
    "            query_to = query_to + pd.tseries.frequencies.to_offset(\"1D\")\n",
    "    query_range = pd.to_datetime([query_from, query_to])\n",
    "    return query_range\n",
    "\n",
    "\n",
    "def get_time_bounds(query_range):\n",
    "    tr = query_range.strftime(\"%Y%m%dT%H%M%S%z\").tolist()\n",
    "    res = tr[0] if tr[0] == tr[1] else \"{}_{}\".format(*tr)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0081b669-5959-4350-9869-fb0520348037",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def datasets_fsspec_args(filepath):\n",
    "    \"\"\"generate a compounded url for fsspec, if relevant, plus arguments for xarray\"\"\"\n",
    "    url = \"\"\n",
    "    args = {}\n",
    "\n",
    "    p = Path(filepath).resolve()\n",
    "\n",
    "    # is the path an archive?\n",
    "    if p.suffixes[-1] == \".zip\" and len(p.suffixes) > 1:\n",
    "        url += f\"zip::\"\n",
    "\n",
    "        # define the suffix, in case of a trailing archive suffix\n",
    "        sufx = p.suffixes[-2]\n",
    "    else:\n",
    "        sufx = p.suffixes[-1]\n",
    "\n",
    "    if sufx == \".zarr\":\n",
    "        args[\"engine\"] = \"zarr\"\n",
    "    elif sufx == \".nc\":\n",
    "        args[\"engine\"] = \"netcdf4\"\n",
    "\n",
    "    # add the file path to the url\n",
    "    # url += p.as_uri()\n",
    "    if url != \"\":\n",
    "        url += \"file://\"\n",
    "\n",
    "    url += f\"{p}\"  # local files\n",
    "\n",
    "    return url, args\n",
    "\n",
    "\n",
    "def datasets_query_time(filepath):\n",
    "    \"\"\"extract time stamp information from the attributes of datasets\"\"\"\n",
    "\n",
    "    store, store_args = datasets_fsspec_args(filepath)\n",
    "    query_time = None\n",
    "    query_res = {}\n",
    "    a_list = [\"production_time\", \"creation_time\"]\n",
    "\n",
    "    root_group = (\n",
    "        input_subset[\"campaign_location\"]\n",
    "        if \"campaign_location\" in input_subset\n",
    "        else \"FR\"\n",
    "    )\n",
    "\n",
    "    if not Path(filepath).exists():\n",
    "        return None, {}\n",
    "\n",
    "    for group in [None, root_group]:\n",
    "        if not query_res:\n",
    "            try:\n",
    "                if store.endswith(\"nc\"):\n",
    "                    with nc.Dataset(store, mode=\"r\", group=group) as ds:\n",
    "                        query_res = {\n",
    "                            a: ds.getncattr(a) for a in ds.ncattrs() if a in a_list\n",
    "                        }\n",
    "                elif store.endswith(\"zarr\") or store.endswith(\"zarr/\"):\n",
    "                    with zarr.open(store, mode=\"r\", path=group) as ds:\n",
    "                        query_res = {\n",
    "                            a: ds.attrs[a] for a in ds.attrs.keys() if a in a_list\n",
    "                        }\n",
    "                elif store.startswith(\"zip::\"):\n",
    "                    with xr.open_dataset(\n",
    "                        store, **store_args, mode=\"r\", group=group\n",
    "                    ) as ds:\n",
    "                        query_res = {\n",
    "                            a: ds.attrs[a] for a in ds.attrs.keys() if a in a_list\n",
    "                        }\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    if \"production_time\" in query_res:\n",
    "        query_time = query_res[\"production_time\"].split(\"/\")\n",
    "        query_time = [pd.to_datetime(qt, format=\"ISO8601\") for qt in query_time]\n",
    "    elif \"creation_time\" in query_res:\n",
    "        query_time = [query_res[\"creation_time\"][1:21]]\n",
    "        query_time = [pd.to_datetime(qt, format=\"ISO8601\") for qt in query_time]\n",
    "\n",
    "    query_mtime = [\n",
    "        pd.Timestamp(\n",
    "            datetime.fromtimestamp(Path(filepath).stat().st_mtime).replace(\n",
    "                tzinfo=timezone.utc\n",
    "            )\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    return query_time, {\n",
    "        \"st_mtime\": query_mtime,\n",
    "        \"attrs\": query_res,\n",
    "        \"fsspec_args\": (store, store_args),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102d0ce6-a4c8-4644-9ad1-dd5d03f4c222",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def datastore_cache(input_file_list, output_path):\n",
    "    def datastore_to_zarr(fn, zfn=None, replace=True, root_group=\"FR\"):\n",
    "        p = Path(fn)\n",
    "        t = p.with_suffix(\".zarr\") if not zfn else Path(zfn)\n",
    "\n",
    "        if t.exists() and t.is_dir() and replace:\n",
    "            logging.warning(\"Removing directory `%s`\", str(t))\n",
    "            shutil.rmtree(t)\n",
    "\n",
    "        with xr.open_dataset(p) as ds:\n",
    "            groups = ds[\"station_id\"].values.tolist()\n",
    "            ds.close()\n",
    "\n",
    "        if not t.is_dir() or replace:\n",
    "            with xr.open_dataset(p) as ds:\n",
    "                ds.to_zarr(t, mode=\"w\", group=root_group, consolidated=True)\n",
    "                ds.close()\n",
    "\n",
    "            for group in groups:\n",
    "                with xr.open_dataset(p, group=group) as ds:\n",
    "                    ds.to_zarr(t, mode=\"a\", group=group, consolidated=True)\n",
    "                    ds.close()\n",
    "\n",
    "        return t, groups\n",
    "\n",
    "    # create output_path\n",
    "    Path(output_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    root_group = (\n",
    "        input_subset[\"campaign_location\"]\n",
    "        if \"campaign_location\" in input_subset\n",
    "        else \"FR\"\n",
    "    )\n",
    "\n",
    "    # to zarr\n",
    "    output_files = []\n",
    "    for input_file_dict in input_file_list:\n",
    "        input_file = Path(input_file_dict[\"file\"])\n",
    "        groups = []\n",
    "\n",
    "        if input_file.exists():\n",
    "            logging.info(\"input file: `%s`\", input_file)\n",
    "            if input_file.suffixes[-1] == \".zip\" and input_file.suffixes[-2] == \".zarr\":\n",
    "                output_zarr = Path(output_path) / input_file.with_suffix(\"\").name\n",
    "                output_zarr_tmp = output_zarr.with_suffix(output_zarr.suffix + \".tmp\")\n",
    "                if datasets_update_check(input_file, output_zarr) in [\n",
    "                    None,\n",
    "                    \"not_equal\",\n",
    "                ]:\n",
    "                    logging.info(\"unpacking file: `%s`\", str(input_file))\n",
    "                    shutil.unpack_archive(input_file, output_zarr_tmp)\n",
    "                    shutil.rmtree(output_zarr, ignore_errors=True)\n",
    "                    output_zarr_tmp.rename(output_zarr)\n",
    "\n",
    "                if Path(output_zarr).exists():\n",
    "                    cache_file = Path(output_zarr)\n",
    "\n",
    "            if input_file.suffixes[-1] == \".nc\":\n",
    "                output_zarr = Path(output_path) / input_file.with_suffix(\".zarr\").name\n",
    "                output_zarr_tmp = output_zarr.with_suffix(output_zarr.suffix + \".tmp\")\n",
    "                if datasets_update_check(input_file, output_zarr) in [\n",
    "                    None,\n",
    "                    \"not_equal\",\n",
    "                ]:\n",
    "                    logging.info(\"unpacking file: `%s`\", str(input_file))\n",
    "                    fn, groups = datastore_to_zarr(\n",
    "                        input_file,\n",
    "                        zfn=output_zarr_tmp,\n",
    "                        replace=True,\n",
    "                        root_group=root_group,\n",
    "                    )\n",
    "                    shutil.rmtree(output_zarr, ignore_errors=True)\n",
    "                    output_zarr_tmp.rename(output_zarr)\n",
    "                if Path(output_zarr).exists():\n",
    "                    cache_file = Path(output_zarr)\n",
    "\n",
    "            if cache_file.suffixes[-1] == \".zarr\":\n",
    "                with zarr.open_consolidated(input_file, mode='r') as dz:\n",
    "                    group_keys = list(dz.group_keys())\n",
    "                groups = [\n",
    "                    n\n",
    "                    for n in group_keys\n",
    "                    if n != root_group and n.isupper() and n.isalpha()\n",
    "                ]\n",
    "\n",
    "            # add missing groups from metadata lookup of stations\n",
    "            \n",
    "            \n",
    "            for g in groups:\n",
    "                output_dict = merge(\n",
    "                    copy.deepcopy(input_file_dict),\n",
    "                    {\n",
    "                        \"catalogue\": {\"station_id\": g},\n",
    "                        \"file\": output_zarr,\n",
    "                    },\n",
    "                )\n",
    "                output_files.append(copy.deepcopy(output_dict))\n",
    "\n",
    "            logging.info(\"cache file: `%s`\", output_zarr)\n",
    "\n",
    "    return output_files\n",
    "\n",
    "\n",
    "def datastore_cache_index(catalogue_path):\n",
    "    output_files = Path(catalogue_path).glob(\"*.zarr\")\n",
    "    for output_zarr in output_files:\n",
    "        logging.info(\"cache file (attach): `%s`\", output_zarr)\n",
    "    return output_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b721f65-ba6d-41ae-ae64-c234df53a1bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def datasets_update_check(src_filepath, dst_filepath):\n",
    "    \"\"\"compare timestamp attributes of two urbisphere data stores\"\"\"\n",
    "    src_time, src_query = datasets_query_time(src_filepath)\n",
    "    dst_time, dst_query = datasets_query_time(dst_filepath)\n",
    "\n",
    "    if isinstance(src_time, list) and isinstance(dst_time, list):\n",
    "        if src_time == dst_time:\n",
    "            # idential lists, no change\n",
    "            return \"equal\"\n",
    "        elif src_time[0] != dst_time[0] and src_time[-1] != dst_time[-1]:\n",
    "            return \"not_equal\"\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f4c4b4-ef83-4d82-b837-5056655098ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ede9afc-5997-4d10-b70d-471dc4957e9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def encode_dataset_id(dataset_dict):\n",
    "    dataset_dict_valid = {\n",
    "        k: v\n",
    "        for k, v in dataset_dict.items()\n",
    "        if k not in [\"file\"] and isinstance(v, str)\n",
    "    }\n",
    "    return urllib.parse.urlencode(dataset_dict_valid)\n",
    "\n",
    "\n",
    "def dicts_equal_all(dict1, dict2, excluded):\n",
    "    keys = dict1.keys() | dict2.keys()\n",
    "    return all((dict1[key] == dict2[key]) ^ (key in excluded) for key in keys)\n",
    "\n",
    "\n",
    "def dicts_equal_any(dict1, dict2, excluded):\n",
    "    keys = dict1.keys() & dict2.keys()\n",
    "    return all((dict1[key] == dict2[key]) ^ (key in excluded) for key in keys)\n",
    "\n",
    "\n",
    "def decode_dataset_id(dataset_id, catalogue_lookup=True):\n",
    "    # default = {\n",
    "    #    \"id\": None,\n",
    "    #    \"system_group\": \"AWS\",\n",
    "    #    \"production_name\": \"urbisphere\",\n",
    "    #    \"production_level\": \"L2R\",\n",
    "    #    \"api_name\": \"StadtFreiburg-latest\",\n",
    "    #    \"api_token\": query_index,\n",
    "    # }\n",
    "    default = {\n",
    "        **copy.deepcopy(root_dict[0]),\n",
    "        **{\n",
    "            \"id\": None,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    dataset_id_unquote = urllib.parse.unquote_plus(dataset_id)\n",
    "    dataset_dict = {**default, **dict(urllib.parse.parse_qsl(dataset_id_unquote))}\n",
    "    logging.debug(\n",
    "        \"`dataset_dict` dict:\\n# start of item\\n%s\\n# end of item\\n\",\n",
    "        pformat(dataset_dict, sort_dicts=False),\n",
    "    )\n",
    "    logging.debug(\"`dataset_id` str: %s\", encode_dataset_id(dataset_dict))\n",
    "\n",
    "    if not catalogue_lookup:\n",
    "        return dataset_dict\n",
    "    try:\n",
    "        exclude_match = [\"id\"]\n",
    "        exclude_keys = [\"subset\", \"type\", \"path\", \"file\"]\n",
    "        catalogue_match = next(\n",
    "            entry\n",
    "            for entry in catalogue_list  # revise: reference to a global() variable\n",
    "            if dicts_equal_all(\n",
    "                {k: v for k, v in entry[\"catalogue\"].items() if k not in exclude_keys},\n",
    "                dataset_dict,\n",
    "                exclude_match,\n",
    "            )\n",
    "        )\n",
    "    except StopIteration:\n",
    "        return {}\n",
    "\n",
    "    catalogue_item = catalogue_match\n",
    "    catalogue_item[\"catalogue\"] = {**dataset_dict, **catalogue_match[\"catalogue\"]}\n",
    "    return catalogue_item\n",
    "\n",
    "\n",
    "def encode_catalogue(catalogue):\n",
    "    query_list = [\n",
    "        encode_dataset_id(\n",
    "            {k: v for k, v in entry[\"catalogue\"].items() if k in [\"station_id\"]}\n",
    "        )\n",
    "        for entry in catalogue  # if entry['file'].exists\n",
    "    ]\n",
    "    return sorted(set(query_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a455f79-50d5-4c40-82bd-b02931942819",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def decode_output_item(item):\n",
    "    items = []\n",
    "    try:\n",
    "        for coord in item[\"catalogue\"][\"subset\"][\"coords\"]:\n",
    "            coord_id_unquote = urllib.parse.unquote_plus(coord)\n",
    "            coord_dict = dict(urllib.parse.parse_qsl(coord_id_unquote))\n",
    "            newitem = {k: v for k, v in item.items() if not k in [\"catalogue\"]}\n",
    "            newitem[\"catalogue\"] = {\n",
    "                **{k: v for k, v in item[\"catalogue\"].items() if not k in [\"subset\"]},\n",
    "                **coord_dict,\n",
    "            }\n",
    "            newsubs = {\n",
    "                k: v\n",
    "                for k, v in item[\"catalogue\"][\"subset\"].items()\n",
    "                if k not in [\"coords\"]\n",
    "            }\n",
    "            if newsubs:\n",
    "                newitem[\"catalogue\"][\"subset\"] = newsubs\n",
    "\n",
    "            items.append(copy.deepcopy(newitem))\n",
    "    except:\n",
    "        items.append(copy.deepcopy(item))\n",
    "    return items\n",
    "\n",
    "\n",
    "def filter_catalogue(catalogue, output_file):\n",
    "    from mergedeep import merge\n",
    "\n",
    "    res = []\n",
    "    for catalogue_entry in catalogue:\n",
    "        for output_entry in output_file:\n",
    "            items = decode_output_item(output_entry)\n",
    "            for item in items:\n",
    "                if dicts_equal_any(catalogue_entry[\"catalogue\"], item[\"catalogue\"], []):\n",
    "                    d = copy.deepcopy(\n",
    "                        merge(copy.deepcopy(catalogue_entry), copy.deepcopy(item))\n",
    "                    )\n",
    "                    res.append(d)\n",
    "                    # print(item)\n",
    "                    # print(catalogue_entry)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9483683-ed95-495b-809b-47e6db5b032a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_input_files(input_subset, default_subset={}, custom_subset={}, debug=False):\n",
    "    if not default_subset:\n",
    "        default_subset = dict(\n",
    "            path_base=input_path_base,\n",
    "            path=input_path,\n",
    "            file=input_file,\n",
    "        )\n",
    "\n",
    "    def force_list(x):\n",
    "        return x if isinstance(x, list) else [x]\n",
    "\n",
    "    input_files = [\n",
    "        {\n",
    "            \"type\": n[\"file\"][\"type\"],\n",
    "            \"path\": os.path.join(n[\"path_base\"], n[\"path\"], n[\"file\"][\"path\"]),\n",
    "            \"file\": n[\"file\"][\"file\"],\n",
    "            \"catalogue\": n[\"file\"][\"catalogue\"],\n",
    "        }\n",
    "        for n in get_dictlist_permutations(\n",
    "            dict(\n",
    "                path_base=force_list(default_subset[\"path_base\"]),\n",
    "                path=force_list(default_subset[\"path\"]),\n",
    "                file=default_subset[\"file\"],\n",
    "            )\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # expand input_files\n",
    "    fn_list = [\n",
    "        {\n",
    "            \"type\": input_file[\"type\"],\n",
    "            \"file\": os.path.join(input_file[\"path\"], input_file[\"file\"]).format(\n",
    "                **{**ss, **custom_subset}\n",
    "            ),\n",
    "            \"catalogue\": input_file[\"catalogue\"],\n",
    "        }\n",
    "        for input_file in input_files\n",
    "        for ss in get_dictlist_permutations(input_subset)\n",
    "    ]  #\n",
    "    return (input_files, fn_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d00e516-6495-4b32-a6ca-459c1f829264",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd7d6d0-c958-4294-875c-980a0645ccb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mask_dataset_zarr(ds, entry):\n",
    "    \"\"\"Mask invalid data as listed in the QC test\"\"\"\n",
    "    ds_mod = None\n",
    "\n",
    "    # tests qc in subset\n",
    "    if \"subset\" in entry[\"catalogue\"].keys():\n",
    "        if ds and \"qc\" in entry[\"catalogue\"][\"subset\"].keys():\n",
    "            logging.debug(\n",
    "                \"`mask_dataset_zarr`: catalogue lookup `entry['catalogue']['subset']['qc']` OK\"\n",
    "            )\n",
    "\n",
    "            # test qc dataset_id\n",
    "            if \"dataset_id\" in entry[\"catalogue\"][\"subset\"][\"qc\"].keys():\n",
    "                dataset_id = entry[\"catalogue\"][\"subset\"][\"qc\"][\"dataset_id\"]\n",
    "                logging.debug(\n",
    "                    \"`mask_dataset_zarr`: catalogue lookup `entry['catalogue']['subset']['qc']['dataset_id']` OK\"\n",
    "                )\n",
    "\n",
    "                logging.debug(\n",
    "                    \"`dataset_id` dict:\\n# start of item\\n%s\\n# end of item\\n\",\n",
    "                    pformat(dataset_id, sort_dicts=False),\n",
    "                )\n",
    "\n",
    "                logging.debug(\n",
    "                    \"`entry` dict:\\n# start of item\\n%s\\n# end of item\\n\",\n",
    "                    pformat(entry, sort_dicts=False),\n",
    "                )\n",
    "\n",
    "                entry_qc_dict = decode_dataset_id(dataset_id, catalogue_lookup=False)\n",
    "\n",
    "                # fix:\n",
    "                entry_qc_dict[\"station_id\"] = entry[\"catalogue\"][\"station_id\"]\n",
    "\n",
    "                logging.debug(\n",
    "                    \"`entry_qc_dict` dict:\\n# start of item\\n%s\\n# end of item\\n\",\n",
    "                    pformat(entry_qc_dict, sort_dicts=False),\n",
    "                )\n",
    "\n",
    "                dataset_id_qc = encode_dataset_id(entry_qc_dict)\n",
    "\n",
    "                logging.debug(\n",
    "                    \"`dataset_id_qc` dict:\\n# start of item\\n%s\\n# end of item\\n\",\n",
    "                    pformat(dataset_id_qc, sort_dicts=False),\n",
    "                )\n",
    "\n",
    "                entry_qc = decode_dataset_id(dataset_id_qc)\n",
    "\n",
    "                logging.debug(\n",
    "                    \"`entry_qc` dict:\\n# start of item\\n%s\\n# end of item\\n\",\n",
    "                    pformat(entry_qc, sort_dicts=False),\n",
    "                )\n",
    "\n",
    "                # test catalogue entry.\n",
    "                if entry_qc:\n",
    "                    # open qc dataset\n",
    "                    ds_qc = open_dataset_zarr(dataset_id_qc)\n",
    "\n",
    "                    if ds_qc:\n",
    "                        logging.debug(\"ds_qc exists\")\n",
    "\n",
    "                        # apply qc dataset as mask\n",
    "                        ds_qc_mod, ds_mod = mask_datasets_qc(ds, ds_qc)\n",
    "\n",
    "    if ds_mod:\n",
    "        logging.info(\"`mask_dataset_zarr`: QC curation applied.\")\n",
    "        return ds_mod\n",
    "    else:\n",
    "        logging.debug(\"`mask_dataset_zarr`: QC no curation applied.\")\n",
    "        return ds\n",
    "\n",
    "\n",
    "def mask_datasets_qc(\n",
    "    ds1,\n",
    "    ds2,\n",
    "    snames=None,\n",
    "    qnames=[\n",
    "        \"vdi3786_min\",\n",
    "        \"vdi3786_max\",\n",
    "        \"vdi3786_absolute_deviation_600S\",\n",
    "        \"vdi3786_absolute_deviation_3600S\",\n",
    "        \"vdi3786_stationarity_duration_min\",\n",
    "        \"vdi3786_stationarity_duration_max\",\n",
    "        \"vdi3786_stationarity_min\",\n",
    "        \"vdi3786_stationarity_max\",\n",
    "        \"vdi3786_ensemble\",\n",
    "    ],\n",
    "):\n",
    "    import itertools\n",
    "\n",
    "    # ds1 = xr.open_dataset(path_ds, group=group_id)\n",
    "    # ds2 = xr.open_dataset(path_qc, group=group_id)\n",
    "    # alignment\n",
    "    # ds2, ds1 = xr.align(ds2, ds1)\n",
    "    ds1, ds2 = xr.align(ds1, ds2)\n",
    "\n",
    "    # attributes dataframe\n",
    "    attrs_df = (\n",
    "        ds2.drop_dims([k for k in list(ds2.dims.keys()) if not k in [\"attributes\"]])\n",
    "        .reset_coords()\n",
    "        .to_dataframe()\n",
    "    )\n",
    "\n",
    "    # qc variable lists\n",
    "    snames = ds1[\"station_id\"].values.tolist()\n",
    "    vnames = attrs_df[\"attributes_name\"].values.tolist()\n",
    "\n",
    "    def ok_filter(attr_index, attr_name, qname):\n",
    "        # some filter combinations lead to unusable results.\n",
    "        if (\n",
    "            attr_index in [0]\n",
    "            and attr_name == \"pr_rate\"\n",
    "            and qname in [\"vdi3786_stationarity_max\"]\n",
    "        ):\n",
    "            return False\n",
    "        elif (\n",
    "            attr_index in [1]\n",
    "            and attr_name == \"pr_rate\"\n",
    "            and qname in [\"vdi3786_stationarity_min\"]\n",
    "        ):\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "    # apply qc mask\n",
    "    ds3 = ds1.copy(deep=False)\n",
    "    for sname, vname in list(itertools.product(snames, vnames)):\n",
    "        for i, row in attrs_df.iterrows():\n",
    "            if vname in ds1.data_vars and vname == row[\"attributes_name\"]:\n",
    "                logging.debug(\"%s\", vname)\n",
    "\n",
    "                # subset masking\n",
    "                va2 = {\n",
    "                    \"attributes\": [\n",
    "                        \"attributes_group\",\n",
    "                        \"attributes_index\",\n",
    "                        \"attributes_name\",\n",
    "                    ]\n",
    "                }\n",
    "                da2 = ds2.set_index(**va2)\n",
    "                da2 = da2.sel(row.to_dict()).drop_vars(\n",
    "                    list(va2.values())[0] + list(va2.keys())\n",
    "                )\n",
    "                da3 = ds3[vname]\n",
    "                da3_attrs = copy.deepcopy(ds3[vname].attrs)\n",
    "                for qname in qnames:\n",
    "                    if qname in da2.data_vars:\n",
    "                        if ok_filter(\n",
    "                            row[\"attributes_index\"], row[\"attributes_name\"], qname\n",
    "                        ):\n",
    "                            logging.debug(\n",
    "                                \" (%s,%s) %s\",\n",
    "                                row[\"attributes_index\"],\n",
    "                                row[\"attributes_name\"],\n",
    "                                qname,\n",
    "                            )\n",
    "                            da3 = xr.where(da2[qname], np.nan, da3)\n",
    "\n",
    "                            # reassign updated/masked variable\n",
    "                            ds3[vname] = da3.assign_attrs(da3_attrs)\n",
    "                        else:\n",
    "                            logging.debug(\n",
    "                                \" Excluded (%s,%s) %s\",\n",
    "                                row[\"attributes_index\"],\n",
    "                                row[\"attributes_name\"],\n",
    "                                qname,\n",
    "                            )\n",
    "\n",
    "    # update global attributes (revise logic)\n",
    "    gattrs_list = [\n",
    "        get_gattrs(ioconf[\"qc\"][\"gattrs\"]),\n",
    "        {k: v for k, v in ds2.attrs.items() if k in [\"history\"]},\n",
    "        ds1.attrs,\n",
    "    ]\n",
    "    gattrs_dict = metadb_combine_globalattrs(gattrs_list)\n",
    "\n",
    "    ds3 = ds3.assign_attrs(gattrs_dict)\n",
    "\n",
    "    # return (ds1, ds2, ds3, attrs, vnames, snames, qnames)\n",
    "    return (ds2, ds3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81560591-96d5-4f97-acdc-ce86076cfbc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def filter_dataset_zarr(dataset, entry):\n",
    "    d_attr_dict = copy.deepcopy(dataset.attrs)\n",
    "    g_attr_dict = get_gattrs(gattrs)\n",
    "\n",
    "    if \"subset\" in entry[\"catalogue\"].keys():\n",
    "        if \"data_vars\" in entry[\"catalogue\"][\"subset\"].keys():\n",
    "            av = entry[\"catalogue\"][\"subset\"][\"data_vars\"]\n",
    "            dv = [dv for dv in list(dataset.data_vars) if dv not in av]\n",
    "            logging.debug(\" catalogue: %s\", \",\".join(av))\n",
    "            logging.debug(\"   dropped: %s\", \",\".join(dv))\n",
    "            dataset = dataset.drop_vars(dv)\n",
    "        else:\n",
    "            logging.debug(\"   dropped: <none>\")\n",
    "\n",
    "        if \"coords\" in entry[\"catalogue\"][\"subset\"].keys():\n",
    "            logging.debug(\n",
    "                \"    coords: %s\", pformat(entry[\"catalogue\"][\"subset\"][\"coords\"])\n",
    "            )\n",
    "        else:\n",
    "            logging.debug(\"    coords: <none>\")\n",
    "\n",
    "    # update global attributes\n",
    "    if (\n",
    "        \"production_level\" in entry[\"catalogue\"].keys()\n",
    "        and \"production_profile\" in d_attr_dict\n",
    "    ):\n",
    "        if entry[\"catalogue\"][\"production_level\"] in [\"L2\", \"L2R\"]:\n",
    "            pub_attr_dict = get_gattrs_pub(\n",
    "                [{\"production_profile\": d_attr_dict[\"production_profile\"]}]\n",
    "            )\n",
    "            d_attr_dict = {**d_attr_dict, **pub_attr_dict}\n",
    "\n",
    "    n_attrs = metadb_combine_globalattrs([d_attr_dict, g_attr_dict])\n",
    "    dataset = dataset.assign_attrs(**n_attrs)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bc5839-f0d2-4c25-aeca-2977e718c484",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ds1, ds2, ds3, attrs, vnames, snames, qnames = dataset_mask(path_ds, path_qc, group_id=\"FRASHA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0bbc67-d8e7-4bff-bf38-ff410a6a25d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@list_to_tuple\n",
    "@cachetools.func.ttl_cache(maxsize=50, ttl=5 * 60)\n",
    "def open_dataset_zarr(dataset_id):\n",
    "    entry = decode_dataset_id(dataset_id)\n",
    "    if entry:\n",
    "        fn_zarr = Path(entry[\"file\"])\n",
    "        ds = xr.open_zarr(fn_zarr, group=entry[\"catalogue\"][\"station_id\"])\n",
    "\n",
    "        # report success\n",
    "        logging.debug(\"`open_dataset_zarr`: `dataset id` = `%s`\", dataset_id)\n",
    "\n",
    "        try:\n",
    "            # modify dataset, as configured in catalogue entry\n",
    "            ds = filter_dataset_zarr(ds, entry)\n",
    "\n",
    "            # apply qc masks, if configured in catalogue entry\n",
    "            ds = mask_dataset_zarr(ds, entry)\n",
    "\n",
    "            # fix a bug\n",
    "            ds = ds.load()\n",
    "\n",
    "            # modify dataset properties for xpublish=>0.30\n",
    "            for var in ds.coords:\n",
    "                if str(ds[var].dtype) == \"object\":\n",
    "                    ds[var] = ds[var].astype(\"<U42\")\n",
    "            # print(ds.data_vars)\n",
    "            for var in ds.data_vars:\n",
    "                if \"chunks\" in ds[var].encoding:\n",
    "                    del ds[var].encoding[\"chunks\"]\n",
    "            for var in ds.coords:\n",
    "                if \"chunks\" in ds[var].encoding:\n",
    "                    del ds[var].encoding[\"chunks\"]\n",
    "            ds = ds.unify_chunks()\n",
    "            ds = ds.reset_encoding()\n",
    "\n",
    "        except Exception as error:\n",
    "            print(\"The following error occurred:\", error)\n",
    "\n",
    "        return ds\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "#@dataset_to_tuple\n",
    "#@cachetools.func.ttl_cache(maxsize=50, ttl=5 * 60)\n",
    "def get_dataset_json(dataset):\n",
    "    # convert non-numeric dtypes\n",
    "    dataset[\"time\"] = dataset.time.dt.strftime(\"%Y-%m-%dT%H:%M:%S.%f%z\")\n",
    "\n",
    "    # largest dim last\n",
    "    # dataset = dataset.transpose(*list(dataset.dims)[::-1])\n",
    "    dataset = dataset.transpose(\"station\", \"system\", \"time\", missing_dims=\"ignore\")\n",
    "\n",
    "    return simplejson.dumps(dataset.to_dict(data=True), ignore_nan=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e099733-3994-410b-9e85-9e985e72d285",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def uvicorn_log_yaml():\n",
    "    import yaml\n",
    "\n",
    "    yml_str = \"\"\"\n",
    "    version: 1\n",
    "    disable_existing_loggers: False\n",
    "    formatters:\n",
    "      default:\n",
    "        \"()\": uvicorn.logging.DefaultFormatter\n",
    "        format: '[%(asctime)s] %(levelname)s %(message)s'\n",
    "      access:\n",
    "        \"()\": uvicorn.logging.AccessFormatter\n",
    "        format: \"[%(asctime)s %(process)d] %(name)s - %(levelname)s - %(message)s\"\n",
    "      logformat:\n",
    "        format: \"[%(asctime)s %(process)d] %(name)s - %(levelname)s - %(message)s\"\n",
    "    handlers:\n",
    "      file_handler:\n",
    "        class: logging.FileHandler\n",
    "        level: INFO\n",
    "        formatter: logformat\n",
    "        filename: info.log\n",
    "        encoding: utf8\n",
    "        mode: a\n",
    "      default:\n",
    "        formatter: default\n",
    "        class: logging.StreamHandler\n",
    "        stream: ext://sys.stderr\n",
    "      access:\n",
    "        formatter: access\n",
    "        class: logging.StreamHandler\n",
    "        stream: ext://sys.stdout\n",
    "    loggers:\n",
    "      uvicorn.error:\n",
    "        level: INFO\n",
    "        handlers:\n",
    "          - default\n",
    "          - file_handler\n",
    "        propagate: no\n",
    "      uvicorn.access:\n",
    "        level: INFO\n",
    "        handlers:\n",
    "          - access\n",
    "          - file_handler\n",
    "        propagate: no\n",
    "    \"\"\"\n",
    "    f_log = os.path.join(log_path, log_file).format(version_id=version[\"id\"])\n",
    "    f_cfg = Path(ioconfig_file).with_suffix(\".yaml\")\n",
    "\n",
    "    yml = yaml.safe_load(yml_str)\n",
    "    yml[\"handlers\"][\"file_handler\"][\"filename\"] = f_log\n",
    "\n",
    "    with open(f_cfg, \"w\", encoding=\"utf-8\") as f:\n",
    "        yaml.dump(yml, f)\n",
    "\n",
    "    if f_cfg.exists():\n",
    "        return str(f_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3e4bc3-417d-49fb-a593-350cae915e60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class UrbisphereDatasetPlugin(Plugin):\n",
    "    name: str = \"dataset_urbisphere\"\n",
    "    version: str = \"0.0.2\"\n",
    "\n",
    "    @hookimpl\n",
    "    def get_datasets(self):\n",
    "        # list datasets\n",
    "        return encode_catalogue(catalogue_list)\n",
    "\n",
    "    @hookimpl\n",
    "    def get_dataset(self, dataset_id: str) -> xr.Dataset | None:\n",
    "        # redirect dataset_id to the correct file and group\n",
    "        try:\n",
    "            return open_dataset_zarr(dataset_id)\n",
    "        except:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93571e3e-3ee7-44da-ad9a-0ff21f174b36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class JSONPlugin(Plugin):\n",
    "    \"\"\"Adds JSON endpoints for datasets\"\"\"\n",
    "\n",
    "    name: str = \"json\"\n",
    "    version: str = \"0.0.2\"\n",
    "\n",
    "    dataset_router_prefix: str = \"/json\"\n",
    "    dataset_router_tags: Sequence[str] = [\"json\"]\n",
    "\n",
    "    @hookimpl\n",
    "    def dataset_router(self, deps: Dependencies):\n",
    "        router = APIRouter(\n",
    "            prefix=self.dataset_router_prefix, tags=list(self.dataset_router_tags)\n",
    "        )\n",
    "\n",
    "        @router.get(\"/info\")\n",
    "        def get_json_info(\n",
    "            dataset=Depends(deps.dataset),\n",
    "            cache=Depends(deps.cache),\n",
    "        ):\n",
    "            \"\"\"Return the schema as a dictionary.\"\"\"\n",
    "            return JSONResponse(dataset.to_dict(data=False))\n",
    "\n",
    "        @router.get(\"/vars\")\n",
    "        def get_variable_list(\n",
    "            dataset=Depends(deps.dataset),\n",
    "            cache=Depends(deps.cache),\n",
    "        ):\n",
    "            \"\"\"Return a list of available variables.\"\"\"\n",
    "            return JSONResponse(list(dataset.data_vars))\n",
    "\n",
    "        @router.get(\"/{var}/isel/{timedelta}/{chunk}\")\n",
    "        def get_variable_chunk(\n",
    "            var: str = apiPath(\n",
    "                description=\"Variable. A single item from the list of {vars} or '-' for all.\"\n",
    "            ),\n",
    "            timedelta: str = apiPath(\n",
    "                description=\"Interval duration in timedelta notation, such as '1D', '1H'.\"\n",
    "            ),\n",
    "            chunk: str = apiPath(\n",
    "                description=\"Chunk index, such as '0', '1', '2', ..., or from the end, '-2', '-1'.\"\n",
    "            ),\n",
    "            dataset=Depends(deps.dataset),\n",
    "            cache=Depends(deps.cache),\n",
    "        ):\n",
    "            \"\"\"Return a dataset in a JSON representation. Query {chunk} from {timedelta} sized chunks.\"\"\"\n",
    "            if var in [\"-\", \"*\"]:\n",
    "                var = list(dataset.data_vars)\n",
    "            elif var not in dataset.data_vars:\n",
    "                raise HTTPException(\n",
    "                    status_code=404, detail=f\"Variable '{var}' not found in dataset\"\n",
    "                )\n",
    "            var_list = [var] if not isinstance(var, list) else var\n",
    "\n",
    "            # not implemented: evaluation of chunks, timedelta\n",
    "            cdx = [int(n) for n in chunk.split(\".\")]\n",
    "\n",
    "            idx = np.cumsum(\n",
    "                dataset[\"time\"].to_pandas().resample(timedelta).count().values\n",
    "            ).tolist()\n",
    "            idx = (\n",
    "                [0] + idx\n",
    "                if idx[-1] == dataset[\"time\"].shape[0]\n",
    "                else [0] + idx + [dataset[\"time\"].shape[0]]\n",
    "            )\n",
    "            idx_pairs = list(itertools.pairwise(idx))\n",
    "\n",
    "            if abs(cdx[0]) <= len(idx_pairs) and abs(\n",
    "                pd.to_timedelta(timedelta)\n",
    "            ) <= pd.to_timedelta(\"1D\"):\n",
    "                # subset\n",
    "                dataset = dataset.isel(time=slice(*idx_pairs[cdx[0]]))[var_list]\n",
    "                return JSONResponse(json.loads(get_dataset_json(dataset)))\n",
    "            else:\n",
    "                raise HTTPException(status_code=403, detail=\"Not available\")\n",
    "\n",
    "        @router.get(\"/{var}/sel/{timedelta}/{timestamp}\")\n",
    "        def get_variable_slice(\n",
    "            var: str = apiPath(\n",
    "                description=\"Variable. A single item from the list of {vars} or '-' for all.\"\n",
    "            ),\n",
    "            timedelta: str = apiPath(\n",
    "                description=\"Interval duration in timedelta notation, such as '1D', '1H', or backwards, '-1D', '-1H'.\"\n",
    "            ),\n",
    "            timestamp: str = apiPath(\n",
    "                description=\"ISO8601 date and time notation, such as '20230101T000000'.\"\n",
    "            ),\n",
    "            dataset=Depends(deps.dataset),\n",
    "            cache=Depends(deps.cache),\n",
    "        ):\n",
    "            \"\"\"Return a dataset in a JSON representation. Query a {timedelta} interval from (or until) a reference {timestamp}.\"\"\"\n",
    "            if var in [\"-\", \"*\"]:\n",
    "                var = list(dataset.data_vars)\n",
    "            elif var not in dataset.data_vars:\n",
    "                raise HTTPException(\n",
    "                    status_code=403, detail=f\"Variable '{var}' not found in dataset\"\n",
    "                )\n",
    "            var_list = [var] if not isinstance(var, list) else var\n",
    "\n",
    "            # not implemented: evaluation of timestamp, timedelta\n",
    "            time_ref = pd.to_datetime(timestamp, format=\"ISO8601\")\n",
    "            time_delta = pd.to_timedelta([\"0S\", timedelta])\n",
    "            time_range = [\n",
    "                str(n) for n in [time_ref + min(time_delta), time_ref + max(time_delta)]\n",
    "            ]\n",
    "            if np.diff(time_delta) <= pd.to_timedelta(\"1D\"):\n",
    "                dataset = dataset.sel(time=slice(*time_range))[var_list]\n",
    "                return JSONResponse(json.loads(get_dataset_json(dataset)))\n",
    "            else:\n",
    "                raise HTTPException(status_code=403, detail=\"Not available\")\n",
    "\n",
    "        return router"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50831321-3fca-4ad9-8858-e395a77e8ae8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# GeoJSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35474cbd-4fce-4fe9-a7bf-1446379f0581",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_dataset_chunk(var, timedelta, chunk, dataset):\n",
    "    def get_idx_pairs(dataset, timedelta):\n",
    "        \"\"\"Convert timedelta to index start and end\"\"\"\n",
    "        idx = np.cumsum(\n",
    "            dataset[\"time\"].to_pandas().resample(timedelta).count().values\n",
    "        ).tolist()\n",
    "        idx = (\n",
    "            [0] + idx\n",
    "            if idx[-1] == dataset[\"time\"].shape[0]\n",
    "            else [0] + idx + [dataset[\"time\"].shape[0]]\n",
    "        )\n",
    "        idx_pairs = list(itertools.pairwise(idx))\n",
    "        return idx_pairs\n",
    "\n",
    "    # eval inputs\n",
    "    # not implemented: evaluation of chunks, timedelta\n",
    "    if var in [\"-\", \"*\"]:\n",
    "        var = list(dataset.data_vars)\n",
    "    elif var not in dataset.data_vars:\n",
    "        return None\n",
    "    var_list = [var] if not isinstance(var, list) else var\n",
    "\n",
    "    if abs(pd.to_timedelta(timedelta)) > pd.to_timedelta(\"1D\"):\n",
    "        return None\n",
    "\n",
    "    cdx = [int(n) for n in chunk.split(\".\")]\n",
    "\n",
    "    idx_pairs = get_idx_pairs(dataset, timedelta)\n",
    "\n",
    "    # return slice\n",
    "    if abs(cdx[0]) <= len(idx_pairs):\n",
    "        # subset\n",
    "        dataset = dataset.isel(time=slice(*idx_pairs[cdx[0]]))[var_list]\n",
    "        return dataset\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_dataset_slice(var, timedelta, timestamp, dataset):\n",
    "    if var in [\"-\", \"*\"]:\n",
    "        var = list(dataset.data_vars)\n",
    "    elif var not in dataset.data_vars:\n",
    "        return None\n",
    "    var_list = [var] if not isinstance(var, list) else var\n",
    "\n",
    "    # not implemented: evaluation of timestamp, timedelta\n",
    "    time_ref = pd.to_datetime(timestamp, format=\"ISO8601\")\n",
    "    time_delta = pd.to_timedelta([\"0S\", timedelta])\n",
    "    time_range = [\n",
    "        str(n) for n in [time_ref + min(time_delta), time_ref + max(time_delta)]\n",
    "    ]\n",
    "    if np.diff(time_delta) <= pd.to_timedelta(\"1D\"):\n",
    "        dataset = dataset.sel(time=slice(*time_range))[var_list]\n",
    "        return dataset\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17db0e9-b85d-4874-b516-3bdd019ca7a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dataset_aggregator(\n",
    "    data,\n",
    "    data_vars=None,\n",
    "    agg_methods=[\"mean\", \"minimum\", \"maximum\"],\n",
    "    agg_periods=[\"24H\", \"1H\"],\n",
    "    delimiter=\".\",\n",
    "):\n",
    "    data_agg = []\n",
    "    data_att = []\n",
    "\n",
    "    if not data_vars or data_vars == [\"-\"]:\n",
    "        data_vars = list(data.data_vars.keys())\n",
    "\n",
    "    variables = {\n",
    "        \"data_vars\": [n for n in data_vars if n in list(data.data_vars.keys())]\n",
    "    }\n",
    "\n",
    "    def func_apply(resampled, method):\n",
    "        \"\"\"Expose functions within the ResampleDataset class\"\"\"\n",
    "        if method == \"mean\":\n",
    "            return resampled.mean()\n",
    "        elif method == \"maximum\":\n",
    "            return resampled.max()\n",
    "        elif method == \"minimum\":\n",
    "            return resampled.min()\n",
    "\n",
    "    # data_vars\n",
    "    for var_group, var_list in variables.items():\n",
    "        for var in var_list:\n",
    "            for ts in agg_periods:\n",
    "                for agg_method in agg_methods:\n",
    "                    # resample\n",
    "                    ds = func_apply(\n",
    "                        data[[var]].resample(time=ts, label=\"right\", origin=\"end\"),\n",
    "                        agg_method,\n",
    "                    )\n",
    "\n",
    "                    # get last value only\n",
    "                    ds = ds.resample(time=\"24H\", label=\"right\", origin=\"end\").last()\n",
    "\n",
    "                    # rename\n",
    "                    agg_key = tuple([var_group, var, \"values\", agg_method, ts])\n",
    "                    ds = ds.rename({var: delimiter.join(agg_key)})\n",
    "\n",
    "                    # convert attributes to data variables\n",
    "                    att = []\n",
    "                    for k, v in data[var].attrs.items():\n",
    "                        att_key = tuple([var_group, var, \"attrs\", k])\n",
    "                        ds = ds.assign({delimiter.join(att_key): ((\"aggregate\"), [v])})\n",
    "\n",
    "                    # store result in list\n",
    "                    data_agg.append(ds.copy())\n",
    "\n",
    "    dx = xr.concat(data_agg, dim=\"aggregate\")\n",
    "\n",
    "    # convert coordinates to data_vars, retain coordinate space\n",
    "    dc = dx.coords.to_dataset()\n",
    "    dc = dc.rename(\n",
    "        dict(\n",
    "            [\n",
    "                *[\n",
    "                    (n, delimiter.join((\"coords\", n)))\n",
    "                    for n in dc.coords.keys()\n",
    "                    if not n in [\"time\"]\n",
    "                ]\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # convert attributes to data_vars, retain some coordinate space\n",
    "    da_dims = (\"station\", \"system\")\n",
    "    da = dx.coords.to_dataset()\n",
    "    da = da.drop_dims([d for d in da.dims if not d in da_dims]).assign(\n",
    "        {\n",
    "            k: (\n",
    "                da_dims,\n",
    "                np.full(\n",
    "                    [da.dims[d] for d in da_dims],\n",
    "                    dtype=\"<U{}\".format(len(v)),\n",
    "                    fill_value=v,\n",
    "                ),\n",
    "            )\n",
    "            for k, v in da.attrs.items()\n",
    "        }\n",
    "    )\n",
    "    da = da.rename(\n",
    "        dict(\n",
    "            [\n",
    "                *[\n",
    "                    (n, delimiter.join((\"coords\", n)))\n",
    "                    for n in da.coords.keys()\n",
    "                    if not n in [\"time\"]\n",
    "                ]\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "    da = da.rename_vars(\n",
    "        dict([*[(n, delimiter.join((\"attrs\", n))) for n in da.data_vars.keys()]])\n",
    "    )\n",
    "\n",
    "    # Merge\n",
    "    dx = dx.merge(dc.reset_coords(drop=False)).merge(da.reset_coords(drop=False))\n",
    "\n",
    "    # Convert to pandas, restructure indices\n",
    "    df = (\n",
    "        dx.to_dataframe()\n",
    "        .reset_index([\"time\"], drop=False)\n",
    "        .groupby(list(dx.coords.keys()))\n",
    "        .last()\n",
    "    )\n",
    "\n",
    "    # Final restructuring\n",
    "    df = df.reindex(sorted(df.columns), axis=1)\n",
    "    df = df.sort_index(level=[\"station_id\"])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a10d1a-0dc9-41b5-bcf5-ea2381cb0386",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@list_to_tuple\n",
    "@cachetools.func.ttl_cache(maxsize=5, ttl=5 * 60)\n",
    "def get_gjson(dataset_ids, var, timedelta, chunk, mode):\n",
    "    dx = []\n",
    "    for dataset_id in dataset_ids:\n",
    "        if decode_dataset_id(dataset_id):\n",
    "            ds = open_dataset_zarr(dataset_id)\n",
    "            if mode == \"chunk\":\n",
    "                ds = get_dataset_chunk(var, timedelta, chunk, ds)\n",
    "            elif mode == \"slice\":\n",
    "                ds = get_dataset_slice(var, timedelta, chunk, ds)\n",
    "            if ds:\n",
    "                dx.append(ds)\n",
    "\n",
    "    # multi-dim workaround\n",
    "    dx = [ds.isel(system=[0], drop=False) for ds in dx]\n",
    "\n",
    "    # concat is sensitive to dimension changes, consider mergeß\n",
    "    data = xr.concat(dx, dim=\"station\")\n",
    "\n",
    "    df_agg = dataset_aggregator(\n",
    "        data,\n",
    "        data_vars=[\n",
    "            v for v in var if v in [\"hur\", \"plev\", \"pr_rate\", \"pwv\", \"rsd\", \"ta\", \"ws\"]\n",
    "        ],\n",
    "        agg_periods=(\n",
    "            [\"24H\", \"1H\"] if timedelta.endswith(\"D\") else list(set([timedelta, \"1H\"]))\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # pandas dataframe preparation\n",
    "    df = df_agg.reset_index(\n",
    "        [\"time\", \"station_id\"], drop=False\n",
    "    )  # reset the index as columns (t.b.d.)\n",
    "    df[\"time\"] = df[\"time\"].astype(str)  # convert Timestamp to string\n",
    "    df = df.set_index(\"station_id\", drop=True).rename(columns={\"time\": \"coords.time\"})\n",
    "\n",
    "    # geopandas\n",
    "    gdf = gpd.GeoDataFrame(\n",
    "        df,\n",
    "        geometry=gpd.points_from_xy(df[\"coords.station_lon\"], df[\"coords.station_lat\"]),\n",
    "        crs=\"EPSG:4326\",\n",
    "    )\n",
    "\n",
    "    gjson = gdf.to_json()\n",
    "\n",
    "    gjson_dict = json.loads(gdf.head(1).to_json())\n",
    "    logging.info(\"Generating GeoJSON\")\n",
    "    # print(\n",
    "    #    pformat(\n",
    "    #        gjson_dict,  # read the json as a dict\n",
    "    #        sort_dicts=False,\n",
    "    #        width=500,\n",
    "    #    )\n",
    "    # )\n",
    "\n",
    "    return gjson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd9e137-a7f1-4b2b-99bf-ed4b84b1058c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GeoJSONPlugin(Plugin):\n",
    "    \"\"\"Adds GeoJSON endpoints for datasets\"\"\"\n",
    "\n",
    "    name: str = \"geojson\"\n",
    "    version: str = \"0.0.1\"\n",
    "\n",
    "    dataset_router_prefix: str = \"/geojson\"\n",
    "    dataset_router_tags: Sequence[str] = [\"geojson\"]\n",
    "\n",
    "    @hookimpl\n",
    "    def dataset_router(self, deps: Dependencies):\n",
    "        router = APIRouter(\n",
    "            prefix=self.dataset_router_prefix, tags=list(self.dataset_router_tags)\n",
    "        )\n",
    "\n",
    "        @router.get(\"/info\")\n",
    "        def get_json_info(\n",
    "            dataset=Depends(deps.dataset),\n",
    "            cache=Depends(deps.cache),\n",
    "        ):\n",
    "            \"\"\"Return the schema as a dictionary.\"\"\"\n",
    "            return JSONResponse(dataset.to_dict(data=False))\n",
    "\n",
    "        @router.get(\"/vars\")\n",
    "        def get_variable_list(\n",
    "            dataset=Depends(deps.dataset),\n",
    "            cache=Depends(deps.cache),\n",
    "        ):\n",
    "            \"\"\"Return a list of available variables.\"\"\"\n",
    "            return JSONResponse(list(dataset.data_vars))\n",
    "\n",
    "        @router.get(\"/{var}/isel/{timedelta}/{chunk}\")\n",
    "        def get_variable_chunk(\n",
    "            var: str = apiPath(\n",
    "                description=\"Variable. A single item from the list of {vars} or '-' for all.\"\n",
    "            ),\n",
    "            timedelta: str = apiPath(\n",
    "                description=\"Interval duration in timedelta notation, such as '1D', '1H'.\"\n",
    "            ),\n",
    "            chunk: str = apiPath(\n",
    "                description=\"Chunk index, such as '0', '1', '2', ..., or from the end, '-2', '-1'.\"\n",
    "            ),\n",
    "            dataset_ids=Depends(deps.dataset_ids),\n",
    "        ):\n",
    "            \"\"\"Return a dataset in a JSON representation. Query {chunk} from {timedelta} sized chunks.\"\"\"\n",
    "\n",
    "            gjson = get_gjson(dataset_ids, var, timedelta, chunk, \"chunk\")\n",
    "\n",
    "            return JSONResponse(json.loads(gjson))\n",
    "\n",
    "        @router.get(\"/{var}/sel/{timedelta}/{timestamp}\")\n",
    "        def get_variable_slice(\n",
    "            var: str = apiPath(\n",
    "                description=\"Variable. A single item from the list of {vars} or '-' for all.\"\n",
    "            ),\n",
    "            timedelta: str = apiPath(\n",
    "                description=\"Interval duration in timedelta notation, such as '1D', '1H', or backwards, '-1D', '-1H'.\"\n",
    "            ),\n",
    "            timestamp: str = apiPath(\n",
    "                description=\"ISO8601 date and time notation, such as '20230101T000000'.\"\n",
    "            ),\n",
    "            dataset_ids=Depends(deps.dataset_ids),\n",
    "        ):\n",
    "            \"\"\"Return a dataset in a JSON representation. Query a {timedelta} interval from (or until) a reference {timestamp}.\"\"\"\n",
    "\n",
    "            gjson = get_gjson(dataset_ids, var, timedelta, timestamp, \"slice\")\n",
    "\n",
    "            return JSONResponse(json.loads(gjson))\n",
    "\n",
    "        return router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a076fe9-6fcb-4575-a894-7b191eaf5b81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2addc28c-eb71-4cd3-a4c2-fb3a7d177e57",
   "metadata": {
    "tags": []
   },
   "source": [
    "# MapBox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5247dccf-ea20-4cfe-9a04-f0c92bdcdcd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_mapbox_html(selected, dataset_ids, var, timedelta, chunk, mode):\n",
    "    # geojson feature properties key delimiter\n",
    "    delimiter = \".\"\n",
    "\n",
    "    # settings\n",
    "    locations = [input_subset[\"campaign_location\"]]\n",
    "\n",
    "    # default\n",
    "    default_selected = (input_subset[\"campaign_location\"], \"ta\", \"mean\", \"24H\")\n",
    "\n",
    "    def get_default_url(mode, host=\"https://someserver.uni-freiburg.de\"):\n",
    "        if mode == \"chunk\":\n",
    "            default_url = \"{api_host}/{api_path}/datasets/{dataset_id}/mapbox/{var}/isel/{timedelta}/{chunk}/{selection}\"\n",
    "            default_url_dict = dict(\n",
    "                api_host=host.strip(\"/\"),\n",
    "                api_path=output_path.strip(\"/\"),\n",
    "            )\n",
    "        elif mode == \"slice\":\n",
    "            default_url = \"{api_host}/{api_path}/datasets/{dataset_id}/mapbox/{var}/sel/{timedelta}/{chunk}/{selection}\"\n",
    "            default_url_dict = dict(\n",
    "                api_host=host.strip(\"/\"),\n",
    "                api_path=output_path.strip(\"/\"),\n",
    "            )\n",
    "        else:\n",
    "            default_url = (\n",
    "                \"{api_host}/{api_path}/datasets/{dataset_id}/mapbox/-/isel/1D/-1\"\n",
    "            )\n",
    "            default_url_dict = dict(\n",
    "                api_host=host.strip(\"/\"),\n",
    "                api_path=output_path.strip(\"/\"),\n",
    "            )\n",
    "\n",
    "        return (default_url, default_url_dict)\n",
    "\n",
    "    default_url, default_url_dict = get_default_url(None)\n",
    "\n",
    "    def get_default_mapbox_config():\n",
    "        default_mapbox_config = {\n",
    "            \"container\": \"map\",\n",
    "            \"style\": \"mapbox://styles/christen/clxjemj7y00bp01qngu6gbd4t\",\n",
    "            \"center\": [-2.58654735763921, 51.4637473335781],\n",
    "            \"zoom\": 11.5,\n",
    "            \"customAttribution\": 'Data: <a href=\"http://www.uni-freiburg.de/en/\">University of Freiburg</a>, Funding: European Research Council (ERC) Grant: 855005',\n",
    "        }\n",
    "        return(default_mapbox_config)\n",
    "\n",
    "    def get_default_template_config():\n",
    "        default_template_config = {\n",
    "            \"input_file\" : 'conf/templates/mapbox_template1.html.jinja2',          \n",
    "        }\n",
    "        return(default_template_config)\n",
    "    \n",
    "    def nested_dict(d, n=None):\n",
    "        if len(d) == 1:\n",
    "            return {d[0]: n}\n",
    "        else:\n",
    "            return {d[0]: nested_dict(d[1:], n)}\n",
    "\n",
    "    def get_features(gjson_dict, locations=[\"\"]):\n",
    "        \"\"\"Filter features\"\"\"\n",
    "        if not \"features\" in gjson_dict:\n",
    "            return None\n",
    "\n",
    "        return [\n",
    "            feature\n",
    "            for feature in gjson_dict[\"features\"]\n",
    "            for location in locations\n",
    "            if \"properties\" in feature and feature[\"id\"].startswith(location)\n",
    "        ]\n",
    "\n",
    "    def select_options(\n",
    "        item_dict,\n",
    "        selected=(\"FR\", \"ta\", \"mean\", \"24H\"),\n",
    "        value_key=\"option_value\",\n",
    "        href_key=\"option_url\",\n",
    "        text_key=\"option_text\",\n",
    "    ):\n",
    "        from lxml import etree\n",
    "\n",
    "        root = etree.Element(\"select\", attrib={\"id\": \"variable\"})\n",
    "        for k, v in item_dict:\n",
    "            att = {\"selected\": \"selected\"} if k == selected else {}\n",
    "            att[\"value\"] = v[value_key]\n",
    "            att[\"href\"] = v[href_key]\n",
    "            el = etree.Element(\"option\", attrib=copy.deepcopy(att))\n",
    "            el.text = v[text_key]\n",
    "            root.append(el)\n",
    "\n",
    "        etree.indent(root, space=\"    \")\n",
    "\n",
    "        return etree.tostring(root).decode(\"utf-8\")\n",
    "\n",
    "    def get_colorscale(var_long_name, indent=\"                \"):\n",
    "        src = os.path.join(\n",
    "            rebase_path(), \"common/colormap/data/ColorRampsWetterstation.nc\"\n",
    "        )\n",
    "\n",
    "        with xr.open_dataset(src) as ds:\n",
    "            if var_long_name in ds[\"name\"]:\n",
    "                df = ds.sel(name=var_long_name)[\"variable_bounds\"].to_pandas()\n",
    "            else:\n",
    "                df = ds.sel(name=\"precipitation\")[\"variable_bounds\"].to_pandas()\n",
    "\n",
    "            javascript = f\"// colorscale: {var_long_name}\\n\"\n",
    "\n",
    "            for index, bounds in df.iterrows():\n",
    "                ifelse = f\"if\" if index == 0 else f\"}} else if\"\n",
    "                bounds_from = (\n",
    "                    str(bounds[\"From\"])\n",
    "                    if not bounds[\"From\"] == -np.inf\n",
    "                    else \"-Infinity\"\n",
    "                )\n",
    "                bounds_to = (\n",
    "                    str(bounds[\"To\"]) if not bounds[\"To\"] == np.inf else \"Infinity\"\n",
    "                )\n",
    "                javascript += f\"{indent}{ifelse} ( DisplayValue >= { bounds_from } && DisplayValue < { bounds_to } ) {{ \\n{indent}\\tColorScale = {index}\\n\"\n",
    "\n",
    "            javascript += f\"{indent}}} else {{\\n{indent}\\tColorScale = 0\\n\"\n",
    "\n",
    "            javascript += f\"{indent}}};\\n\"\n",
    "\n",
    "            ds.close()\n",
    "\n",
    "        return javascript\n",
    "\n",
    "    def get_colormap(var_long_name, indent=\"                \"):\n",
    "        src = os.path.join(\n",
    "            rebase_path(), \"common/colormap/data/ColorRampsWetterstation.nc\"\n",
    "        )\n",
    "\n",
    "        with xr.open_dataset(src) as ds:\n",
    "            if var_long_name in ds[\"name\"]:\n",
    "                df = ds.sel(name=var_long_name, bounds=[\"From\"])[\"HEX\"].to_pandas()\n",
    "            else:\n",
    "                df = ds.sel(name=\"precipitation\", bounds=[\"From\"])[\"HEX\"].to_pandas()\n",
    "\n",
    "            javascript = f\"// colormap: {var_long_name}\\n\"\n",
    "            javascript += f\"{indent}\" + \"switch(ColorScale){\\n\"\n",
    "\n",
    "            for index, hex_color in df.iterrows():\n",
    "                javascript += f\"{indent}\\tcase {index}: // en\\n\"\n",
    "                javascript += (\n",
    "                    f\"{indent}\\t\\tel.style.backgroundColor = '{ hex_color.iloc[0] }';\\n\"\n",
    "                )\n",
    "                javascript += f\"{indent}\\t\\tbreak;\\n\"\n",
    "\n",
    "            javascript += f\"{indent}\\t}};\\n\"\n",
    "\n",
    "            ds.close()\n",
    "\n",
    "        return javascript\n",
    "\n",
    "    def get_properties_table():\n",
    "        feature_props = [\n",
    "            [\"coords.time\",\"coords.time\"],\n",
    "            \"coords.station_lat\",\n",
    "            \"coords.station_lon\",\n",
    "            \"coords.system_group\",\n",
    "            \"coords.system_id\",\n",
    "            \"coords.system_name\",\n",
    "        ]\n",
    "        table = \"\"\n",
    "        \n",
    "        table +='<table align=\"left\">'\n",
    "        for item in feature_props:\n",
    "            if isinstance(item,list):\n",
    "                table += f'<tr><td>\"{item[1]}\":</td><td>\"/${{feature.properties[\"{item[1]}\"]}}\"</td></tr>'\n",
    "            else:\n",
    "                table += f'<tr><td>\"{item}\":</td><td>\"${{feature.properties[\"{item}\"]}}\"</td></tr>'\n",
    "\n",
    "        table += \"</table>\"\n",
    "        return table\n",
    "\n",
    "    # translations and title formating and urls\n",
    "    def get_options(selected, options, mode, timedelta, chunk):\n",
    "        # geojson feature properties key delimiter\n",
    "        delimiter = \".\"\n",
    "\n",
    "        # settings\n",
    "        locations = [input_subset[\"campaign_location\"]]\n",
    "\n",
    "        # default\n",
    "        default_selected = (input_subset[\"campaign_location\"], \"ta\", \"mean\", \"24H\")\n",
    "\n",
    "        default_var = options[1][0] if len(options[1]) == 1 else \"-\"\n",
    "        default_url, default_url_dict = get_default_url(mode)\n",
    "\n",
    "        # Mapbox template\n",
    "        default_mapbox_config = get_default_mapbox_config()\n",
    "        mapbox_config = {**default_mapbox_config, **output_api['mapbox']}\n",
    "\n",
    "        # HTML template\n",
    "        default_template_config = get_default_template_config()        \n",
    "        template_config = {**default_template_config, **output_api['template']}\n",
    "\n",
    "        # translations and title formating\n",
    "        units_dict = {\"degree_\": \"°\", \"degrees\": \"°\", \"degree\": \"°\", \"percent\": \"%\"}\n",
    "        locations_dict = {\"FR\": \"Freiburg, Germany\", \"BR\": \"Bristol, UK\"}\n",
    "\n",
    "        options_list = []\n",
    "        for location, variable, method, period in itertools.product(*options):\n",
    "            key = (location, variable, method, period)\n",
    "\n",
    "            # city/area\n",
    "            city = location[:2]\n",
    "            for k in locations_dict.keys():\n",
    "                city = city.replace(k, locations_dict[k])\n",
    "\n",
    "            # variable name\n",
    "            name = feature_properties[\"data_vars\"][variable][\"attrs\"][\n",
    "                \"long_name\"\n",
    "            ].title()\n",
    "\n",
    "            # unit\n",
    "            units = feature_properties[\"data_vars\"][variable][\"attrs\"][\"units\"]\n",
    "            for k in units_dict.keys():\n",
    "                units = units.replace(k, units_dict[k])\n",
    "\n",
    "            # title\n",
    "            option_str = (\n",
    "                f\"{ name } { method.title() } last { period.lower() } ({ units })\"\n",
    "            )\n",
    "            option_url_selection = f\"location={location}&variable={variable}&period={period}&method={method}\"\n",
    "            option_url_dict = {\n",
    "                **default_url_dict,\n",
    "                **dict(\n",
    "                    dataset_id=f\"station_id={location}\",\n",
    "                    selection=option_url_selection,\n",
    "                    timedelta=timedelta,\n",
    "                    var=default_var,\n",
    "                    chunk=chunk,\n",
    "                    mode=mode,\n",
    "                ),\n",
    "            }\n",
    "            option_url = default_url.format(**option_url_dict)\n",
    "            option_geojson_dict = {\n",
    "                **default_url_dict,\n",
    "                **dict(\n",
    "                    dataset_id=location,\n",
    "                    var=default_var,\n",
    "                    timedelta=timedelta,\n",
    "                    chunk=chunk,\n",
    "                    mode=mode,\n",
    "                ),\n",
    "            }\n",
    "            option_geojson = (\n",
    "                default_url.replace(\"/mapbox/\", \"/geojson/\")\n",
    "                .replace(\"/{selection}\", \"\")\n",
    "                .format(**option_geojson_dict)\n",
    "            )\n",
    "            option_key = f\"data_vars.{ variable }.values.{ method }.{ period }\"\n",
    "\n",
    "            # colormaps\n",
    "            if key == selected:\n",
    "                colorscale = get_colorscale(\n",
    "                    feature_properties[\"data_vars\"][variable][\"attrs\"][\"long_name\"]\n",
    "                )\n",
    "                colormap = get_colormap(\n",
    "                    feature_properties[\"data_vars\"][variable][\"attrs\"][\"long_name\"]\n",
    "                )\n",
    "\n",
    "            result = [\n",
    "                (\"city\", city),\n",
    "                (\"option_text\", option_str),\n",
    "                (\"option_value\", option_key),\n",
    "                (\"option_url\", option_url),\n",
    "                (\"option_geojson\", option_geojson),\n",
    "            ]\n",
    "            options_list.append(\n",
    "                (\n",
    "                    key,\n",
    "                    dict(\n",
    "                        [\n",
    "                            *zip([\"location\", \"variable\", \"method\", \"period\"], key),\n",
    "                            *result,\n",
    "                        ]\n",
    "                    ),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return (options_list, mapbox_config, template_config, colorscale, colormap)\n",
    "\n",
    "    def mapbox_html(selected, options, centroid, mode, chunk):\n",
    "        \"\"\"Jinja2 Template\"\"\"\n",
    "        from jinja2 import Template\n",
    "\n",
    "        # options\n",
    "        options_list, mapbox_config, template_config, colorscale, colormap = get_options(\n",
    "            selected, options, mode, timedelta, chunk\n",
    "        )\n",
    "\n",
    "        # Define a macro\n",
    "        macro_values = {\n",
    "            \"mapbox_config\": json.dumps({**mapbox_config, **centroid}),\n",
    "            \"select_variables\": select_options(options_list, selected=selected),\n",
    "            \"select_variable\": dict(options_list)[selected][\"option_value\"],\n",
    "            \"title\": f\"Weather Station Network of { dict(options_list)[selected]['city']}\",\n",
    "            \"colorscale\": colorscale,\n",
    "            \"colormap\": colormap,\n",
    "            \"properties\": get_properties_table(),\n",
    "            \"geojson_url\": dict(options_list)[selected][\"option_geojson\"],\n",
    "        }\n",
    "\n",
    "        # Load the template from file (\"./conf/templates/mapbox_template1.html.jinja2\")\n",
    "        with open(template_config['input_file']) as file_:\n",
    "            template = Template(file_.read())\n",
    "\n",
    "        # render from template and macro\n",
    "        html_content = template.render(**macro_values)\n",
    "\n",
    "        return html_content\n",
    "\n",
    "    # read data\n",
    "    gjson = get_gjson(dataset_ids, var, timedelta, chunk, mode)\n",
    "\n",
    "    # conversion\n",
    "    gjson_dict = json.loads(gjson)\n",
    "\n",
    "    # features\n",
    "    features = get_features(gjson_dict, locations)\n",
    "\n",
    "    # centroid\n",
    "    import geopandas as gpd\n",
    "\n",
    "    gdf = gpd.read_file(gjson, driver=\"GeoJSON\")\n",
    "    gdf = gdf.to_crs(\"EPSG:4326\")\n",
    "\n",
    "    import warnings\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        centroid = {\n",
    "            \"center\": [\n",
    "                [x, y]\n",
    "                for x, y in zip(gdf.dissolve().centroid.x, gdf.dissolve().centroid.y)\n",
    "            ][0]\n",
    "        }\n",
    "        #print(centroid)\n",
    "        \n",
    "    # single feature/station properties as a dict\n",
    "    feature_properties = merge(\n",
    "        *[\n",
    "            nested_dict(d.split(delimiter), n)\n",
    "            for feature in [features[0]]\n",
    "            for d, n in feature[\"properties\"].items()\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # list of data_var variables\n",
    "    variables = list(feature_properties[\"data_vars\"].keys())\n",
    "\n",
    "    # unique list of methods\n",
    "    methods = list(\n",
    "        dict.fromkeys(\n",
    "            list(\n",
    "                itertools.chain(\n",
    "                    *[\n",
    "                        list(feature_properties[\"data_vars\"][var][\"values\"].keys())\n",
    "                        for var in variables\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # unique list of averaging periods\n",
    "    periods = list(\n",
    "        dict.fromkeys(\n",
    "            list(\n",
    "                itertools.chain(\n",
    "                    *[\n",
    "                        list(\n",
    "                            feature_properties[\"data_vars\"][var][\"values\"][\n",
    "                                method\n",
    "                            ].keys()\n",
    "                        )\n",
    "                        for method in methods\n",
    "                        for var in variables\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    options = (locations, variables, methods, periods)\n",
    "\n",
    "    # evaluate\n",
    "    # options_dict, colorscale, colormap = get_options(selected, locations, variables, methods, periods)\n",
    "    # print(dict(options)[default_selected])\n",
    "\n",
    "    html_content = mapbox_html(\n",
    "        selected, options, centroid, mode, chunk\n",
    "    )\n",
    "    return html_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37e7258-441e-41fc-b8e6-c83ace09e2ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MapBoxPlugin(Plugin):\n",
    "\n",
    "    \"\"\"Adds MapBox endpoints for datasets\"\"\"\n",
    "\n",
    "    name: str = \"mapbox\"\n",
    "    version: str = \"0.0.1\"\n",
    "\n",
    "    dataset_router_prefix: str = \"/mapbox\"\n",
    "    dataset_router_tags: Sequence[str] = [\"mapbox\"]\n",
    "\n",
    "    def decode_dataset_id(dataset_id, catalogue_lookup=False):\n",
    "        default = {\n",
    "            \"location\": input_subset[\"campaign_location\"],\n",
    "            \"variable\": \"ta\",\n",
    "            \"period\": \"1H\",\n",
    "            \"method\": \"mean\",\n",
    "            \"api_name\": root_dict[0][\"api_name\"],\n",
    "            \"api_token\": \"***\",\n",
    "        }\n",
    "        dataset_id_unquote = urllib.parse.unquote_plus(dataset_id)\n",
    "        dataset_dict = {\n",
    "            **default,\n",
    "            **dict(urllib.parse.parse_qsl(dataset_id_unquote)),\n",
    "        }\n",
    "        logging.debug(\n",
    "            \"`dataset_dict` dict:\\n# start of item\\n%s\\n# end of item\\n\",\n",
    "            pformat(dataset_dict, sort_dicts=False),\n",
    "        )\n",
    "        logging.debug(\"`dataset_id` str: %s\", encode_dataset_id(dataset_dict))\n",
    "        return dataset_dict\n",
    "\n",
    "    @hookimpl\n",
    "    def dataset_router(self, deps: Dependencies):\n",
    "        router = APIRouter(\n",
    "            prefix=self.dataset_router_prefix, tags=list(self.dataset_router_tags)\n",
    "        )\n",
    "\n",
    "        @router.get(\"/info\")\n",
    "        def get_json_info(\n",
    "            dataset=Depends(deps.dataset),\n",
    "            cache=Depends(deps.cache),\n",
    "        ):\n",
    "            \"\"\"Return the schema as a dictionary.\"\"\"\n",
    "            return JSONResponse(dataset.to_dict(data=False))\n",
    "\n",
    "        @router.get(\"/vars\")\n",
    "        def get_variable_list(\n",
    "            dataset=Depends(deps.dataset),\n",
    "            cache=Depends(deps.cache),\n",
    "        ):\n",
    "            \"\"\"Return a list of available variables.\"\"\"\n",
    "            return JSONResponse(list(dataset.data_vars))\n",
    "\n",
    "        @router.get(\"/{var}/isel/{timedelta}/{chunk}/{options}\")\n",
    "        def get_variable_chunk(\n",
    "            var: str = apiPath(\n",
    "                description=\"Variable. A single item from the list of {vars} or '-' for all.\"\n",
    "            ),\n",
    "            timedelta: str = apiPath(\n",
    "                description=\"Interval duration in timedelta notation, such as '1D', '1H'.\"\n",
    "            ),\n",
    "            chunk: str = apiPath(\n",
    "                description=\"Chunk index, such as '0', '1', '2', ..., or from the end, '-2', '-1'.\"\n",
    "            ),\n",
    "            options: str = apiPath(description=\"Map options\"),\n",
    "            dataset_ids=Depends(deps.dataset_ids),\n",
    "        ):\n",
    "            \"\"\"Return a dataset in a JSON representation. Query {chunk} from {timedelta} sized chunks.\"\"\"\n",
    "\n",
    "            \"\"\" handle selection and mapbox html\"\"\"\n",
    "            dataset_dict = decode_dataset_id(options, catalogue_lookup=False)\n",
    "            selected = tuple(\n",
    "                [dataset_dict[k] for k in [\"location\", \"variable\", \"method\", \"period\"]]\n",
    "            )\n",
    "\n",
    "            html_content = get_mapbox_html(\n",
    "                selected, dataset_ids, var, timedelta, chunk, \"chunk\"\n",
    "            )\n",
    "            return HTMLResponse(content=html_content, status_code=200)\n",
    "\n",
    "        @router.get(\"/{var}/sel/{timedelta}/{timestamp}/{options}\")\n",
    "        def get_variable_slice(\n",
    "            var: str = apiPath(\n",
    "                description=\"Variable. A single item from the list of {vars} or '-' for all.\"\n",
    "            ),\n",
    "            timedelta: str = apiPath(\n",
    "                description=\"Interval duration in timedelta notation, such as '1D', '1H', or backwards, '-1D', '-1H'.\"\n",
    "            ),\n",
    "            timestamp: str = apiPath(\n",
    "                description=\"ISO8601 date and time notation, such as '20230101T000000'.\"\n",
    "            ),\n",
    "            options: str = apiPath(description=\"Map options\"),\n",
    "            dataset_ids=Depends(deps.dataset_ids),\n",
    "        ):\n",
    "            \"\"\"Return a dataset in a JSON representation. Query a {timedelta} interval from (or until) a reference {timestamp}.\"\"\"\n",
    "\n",
    "            \"\"\" handle selection and mapbox html\"\"\"\n",
    "            dataset_dict = decode_dataset_id(options, catalogue_lookup=False)\n",
    "            selected = tuple(\n",
    "                [dataset_dict[k] for k in [\"location\", \"variable\", \"method\", \"period\"]]\n",
    "            )\n",
    "\n",
    "            print(dataset_dict)\n",
    "\n",
    "            html_content = get_mapbox_html(\n",
    "                selected, dataset_ids, var, timedelta, timestamp, \"slice\"\n",
    "            )\n",
    "            return HTMLResponse(content=html_content, status_code=200)\n",
    "\n",
    "        return router"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49acf12a-35f9-4a07-b338-d961afc1ee2f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d1bc1a-7a8d-4ba9-9808-d1d45a85f5a7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Static Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b71e39-99b9-4f7e-9231-c21f8ee1af15",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Version\n",
    "version = {\n",
    "    \"id\": \"v1.0.0\",\n",
    "    \"time\": \"2023-07-14\",\n",
    "}  # first version.\n",
    "version = {\n",
    "    \"id\": \"v1.0.1\",\n",
    "    \"time\": \"2023-09-30\",\n",
    "}  # modified for using a TOML config.\n",
    "version = {\n",
    "    \"id\": \"v1.0.2\",\n",
    "    \"time\": \"2023-12-05\",\n",
    "}  # modified for using curation based on QC tests.\n",
    "version = {\n",
    "    \"id\": \"v1.0.3\",\n",
    "    \"time\": \"2025-02-27\",\n",
    "}  # modified for using maps and metadb\n",
    "\n",
    "\n",
    "# Configuration file for input / output files\n",
    "ioconfig_name = \"datasets_api\"\n",
    "try:\n",
    "    import ipynbname\n",
    "\n",
    "    ioconfig_file = \"{}.toml\".format(ipynbname.name())\n",
    "except:\n",
    "    ioconfig_file = \"../conf/{}.toml\".format(ioconfig_name)\n",
    "\n",
    "# ----- Papermill injection below this cell -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670573b3-50e7-4c89-b68e-39550377b2dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# input/output config\n",
    "ioconf = parse_config(ioconfig_file, ioconfig_name, version)\n",
    "\n",
    "# validate config (to do)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9824a1a8-da3b-48a5-86fe-2caf737a5524",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Note: the approach to set global helper variables should be revised. \n",
    "But was/is used in combination with papermill automation.\n",
    "\"\"\"\n",
    "\n",
    "# set global variables\n",
    "query_from = ioconf[\"query\"][\"start\"]\n",
    "query_to = None if not \"end\" in ioconf[\"query\"] else ioconf[\"query\"][\"end\"]\n",
    "query_period = ioconf[\"query\"][\"period\"]\n",
    "query_index = ioconf[\"query\"][\"system_index\"]\n",
    "query_latest = ioconf[\"query\"][\"latest\"]\n",
    "query_cache = ioconf[\"query\"][\"cache\"]\n",
    "query_tasks = ioconf[\"query\"][\"tasks\"]\n",
    "\n",
    "input_path_base = ioconf[\"input\"][\"path_base\"]\n",
    "input_path = ioconf[\"input\"][\"path\"]\n",
    "input_file = ioconf[\"input\"][\"file\"]\n",
    "input_subset = ioconf[\"input\"][\"subset\"]\n",
    "cache_path_base = ioconf[\"cache\"][\"path_base\"]\n",
    "cache_path = ioconf[\"cache\"][\"path\"]\n",
    "cache_file = ioconf[\"cache\"][\"file\"]\n",
    "output_path_base = ioconf[\"output\"][\"path_base\"]\n",
    "output_path = ioconf[\"output\"][\"path\"]\n",
    "output_file = ioconf[\"output\"][\"file\"]\n",
    "output_api = ioconf[\"output\"][\"api\"]  # deviation from other notebooks\n",
    "\n",
    "log_path = ioconf[\"logging\"][\"path\"]\n",
    "log_file = ioconf[\"logging\"][\"file\"]\n",
    "log_format = ioconf[\"logging\"][\"format\"]\n",
    "log_filemode = (\n",
    "    \"a\" if not \"filemode\" in ioconf[\"logging\"] else ioconf[\"logging\"][\"filemode\"]\n",
    ")\n",
    "\n",
    "gattrs = ioconf[\"gattrs\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491ca3db-c478-47a6-a7d1-fdbbf0190d07",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Logging configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1003fae6-0442-4932-90f2-5091c4f8810f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create logger\n",
    "import logging\n",
    "import logging.handlers\n",
    "from pprint import pformat\n",
    "\n",
    "logging.basicConfig(\n",
    "    encoding=\"utf-8\",\n",
    "    format=log_format,\n",
    "    level=logging.INFO,\n",
    "    # Declare handlers\n",
    "    handlers=[\n",
    "        logging.FileHandler(\n",
    "            os.path.join(log_path, log_file).format(version_id=version[\"id\"]),\n",
    "            mode=log_filemode,\n",
    "        ),\n",
    "        logging.StreamHandler(sys.stdout),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52d073f-e702-4e81-9e23-09cfe7b1a417",
   "metadata": {},
   "source": [
    "## Dynamic Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08317861-5f7d-452b-bf4c-e6edc974941a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# summarize\n",
    "logging.info(\"`ioconf` file: %s\", ioconfig_file)\n",
    "logging.info(\n",
    "    \"`ioconf` dict:\\n# start of item\\n%s\\n# end of item\\n\",\n",
    "    pformat(ioconf, sort_dicts=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ee8cf1-b7cf-48c3-823c-5cc98e3c38a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9f6dbd-9d5b-4bb1-b721-578b3260a042",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# init root dict\n",
    "root_dict = [{}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d01dd6-aa3e-4d21-9782-5fcae2084a08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reset structure\n",
    "dim_list = [\"time\", \"station\", \"system\", \"sensor\", \"channel\", \"cell\", \"attributes\"]\n",
    "enc_conf = {\n",
    "    \"time\": {\n",
    "        \"units\": \"nanoseconds since 1970-01-01 00:00:00\",  # \" +0000\"\n",
    "        \"calendar\": \"proleptic_gregorian\",\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5c66a6-8a45-4dc9-a617-172860ca5da3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query_range = get_query_range(query_from, query_period, query_to)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd3f5d0-346c-40e0-9fbb-1b89a0b982be",
   "metadata": {
    "tags": []
   },
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30aa587-99e3-4821-b6d3-01db36e690be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # catalogue path\n",
    "    catalogue_path = (\n",
    "        Path(cache_path_base) / Path(cache_path) / Path(output_path).relative_to(\"/\")\n",
    "        if output_path[0] == \"/\"\n",
    "        else Path(output_path)\n",
    "    )\n",
    "\n",
    "    # get input files, built a catalogue\n",
    "    input_files_dict, input_files = get_input_files(\n",
    "        input_subset,\n",
    "        custom_subset={\n",
    "            \"time_bounds\": get_time_bounds(\n",
    "                # get_query_range(pd.Timestamp.now().floor(\"1D\"), \"1D\") # 1 D\n",
    "                get_query_range(pd.Timestamp.now().floor(\"1D\"), \"2D\") - pd.to_timedelta(\"1D\") # 3 D\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "\n",
    "    if \"cache\" in query_tasks:\n",
    "        catalogue_file = datastore_cache(\n",
    "            input_files,\n",
    "            catalogue_path,\n",
    "        )\n",
    "    else:\n",
    "        catalogue_file = datastore_cache(\n",
    "            input_files,\n",
    "            catalogue_path,\n",
    "        )  # ToDo; limit re-caching.\n",
    "\n",
    "    # filter catalogue list\n",
    "    catalogue_list = filter_catalogue(catalogue_file, output_file)\n",
    "\n",
    "    # root defaults\n",
    "    if isinstance(query_index, list):\n",
    "        root_dict = [\n",
    "            {\n",
    "                k: v\n",
    "                for k, v in decode_dataset_id(qi, catalogue_lookup=False).items()\n",
    "                if not k in [\"id\"]\n",
    "            }\n",
    "            for qi in query_index\n",
    "        ]\n",
    "    else:\n",
    "        root_dict = [\n",
    "            {\n",
    "                \"system_group\": \"AWS\",\n",
    "                \"production_level\": \"L2R\",\n",
    "                \"production_name\": \"urbisphere\",\n",
    "                \"api_name\": \"latest\",\n",
    "                \"api_token\": query_index,\n",
    "            }\n",
    "        ]\n",
    "    logging.info(\n",
    "        \"`root_dict` dict:\\n# start of item\\n%s\\n# end of item\\n\",\n",
    "        pformat(root_dict, sort_dicts=False),\n",
    "    )\n",
    "\n",
    "    # the public catalogues,\n",
    "    # currently a union with root_dict...\n",
    "    catalogue_public = [\n",
    "        entry\n",
    "        for item in root_dict\n",
    "        for entry in catalogue_list  # revise: reference to a global() variable\n",
    "        if dicts_equal_any(\n",
    "            entry[\"catalogue\"],\n",
    "            item,\n",
    "            [],\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # ... extended with all other in the list\n",
    "    catalogue_public.extend(\n",
    "        [\n",
    "            entry\n",
    "            for item in root_dict\n",
    "            for entry in catalogue_list  # revise: reference to a global() variable\n",
    "            if not dicts_equal_any(\n",
    "                entry[\"catalogue\"],\n",
    "                item,\n",
    "                [],\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    logging.debug(\n",
    "        \"`catalogue_public` dict:\\n# start of item\\n%s\\n# end of item\\n\",\n",
    "        pformat(catalogue_public, sort_dicts=False),\n",
    "    )\n",
    "\n",
    "    # catalogue_root = decode_dataset_id(encode_dataset_id(root_dict))\n",
    "    catalogue_root = catalogue_public[0]\n",
    "    logging.debug(\n",
    "        \"`catalogue_root` dict:\\n# start of item\\n%s\\n# end of item\\n\",\n",
    "        pformat(catalogue_root, sort_dicts=False),\n",
    "    )\n",
    "\n",
    "    if \"serve\" in query_tasks:\n",
    "        # extract app meta information\n",
    "        datasets_root = xr.open_zarr(\n",
    "            catalogue_root[\"file\"], group=catalogue_root[\"catalogue\"][\"station_id\"]\n",
    "        )\n",
    "        datasets_attrs = copy.deepcopy(datasets_root.attrs)\n",
    "        datasets_name = datasets_attrs[\"title\"]\n",
    "        datasets_info = \"\\n\".join(\n",
    "            [\n",
    "                f\"<h4>{n.title()}</h4><p>\\n\\n{str(datasets_attrs[n])}\\n</p>\"\n",
    "                for n in [*datasets_attrs.keys()]\n",
    "                if n in [\"keywords\", \"references\"]\n",
    "            ]\n",
    "        )\n",
    "        datasets_version = datasets_attrs[\"production_version\"]\n",
    "\n",
    "        if datasets_version == \"\":\n",
    "            datasets_version = \"v1.0.5\"\n",
    "\n",
    "        # api server settings\n",
    "        rest_config = dict(\n",
    "            app_kws=dict(\n",
    "                title=datasets_name,\n",
    "                description=datasets_info,\n",
    "                version=datasets_version,\n",
    "                docs_url=\"/docs\",\n",
    "                redoc_url=\"/redoc\",\n",
    "                openapi_url=\"/openapi.json\",\n",
    "            ),\n",
    "            cache_kws=dict(available_bytes=0),\n",
    "        )\n",
    "        serve_log_config = uvicorn_log_yaml()\n",
    "        serve_config = dict(\n",
    "            host=output_path_base.split(\":\")[0],  # e.g., \"127.0.0.1\",\n",
    "            port=int(output_path_base.split(\":\")[1]),  # e.g., 49240,\n",
    "            log_level=\"debug\",\n",
    "            log_config=serve_log_config,\n",
    "            root_path=output_path,  # e.g., \"services/x/api/v1/\"\n",
    "            app_dir=output_path,\n",
    "        )\n",
    "\n",
    "        # configure REST API\n",
    "        rest = Rest({}, **rest_config)\n",
    "        rest.register_plugin(UrbisphereDatasetPlugin())\n",
    "        rest.register_plugin(JSONPlugin())\n",
    "        rest.register_plugin(GeoJSONPlugin())\n",
    "        rest.register_plugin(MapBoxPlugin())\n",
    "\n",
    "        if Path(output_api['assets']['input_path']).exists():\n",
    "            rest.app.mount(\"/assets\", StaticFiles(directory=output_api['assets']['input_path']), name=\"static\")\n",
    "\n",
    "        # Start the server\n",
    "        if True:\n",
    "            rest.serve(**serve_config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
