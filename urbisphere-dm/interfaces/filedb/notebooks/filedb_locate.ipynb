{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "100009c0-0ffa-4aa2-95ce-c03901ca82c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import itertools\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import pygrok\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed70fc3-5990-4298-963f-6010ae42f2f7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Features\n",
    "- [x] FileDB: an index for lookup of paths, file name parts and patterns, also for offline use.\n",
    "- [x] Preprocess using updatedb/plocate RegExp functions (fast)\n",
    "- [x] Process using python RegExp functions (slow)\n",
    "- [x] Save as shared file (DB, cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e85fb51-a070-4d33-815e-46d96387e757",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def filedb_query_updatedb(\n",
    "    updatedb_path=[\n",
    "        \"interfaces/filedb/data/updatedb/urbisphere_RAW.db\",\n",
    "        \"interfaces/filedb/data/updatedb/urbisphere_L1.db\",\n",
    "    ],\n",
    "    patterns_chain=[\n",
    "        [\"PARIS\", \"BERLIN\", \"FREIBURG\", \"BRISTOL\"],\n",
    "        [\"TIMESTAMP_FILENAME\", \"TIMESTAMP_DWL_BG\", \"TIMESTAMP_ALC_CL31\"],\n",
    "    ],\n",
    "    replace_cache=False,\n",
    "    verbose=False,\n",
    "):\n",
    "    def rebase_path(path_base=\"urbisphere-dm\", path_root=None):\n",
    "        \"\"\"return abs path of a higher level directory\"\"\"\n",
    "        from pathlib import Path\n",
    "\n",
    "        path_root = Path(\"__file__\").parent.resolve() if not path_root else path_root\n",
    "        path_parts = lambda p: p[\n",
    "            0 : (p.index(path_base) + 1 if path_base in p else len(p))\n",
    "        ]\n",
    "        return str(Path(*[n for n in path_parts(Path(path_root).parts)]))\n",
    "\n",
    "    def convert_search_pattern(\n",
    "        py_regex_pattern, predefined_patterns, regex_type=\"plocate\"\n",
    "    ):\n",
    "        \"\"\"lookup search patterns; modified from pygrok\"\"\"\n",
    "        type_mapper = {}\n",
    "\n",
    "        if regex_type == \"python\":\n",
    "            re_sub_fun = (\n",
    "                lambda m: \"(?P<\"\n",
    "                + m.group(2)\n",
    "                + \">\"\n",
    "                + predefined_patterns[m.group(1)].regex_str\n",
    "                + \")\"\n",
    "            )\n",
    "        else:\n",
    "            re_sub_fun = lambda m: \"(\" + predefined_patterns[m.group(1)].regex_str + \")\"\n",
    "\n",
    "        while True:\n",
    "            # replace %{pattern_name:custom_name} (or %{pattern_name:custom_name:type}\n",
    "            # with regex and regex group name\n",
    "\n",
    "            py_regex_pattern = re.sub(\n",
    "                r\"%{(\\w+):(\\w+)(?::\\w+)?}\",\n",
    "                re_sub_fun,\n",
    "                py_regex_pattern,\n",
    "            )\n",
    "\n",
    "            # replace %{pattern_name} with regex\n",
    "            py_regex_pattern = re.sub(\n",
    "                r\"%{(\\w+)}\",\n",
    "                lambda m: \"(\" + predefined_patterns[m.group(1)].regex_str + \")\",\n",
    "                py_regex_pattern,\n",
    "            )\n",
    "\n",
    "            if re.search(r\"%{\\w+(:\\w+)?}\", py_regex_pattern) is None:\n",
    "                break\n",
    "\n",
    "        regex_obj = re.compile(py_regex_pattern)\n",
    "\n",
    "        return regex_obj\n",
    "\n",
    "    # Convert custom patterns\n",
    "    def custom_regex(custom_patterns):\n",
    "        \"\"\"break down pygrok custom patterns into regex strings for plocate and python, as comprehensive as possible\"\"\"\n",
    "        custom_pats = {}\n",
    "\n",
    "        for pat_name, regex_str in custom_patterns.items():\n",
    "            custom_pats[pat_name] = pygrok.Pattern(pat_name, regex_str)\n",
    "\n",
    "        # Translation of custom and default patterns to python and normal flavour REGEX\n",
    "        Grok = pygrok.Grok(\n",
    "            \"example\",\n",
    "            custom_patterns_dir=\"../conf/pygrok/\",\n",
    "            custom_patterns=custom_patterns,\n",
    "        )\n",
    "        predefined_patterns = Grok.predefined_patterns\n",
    "\n",
    "        custom_patterns_plocate = {\n",
    "            k: convert_search_pattern(\n",
    "                predefined_patterns[k].regex_str,\n",
    "                predefined_patterns,\n",
    "                regex_type=\"plocate\",\n",
    "            )\n",
    "            for k, v in custom_patterns.items()\n",
    "        }\n",
    "        custom_patterns_py = {\n",
    "            k: convert_search_pattern(\n",
    "                predefined_patterns[k].regex_str,\n",
    "                predefined_patterns,\n",
    "                regex_type=\"python\",\n",
    "            )\n",
    "            for k, v in custom_patterns.items()\n",
    "        }\n",
    "\n",
    "        custom_patterns = {k: v.pattern for k, v in custom_patterns_plocate.items()}\n",
    "\n",
    "        return (custom_patterns, custom_patterns_plocate, custom_patterns_py)\n",
    "\n",
    "    def plocate_cmd(\n",
    "        pattern,\n",
    "        predefined_patterns,\n",
    "        updatedb_path=\"urbisphere_RAW.db\",\n",
    "        count_only=False,\n",
    "        basename_only=False,\n",
    "    ):\n",
    "        \"\"\"Return plocate shell commands\"\"\"\n",
    "        if isinstance(pattern, str):\n",
    "            pattern = [pattern]\n",
    "\n",
    "        import copy\n",
    "        import shutil\n",
    "\n",
    "        plocate_cmd = [shutil.which(c) for c in [\"plocate\",\"locate\"] if shutil.which(c)][0]\n",
    "        \n",
    "\n",
    "        run_cmd = [\n",
    "            str(plocate_cmd),\n",
    "            \"-d\",\n",
    "            \"'{}'\".format(\n",
    "                \":\".join(updatedb_path)\n",
    "                if isinstance(updatedb_path, list)\n",
    "                else str(updatedb_path)\n",
    "            ),\n",
    "            \"--null\",\n",
    "        ]\n",
    "\n",
    "        if basename_only:\n",
    "            run_cmd.append(\"-b\")\n",
    "\n",
    "        if count_only:\n",
    "            run_cmd.append(\"-c\")\n",
    "\n",
    "        run_p = []\n",
    "        run_cmds = []\n",
    "        for p in pattern:\n",
    "            if p in predefined_patterns:\n",
    "                pattern_regex = (\n",
    "                    predefined_patterns[p].pattern\n",
    "                    if isinstance(predefined_patterns[p], re.Pattern)\n",
    "                    else predefined_patterns[p]\n",
    "                )\n",
    "            else:\n",
    "                return []\n",
    "\n",
    "            run_cmd.append(\"--regex\")\n",
    "            run_cmd.append(\"'{}'\".format(pattern_regex))\n",
    "            run_cmds.append(copy.deepcopy(run_cmd))\n",
    "            run_p.append(p)\n",
    "\n",
    "        run_dict = dict(zip(run_p, run_cmds))\n",
    "        return run_dict\n",
    "\n",
    "    def match_dict(text, regex_obj):\n",
    "        \"\"\"If text is matched with pattern, return variable names specified(%{pattern:variable name})\n",
    "        in pattern and their corresponding values.If not matched, return None.\n",
    "        \"\"\"\n",
    "\n",
    "        match_obj = regex_obj.search(text)\n",
    "\n",
    "        if match_obj == None:\n",
    "            return {}\n",
    "        matches = match_obj.groupdict()\n",
    "        return matches\n",
    "\n",
    "    # filters - years\n",
    "    def func_year(x):\n",
    "        if ~isinstance(x, float):\n",
    "            if isinstance(x, str):\n",
    "                if x.startswith(\"Q\"):\n",
    "                    # Vaisala years\n",
    "                    x = \"202\" + x[1:]\n",
    "                elif len(x) == 2:\n",
    "                    # Decadal years\n",
    "                    x = \"20\" + x\n",
    "        return x\n",
    "\n",
    "    # filters - datetime\n",
    "    def func_date(row):\n",
    "        date_cols = [\"time_start_year\", \"time_start_month\", \"time_start_day\"]\n",
    "        time_cols = [\"time_start_hour\", \"time_start_minute\", \"time_start_second\"]\n",
    "        if not any(\n",
    "            [isinstance(row[k], float) for k in date_cols + time_cols]\n",
    "        ) and not any(\n",
    "            [pd.isna(row[k]) for k in time_cols]\n",
    "        ):  # date and time\n",
    "            return datetime(*[int(row[k]) for k in date_cols + time_cols])\n",
    "        elif not any([isinstance(row[k], float) for k in date_cols]) and not all(\n",
    "            [pd.isna(row[k]) for k in time_cols]\n",
    "        ):  # date and partial time\n",
    "            return datetime(\n",
    "                *[\n",
    "                    int(row[k]) if not isinstance(row[k], float) else int(0)\n",
    "                    for k in date_cols + time_cols\n",
    "                ]\n",
    "            )\n",
    "        elif not any([isinstance(row[k], float) for k in date_cols]) and not any(\n",
    "            [pd.isna(row[k]) for k in date_cols]\n",
    "        ):  # date only\n",
    "            return datetime(*[int(row[k]) for k in date_cols])\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def filter_dataframe(df):\n",
    "        # deduplicate columns\n",
    "        for k in tqdm(\n",
    "            list(dict.fromkeys(df.columns[df.columns.duplicated(keep=False)]))\n",
    "        ):\n",
    "            print(f\"Filter {k}\")\n",
    "            ds = df.loc[:, k]\n",
    "            ds = ds.stack().groupby(level=0).first().reindex(ds.index)\n",
    "            df = df.drop(columns=k)\n",
    "            df[k] = ds\n",
    "\n",
    "        # apply filters\n",
    "        for k in [\"time_start_year\"]:\n",
    "            print(f\"Filter {k}\")\n",
    "            df[k] = df[k].apply(lambda x: func_year(x))\n",
    "\n",
    "        for k in [\"time\"]:\n",
    "            print(f\"Filter {k}\")\n",
    "            df[k] = df.apply(lambda row: func_date(row), axis=1)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def cache_file_name(updatedb_path):\n",
    "        L = list(itertools.chain(*[Path(n).stem.split(\"_\") for n in updatedb_path]))\n",
    "        L = list(dict.fromkeys(L))\n",
    "        P = list(itertools.chain(*[[str(Path(n).parent)] for n in updatedb_path]))\n",
    "        P = list(dict.fromkeys(P))[0]\n",
    "        E = list(itertools.chain(*[[str(Path(n).suffix)] for n in updatedb_path]))\n",
    "        E = list(dict.fromkeys(E))[0]\n",
    "\n",
    "        return Path(os.path.join(P, \"_\".join(L) + E)).with_suffix(\".feather\")\n",
    "\n",
    "    def export_feather(df, cache_path):\n",
    "        df.columns = [str(x) for x in df.columns.to_flat_index()]\n",
    "        df = df.reset_index()\n",
    "        df.to_feather(cache_path)\n",
    "\n",
    "    def import_feather(cache_path):\n",
    "        from ast import literal_eval as make_tuple\n",
    "\n",
    "        df = pd.read_feather(cache_path)\n",
    "        df = df.set_index(\"file_path\")\n",
    "        df.columns = pd.MultiIndex.from_tuples([make_tuple(n) for n in df.columns])\n",
    "        return df\n",
    "\n",
    "    # plocate accepts ERE, but without atomic grouping\n",
    "    custom_patterns = {\n",
    "        # file system\n",
    "        \"UNIXPATH\": r\"(/([\\w_%!$@:.,~-]+|\\\\.)*)+\",\n",
    "        \"UNIXPATH2\": \"(([^/]+[/])+)\",\n",
    "        \"UNIXFILE\": \".*?[a-zA-Z0-9]$\",\n",
    "        \"UNIXFILE2\": r\"urbishere_[\\w_%!$@:.,~-]*$\",        \n",
    "        \"NULL\": \"\\x00\",\n",
    "        # file locations\n",
    "        \"REPOSITORY\": \"^(/srv/meteo/archive/urbisphere/data/)\",\n",
    "        \"PRODUCTION\": \"((RAW|L0|L1|L2)/)\",\n",
    "        # networks\n",
    "        \"SMUROBS\": \"(by-source/smurobs/)\",\n",
    "        \"UKMO\": \"(by-source/ukmo/)\",\n",
    "        \"DWD\": \"(by-source/dwd/)\",\n",
    "        \"SOURCES\": \"(%{SMUROBS}|%{UKMO}|%{DWD})\",\n",
    "        # campaigns\n",
    "        \"PA\": \"((by-location|by-serialnr)/France/Paris/)\",\n",
    "        \"BE\": \"((by-location|by-serialnr)/Germany/Berlin/)\",\n",
    "        \"FR\": \"((by-location|by-serialnr)/Germany/Freiburg/)\",\n",
    "        \"BR\": \"((by-location|by-serialnr)/UK/Bristol/)\",         \n",
    "        # sensors, systems\n",
    "        \"SUBPATH\": \"(%{UNIXPATH2})\",\n",
    "        # time blocks\n",
    "        \"YEAR\": \"([2][0][2][1-7])\",  # only 2021 to 2027!\n",
    "        \"YEAR2\": \"([0-9][0-9])\",\n",
    "        \"YEAR2VAISALA\": \"([Q][1-5]){1}\",  # A calendar in Helsinki; only 2021 to 2025!\n",
    "        \"YEAR4\": \"([1-2][0-9][0-9][0-9])\",\n",
    "        \"MONTHNUM\": \"(0?[1-9]|1[0-2])\",\n",
    "        \"MONTHNUM2\": \"(0[1-9]|1[0-2])\",\n",
    "        \"MONTHDAY2\": \"((0[1-9])|([12][0-9])|(3[01]))\",\n",
    "        \"EPOCHSECONDS\": \"([0-9]{10}){1}\",\n",
    "        \"HOUR\": \"(2[0123]|[01]?[0-9])\",\n",
    "        \"MINUTE\": \"([0-5][0-9])\",\n",
    "        \"SECOND\": \"(([0-5]?[0-9]|60)([:.,][0-9]+)?)\",  # '60' is a leap second in most time standards and thus is valid.\n",
    "        # compounded time blocks\n",
    "        \"TIME\": \"(?!<[0-9])%{HOUR}:%{MINUTE}(:%{SECOND})(?![0-9])\",\n",
    "        \"ISO8601_TIMEZONE\": \"(Z|[+-]%{HOUR}(:?%{MINUTE}))\",\n",
    "        \"ISO8601_TIMESTAMP\": \"%{YEAR}-%{MONTHNUM}-%{MONTHDAY}[T ]%{HOUR}:?%{MINUTE}(:?%{SECOND})?%{ISO8601_TIMEZONE}?\",\n",
    "        \"ISO8601_TIMESTAMP_CONDENSED\": \"%{YEAR}%{MONTHNUM2}%{MONTHDAY2}[T]%{HOUR}%{MINUTE}%{SECOND}%{ISO8601_TIMEZONE}?\",\n",
    "        \"ISO8601_TIMEBOUND_CONDENSED\": \"%{ISO8601_TIMESTAMP_CONDENSED}_%{ISO8601_TIMESTAMP_CONDENSED}\",\n",
    "        \"TIMESTAMP_FILENAME\": r\"%{YEAR:time_start_year:int}[-]?%{MONTHNUM2:time_start_month:int}[-]?%{MONTHDAY2:time_start_day:int}[_-]?(%{HOUR:time_start_hour:int}[-]?%{MINUTE:time_start_minute:int}[-]?%{SECOND:time_start_second:int}(%{ISO8601_TIMEZONE:time_start_tzinfo:str})?)?[\\.]?\",\n",
    "        \"TIMESTAMP_DWL_BG\": r\"(Background)[_]%{MONTHDAY2:time_start_day:int}%{MONTHNUM2:time_start_month:int}%{YEAR2:time_start_year:int}[-]%{HOUR:time_start_hour:int}%{MINUTE:time_start_minute:int}%{SECOND:time_start_second:int}[\\.]\",  #'Background_080124-210017'\n",
    "        \"TIMESTAMP_ALC_CL31\": r\"[_]%{YEAR2VAISALA:time_start_year:int}%{MONTHNUM2:time_start_month:int}%{MONTHDAY2:time_start_day:int}%{HOUR:time_start_hour:int}[\\.]\",  #'CL31/S2010717/S2010717_Q3113006.dat''\n",
    "        # compounded other\n",
    "        \"PARIS\": \"%{REPOSITORY:repository_path:str}%{PRODUCTION:production_path:str}%{SOURCES:source_path:str}%{PA:campaign_path:str}%{SUBPATH:system_path:str}\",\n",
    "        \"BERLIN\": \"%{REPOSITORY:repository_path:str}%{PRODUCTION:production_path:str}%{SOURCES:source_path:str}%{BE:campaign_path:str}%{SUBPATH:system_path:str}\",\n",
    "        \"FREIBURG\": \"%{REPOSITORY:repository_path:str}%{PRODUCTION:production_path:str}%{SMUROBS:source_path:str}%{FR:campaign_path:str}%{SUBPATH:system_path:str}\",\n",
    "        \"BRISTOL\": \"%{REPOSITORY:repository_path:str}%{PRODUCTION:production_path:str}%{SMUROBS:source_path:str}%{BR:campaign_path:str}%{SUBPATH:system_path:str}\",                \n",
    "    }\n",
    "\n",
    "    # absolute paths\n",
    "    updatedb_path = [os.path.join(rebase_path(), n) for n in updatedb_path]\n",
    "\n",
    "    # cache path\n",
    "    cache_path = cache_file_name(updatedb_path)\n",
    "    print(cache_path)\n",
    "\n",
    "    # cache\n",
    "    if cache_path.exists() and not replace_cache:\n",
    "        # if a buffered copy exists, read it.\n",
    "        # print(\"Reading cache...\")\n",
    "        df = import_feather(cache_path)\n",
    "        return df\n",
    "\n",
    "    # helper config\n",
    "    patterns_sets = list(itertools.product(*patterns_chain))\n",
    "    updatedb_cwd = Path(updatedb_path[0]).absolute().resolve().parent\n",
    "    custom_patterns, custom_patterns_plocate, custom_patterns_py = custom_regex(\n",
    "        custom_patterns\n",
    "    )\n",
    "\n",
    "    # plocate commands\n",
    "    commands_sets = []\n",
    "    for patterns in patterns_sets:\n",
    "        cmds = plocate_cmd(\n",
    "            list(patterns),\n",
    "            custom_patterns_plocate,\n",
    "            updatedb_path=\":\".join([Path(p).name for p in updatedb_path]),\n",
    "        )\n",
    "        commands_sets.append(copy.deepcopy(cmds))\n",
    "\n",
    "    # plocate output\n",
    "    outputs_sets = []\n",
    "\n",
    "    for cmds in commands_sets:\n",
    "        outputs = []\n",
    "        for k, command in cmds.items():\n",
    "            command_str = r\" \".join(s for s in command)\n",
    "            if verbose:\n",
    "                print(f\"Command:\\n{command_str}\")\n",
    "\n",
    "            with subprocess.Popen(\n",
    "                [command_str],\n",
    "                stdout=subprocess.PIPE,\n",
    "                stderr=subprocess.PIPE,\n",
    "                shell=True,\n",
    "                cwd=updatedb_cwd,\n",
    "            ) as process:\n",
    "                output = process.communicate()[0].decode(\"utf-8\")\n",
    "                returncode = process.returncode\n",
    "                outputs.append(\n",
    "                    {\"cmd\": command_str, \"returncode\": returncode, \"output\": output}\n",
    "                )\n",
    "\n",
    "        outputs_sets.append(outputs)\n",
    "\n",
    "    # python regex / pygrok decoding\n",
    "    pattern_match = {}\n",
    "    for patterns, outputs in zip(patterns_sets, outputs_sets):\n",
    "        for n, p in enumerate(patterns):\n",
    "            new_pattern = patterns[0 : (n + 1)]\n",
    "\n",
    "            if new_pattern in pattern_match:\n",
    "                continue\n",
    "\n",
    "            regex_obj = custom_patterns_py[p]\n",
    "            output_list = outputs[n][\"output\"].rstrip(\"\\x00\").split(\"\\x00\")\n",
    "            output_list_search = list(filter(regex_obj.search, output_list))\n",
    "            output_list_result_1 = [\n",
    "                match_dict(text, regex_obj) for text in output_list_search\n",
    "            ]\n",
    "            output_list_result_2 = [\n",
    "                match_dict(Path(text).name, regex_obj) for text in output_list_search\n",
    "            ]\n",
    "\n",
    "            pattern_match[new_pattern] = {\n",
    "                \"text\": [],  # output_list\n",
    "                \"search\": output_list_search,\n",
    "                \"match_fullpath\": output_list_result_1,\n",
    "                \"match_basename\": output_list_result_2,\n",
    "            }\n",
    "\n",
    "            if verbose:\n",
    "                print(\"pattern {}\".format(str(new_pattern)))\n",
    "                print(\n",
    "                    \"text {text} / search {search} / match {match_fullpath} / match basename {match_basename}\".format(\n",
    "                        **{k: len(v) for k, v in pattern_match[new_pattern].items()}\n",
    "                    )\n",
    "                )\n",
    "\n",
    "    # Construct list of dataframes, concatenate list along index\n",
    "    # (1) Full path\n",
    "    dx_full = [\n",
    "        pd.DataFrame(v[\"match_fullpath\"], index=v[\"search\"])\n",
    "        for k, v in pattern_match.items()\n",
    "    ]\n",
    "    df_full = pd.concat(dx_full, axis=1)\n",
    "    df_full[\"file_name\"] = df_full.index.to_series().apply(lambda x: Path(x).name)\n",
    "    df_full.index.set_names([\"file_path\"], inplace=True)\n",
    "\n",
    "    # (2) Basename only\n",
    "    dx_base = [\n",
    "        pd.DataFrame(v[\"match_basename\"], index=v[\"search\"])\n",
    "        if any(v[\"match_basename\"])\n",
    "        else None\n",
    "        for k, v in pattern_match.items()\n",
    "    ]\n",
    "    df_base = pd.concat(dx_base, axis=1)\n",
    "    df_base[\"file_name\"] = df_base.index.to_series().apply(lambda x: Path(x).name)\n",
    "    df_base.index.set_names([\"file_path\"], inplace=True)\n",
    "\n",
    "    # Filter and merge\n",
    "    df_full = filter_dataframe(df_full)\n",
    "    df_base = filter_dataframe(df_base)\n",
    "\n",
    "    df_full.columns = pd.MultiIndex.from_product([[\"fullpath\"]] + [df_full.columns])\n",
    "    df_base.columns = pd.MultiIndex.from_product([[\"basename\"]] + [df_base.columns])\n",
    "\n",
    "    df = pd.concat([df_full, df_base], axis=1)\n",
    "\n",
    "    if not cache_path.exists() or replace_cache:\n",
    "        # write cache copy\n",
    "        print(\"Writing cache...\")\n",
    "        export_feather(df, cache_path)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5982b36c-705d-4211-a10e-e48be86b31aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "574597ba-ee4f-4399-a0cd-a0cc26762381",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/investigator8/jupyter/urbisphere-dm/interfaces/filedb/data/updatedb/urbisphere_RAW_L0_L1_L2.feather\n",
      "Command:\n",
      "/bin/plocate -d 'urbisphere_RAW.db:urbisphere_L0.db:urbisphere_L1.db:urbisphere_L2.db' --null --regex '(^(/srv/meteo/archive/urbisphere/data/))(((RAW|L0|L1|L2)/))((((by-source/smurobs/))|((by-source/ukmo/))|((by-source/dwd/))))(((by-location|by-serialnr)/France/Paris/))((((([^/]+[/])+))))'\n",
      "Command:\n",
      "/bin/plocate -d 'urbisphere_RAW.db:urbisphere_L0.db:urbisphere_L1.db:urbisphere_L2.db' --null --regex '(^(/srv/meteo/archive/urbisphere/data/))(((RAW|L0|L1|L2)/))((((by-source/smurobs/))|((by-source/ukmo/))|((by-source/dwd/))))(((by-location|by-serialnr)/France/Paris/))((((([^/]+[/])+))))' --regex '(([2][0][2][1-7]))[-]?((0[1-9]|1[0-2]))[-]?(((0[1-9])|([12][0-9])|(3[01])))[_-]?(((2[0123]|[01]?[0-9]))[-]?(([0-5][0-9]))[-]?((([0-5]?[0-9]|60)([:.,][0-9]+)?))(((Z|[+-]((2[0123]|[01]?[0-9]))(:?(([0-5][0-9]))))))?)?[\\.]?'\n",
      "Command:\n",
      "/bin/plocate -d 'urbisphere_RAW.db:urbisphere_L0.db:urbisphere_L1.db:urbisphere_L2.db' --null --regex '(^(/srv/meteo/archive/urbisphere/data/))(((RAW|L0|L1|L2)/))((((by-source/smurobs/))|((by-source/ukmo/))|((by-source/dwd/))))(((by-location|by-serialnr)/France/Paris/))((((([^/]+[/])+))))'\n",
      "Command:\n",
      "/bin/plocate -d 'urbisphere_RAW.db:urbisphere_L0.db:urbisphere_L1.db:urbisphere_L2.db' --null --regex '(^(/srv/meteo/archive/urbisphere/data/))(((RAW|L0|L1|L2)/))((((by-source/smurobs/))|((by-source/ukmo/))|((by-source/dwd/))))(((by-location|by-serialnr)/France/Paris/))((((([^/]+[/])+))))' --regex '(Background)[_](((0[1-9])|([12][0-9])|(3[01])))((0[1-9]|1[0-2]))(([0-9][0-9]))[-]((2[0123]|[01]?[0-9]))(([0-5][0-9]))((([0-5]?[0-9]|60)([:.,][0-9]+)?))[\\.]'\n",
      "Command:\n",
      "/bin/plocate -d 'urbisphere_RAW.db:urbisphere_L0.db:urbisphere_L1.db:urbisphere_L2.db' --null --regex '(^(/srv/meteo/archive/urbisphere/data/))(((RAW|L0|L1|L2)/))((((by-source/smurobs/))|((by-source/ukmo/))|((by-source/dwd/))))(((by-location|by-serialnr)/France/Paris/))((((([^/]+[/])+))))'\n",
      "Command:\n",
      "/bin/plocate -d 'urbisphere_RAW.db:urbisphere_L0.db:urbisphere_L1.db:urbisphere_L2.db' --null --regex '(^(/srv/meteo/archive/urbisphere/data/))(((RAW|L0|L1|L2)/))((((by-source/smurobs/))|((by-source/ukmo/))|((by-source/dwd/))))(((by-location|by-serialnr)/France/Paris/))((((([^/]+[/])+))))' --regex '[_](([Q][1-5]){1})((0[1-9]|1[0-2]))(((0[1-9])|([12][0-9])|(3[01])))((2[0123]|[01]?[0-9]))[\\.]'\n",
      "Command:\n",
      "/bin/plocate -d 'urbisphere_RAW.db:urbisphere_L0.db:urbisphere_L1.db:urbisphere_L2.db' --null --regex '(^(/srv/meteo/archive/urbisphere/data/))(((RAW|L0|L1|L2)/))((((by-source/smurobs/))|((by-source/ukmo/))|((by-source/dwd/))))(((by-location|by-serialnr)/Germany/Berlin/))((((([^/]+[/])+))))'\n",
      "Command:\n",
      "/bin/plocate -d 'urbisphere_RAW.db:urbisphere_L0.db:urbisphere_L1.db:urbisphere_L2.db' --null --regex '(^(/srv/meteo/archive/urbisphere/data/))(((RAW|L0|L1|L2)/))((((by-source/smurobs/))|((by-source/ukmo/))|((by-source/dwd/))))(((by-location|by-serialnr)/Germany/Berlin/))((((([^/]+[/])+))))' --regex '(([2][0][2][1-7]))[-]?((0[1-9]|1[0-2]))[-]?(((0[1-9])|([12][0-9])|(3[01])))[_-]?(((2[0123]|[01]?[0-9]))[-]?(([0-5][0-9]))[-]?((([0-5]?[0-9]|60)([:.,][0-9]+)?))(((Z|[+-]((2[0123]|[01]?[0-9]))(:?(([0-5][0-9]))))))?)?[\\.]?'\n",
      "Command:\n",
      "/bin/plocate -d 'urbisphere_RAW.db:urbisphere_L0.db:urbisphere_L1.db:urbisphere_L2.db' --null --regex '(^(/srv/meteo/archive/urbisphere/data/))(((RAW|L0|L1|L2)/))((((by-source/smurobs/))|((by-source/ukmo/))|((by-source/dwd/))))(((by-location|by-serialnr)/Germany/Berlin/))((((([^/]+[/])+))))'\n",
      "Command:\n",
      "/bin/plocate -d 'urbisphere_RAW.db:urbisphere_L0.db:urbisphere_L1.db:urbisphere_L2.db' --null --regex '(^(/srv/meteo/archive/urbisphere/data/))(((RAW|L0|L1|L2)/))((((by-source/smurobs/))|((by-source/ukmo/))|((by-source/dwd/))))(((by-location|by-serialnr)/Germany/Berlin/))((((([^/]+[/])+))))' --regex '(Background)[_](((0[1-9])|([12][0-9])|(3[01])))((0[1-9]|1[0-2]))(([0-9][0-9]))[-]((2[0123]|[01]?[0-9]))(([0-5][0-9]))((([0-5]?[0-9]|60)([:.,][0-9]+)?))[\\.]'\n",
      "Command:\n",
      "/bin/plocate -d 'urbisphere_RAW.db:urbisphere_L0.db:urbisphere_L1.db:urbisphere_L2.db' --null --regex '(^(/srv/meteo/archive/urbisphere/data/))(((RAW|L0|L1|L2)/))((((by-source/smurobs/))|((by-source/ukmo/))|((by-source/dwd/))))(((by-location|by-serialnr)/Germany/Berlin/))((((([^/]+[/])+))))'\n",
      "Command:\n",
      "/bin/plocate -d 'urbisphere_RAW.db:urbisphere_L0.db:urbisphere_L1.db:urbisphere_L2.db' --null --regex '(^(/srv/meteo/archive/urbisphere/data/))(((RAW|L0|L1|L2)/))((((by-source/smurobs/))|((by-source/ukmo/))|((by-source/dwd/))))(((by-location|by-serialnr)/Germany/Berlin/))((((([^/]+[/])+))))' --regex '[_](([Q][1-5]){1})((0[1-9]|1[0-2]))(((0[1-9])|([12][0-9])|(3[01])))((2[0123]|[01]?[0-9]))[\\.]'\n",
      "Command:\n",
      "/bin/plocate -d 'urbisphere_RAW.db:urbisphere_L0.db:urbisphere_L1.db:urbisphere_L2.db' --null --regex '(^(/srv/meteo/archive/urbisphere/data/))(((RAW|L0|L1|L2)/))((by-source/smurobs/))(((by-location|by-serialnr)/Germany/Freiburg/))((((([^/]+[/])+))))'\n",
      "Command:\n",
      "/bin/plocate -d 'urbisphere_RAW.db:urbisphere_L0.db:urbisphere_L1.db:urbisphere_L2.db' --null --regex '(^(/srv/meteo/archive/urbisphere/data/))(((RAW|L0|L1|L2)/))((by-source/smurobs/))(((by-location|by-serialnr)/Germany/Freiburg/))((((([^/]+[/])+))))' --regex '(([2][0][2][1-7]))[-]?((0[1-9]|1[0-2]))[-]?(((0[1-9])|([12][0-9])|(3[01])))[_-]?(((2[0123]|[01]?[0-9]))[-]?(([0-5][0-9]))[-]?((([0-5]?[0-9]|60)([:.,][0-9]+)?))(((Z|[+-]((2[0123]|[01]?[0-9]))(:?(([0-5][0-9]))))))?)?[\\.]?'\n",
      "Command:\n",
      "/bin/plocate -d 'urbisphere_RAW.db:urbisphere_L0.db:urbisphere_L1.db:urbisphere_L2.db' --null --regex '(^(/srv/meteo/archive/urbisphere/data/))(((RAW|L0|L1|L2)/))((by-source/smurobs/))(((by-location|by-serialnr)/Germany/Freiburg/))((((([^/]+[/])+))))'\n",
      "Command:\n",
      "/bin/plocate -d 'urbisphere_RAW.db:urbisphere_L0.db:urbisphere_L1.db:urbisphere_L2.db' --null --regex '(^(/srv/meteo/archive/urbisphere/data/))(((RAW|L0|L1|L2)/))((by-source/smurobs/))(((by-location|by-serialnr)/Germany/Freiburg/))((((([^/]+[/])+))))' --regex '(Background)[_](((0[1-9])|([12][0-9])|(3[01])))((0[1-9]|1[0-2]))(([0-9][0-9]))[-]((2[0123]|[01]?[0-9]))(([0-5][0-9]))((([0-5]?[0-9]|60)([:.,][0-9]+)?))[\\.]'\n",
      "Command:\n",
      "/bin/plocate -d 'urbisphere_RAW.db:urbisphere_L0.db:urbisphere_L1.db:urbisphere_L2.db' --null --regex '(^(/srv/meteo/archive/urbisphere/data/))(((RAW|L0|L1|L2)/))((by-source/smurobs/))(((by-location|by-serialnr)/Germany/Freiburg/))((((([^/]+[/])+))))'\n",
      "Command:\n",
      "/bin/plocate -d 'urbisphere_RAW.db:urbisphere_L0.db:urbisphere_L1.db:urbisphere_L2.db' --null --regex '(^(/srv/meteo/archive/urbisphere/data/))(((RAW|L0|L1|L2)/))((by-source/smurobs/))(((by-location|by-serialnr)/Germany/Freiburg/))((((([^/]+[/])+))))' --regex '[_](([Q][1-5]){1})((0[1-9]|1[0-2]))(((0[1-9])|([12][0-9])|(3[01])))((2[0123]|[01]?[0-9]))[\\.]'\n",
      "Command:\n",
      "/bin/plocate -d 'urbisphere_RAW.db:urbisphere_L0.db:urbisphere_L1.db:urbisphere_L2.db' --null --regex '(^(/srv/meteo/archive/urbisphere/data/))(((RAW|L0|L1|L2)/))((by-source/smurobs/))(((by-location|by-serialnr)/UK/Bristol/))((((([^/]+[/])+))))'\n",
      "Command:\n",
      "/bin/plocate -d 'urbisphere_RAW.db:urbisphere_L0.db:urbisphere_L1.db:urbisphere_L2.db' --null --regex '(^(/srv/meteo/archive/urbisphere/data/))(((RAW|L0|L1|L2)/))((by-source/smurobs/))(((by-location|by-serialnr)/UK/Bristol/))((((([^/]+[/])+))))' --regex '(([2][0][2][1-7]))[-]?((0[1-9]|1[0-2]))[-]?(((0[1-9])|([12][0-9])|(3[01])))[_-]?(((2[0123]|[01]?[0-9]))[-]?(([0-5][0-9]))[-]?((([0-5]?[0-9]|60)([:.,][0-9]+)?))(((Z|[+-]((2[0123]|[01]?[0-9]))(:?(([0-5][0-9]))))))?)?[\\.]?'\n",
      "Command:\n",
      "/bin/plocate -d 'urbisphere_RAW.db:urbisphere_L0.db:urbisphere_L1.db:urbisphere_L2.db' --null --regex '(^(/srv/meteo/archive/urbisphere/data/))(((RAW|L0|L1|L2)/))((by-source/smurobs/))(((by-location|by-serialnr)/UK/Bristol/))((((([^/]+[/])+))))'\n",
      "Command:\n",
      "/bin/plocate -d 'urbisphere_RAW.db:urbisphere_L0.db:urbisphere_L1.db:urbisphere_L2.db' --null --regex '(^(/srv/meteo/archive/urbisphere/data/))(((RAW|L0|L1|L2)/))((by-source/smurobs/))(((by-location|by-serialnr)/UK/Bristol/))((((([^/]+[/])+))))' --regex '(Background)[_](((0[1-9])|([12][0-9])|(3[01])))((0[1-9]|1[0-2]))(([0-9][0-9]))[-]((2[0123]|[01]?[0-9]))(([0-5][0-9]))((([0-5]?[0-9]|60)([:.,][0-9]+)?))[\\.]'\n",
      "Command:\n",
      "/bin/plocate -d 'urbisphere_RAW.db:urbisphere_L0.db:urbisphere_L1.db:urbisphere_L2.db' --null --regex '(^(/srv/meteo/archive/urbisphere/data/))(((RAW|L0|L1|L2)/))((by-source/smurobs/))(((by-location|by-serialnr)/UK/Bristol/))((((([^/]+[/])+))))'\n",
      "Command:\n",
      "/bin/plocate -d 'urbisphere_RAW.db:urbisphere_L0.db:urbisphere_L1.db:urbisphere_L2.db' --null --regex '(^(/srv/meteo/archive/urbisphere/data/))(((RAW|L0|L1|L2)/))((by-source/smurobs/))(((by-location|by-serialnr)/UK/Bristol/))((((([^/]+[/])+))))' --regex '[_](([Q][1-5]){1})((0[1-9]|1[0-2]))(((0[1-9])|([12][0-9])|(3[01])))((2[0123]|[01]?[0-9]))[\\.]'\n",
      "pattern ('PARIS',)\n",
      "text 0 / search 1625986 / match 1625986 / match basename 1625986\n",
      "pattern ('PARIS', 'TIMESTAMP_FILENAME')\n",
      "text 0 / search 1557163 / match 1557163 / match basename 1557163\n",
      "pattern ('PARIS', 'TIMESTAMP_DWL_BG')\n",
      "text 0 / search 39858 / match 39858 / match basename 39858\n",
      "pattern ('PARIS', 'TIMESTAMP_ALC_CL31')\n",
      "text 0 / search 29627 / match 29627 / match basename 29627\n",
      "pattern ('BERLIN',)\n",
      "text 0 / search 1766596 / match 1766596 / match basename 1766596\n",
      "pattern ('BERLIN', 'TIMESTAMP_FILENAME')\n",
      "text 0 / search 1675126 / match 1675126 / match basename 1675126\n",
      "pattern ('BERLIN', 'TIMESTAMP_DWL_BG')\n",
      "text 0 / search 29150 / match 29150 / match basename 29150\n",
      "pattern ('BERLIN', 'TIMESTAMP_ALC_CL31')\n",
      "text 0 / search 73130 / match 73130 / match basename 73130\n",
      "pattern ('FREIBURG',)\n",
      "text 0 / search 422050 / match 422050 / match basename 422050\n",
      "pattern ('FREIBURG', 'TIMESTAMP_FILENAME')\n",
      "text 0 / search 421229 / match 421229 / match basename 421229\n",
      "pattern ('FREIBURG', 'TIMESTAMP_DWL_BG')\n",
      "text 0 / search 0 / match 0 / match basename 0\n",
      "pattern ('FREIBURG', 'TIMESTAMP_ALC_CL31')\n",
      "text 0 / search 0 / match 0 / match basename 0\n",
      "pattern ('BRISTOL',)\n",
      "text 0 / search 261045 / match 261045 / match basename 261045\n",
      "pattern ('BRISTOL', 'TIMESTAMP_FILENAME')\n",
      "text 0 / search 256695 / match 256695 / match basename 256695\n",
      "pattern ('BRISTOL', 'TIMESTAMP_DWL_BG')\n",
      "text 0 / search 4220 / match 4220 / match basename 4220\n",
      "pattern ('BRISTOL', 'TIMESTAMP_ALC_CL31')\n",
      "text 0 / search 0 / match 0 / match basename 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "251fe30345944454b4f08fe430fba8ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filter repository_path\n",
      "Filter production_path\n",
      "Filter source_path\n",
      "Filter campaign_path\n",
      "Filter system_path\n",
      "Filter time_start_year\n",
      "Filter time_start_month\n",
      "Filter time_start_day\n",
      "Filter time_start_hour\n",
      "Filter time_start_minute\n",
      "Filter time_start_second\n",
      "Filter time_start_tzinfo\n",
      "Filter time_start_year\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3101014/1626567219.py:169: DeprecationWarning: Bitwise inversion '~' on bool is deprecated and will be removed in Python 3.16. This returns the bitwise inversion of the underlying int object and is usually not what you expect from negating a bool. Use the 'not' operator for boolean negation or ~int(x) if you really want the bitwise inversion of the underlying int.\n",
      "  if ~isinstance(x, float):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filter time\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3552fc4ddc904b8c8bca6b81ac49feb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filter time_start_year\n",
      "Filter time_start_month\n",
      "Filter time_start_day\n",
      "Filter time_start_hour\n",
      "Filter time_start_minute\n",
      "Filter time_start_second\n",
      "Filter time_start_tzinfo\n",
      "Filter time_start_year\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3101014/1626567219.py:169: DeprecationWarning: Bitwise inversion '~' on bool is deprecated and will be removed in Python 3.16. This returns the bitwise inversion of the underlying int object and is usually not what you expect from negating a bool. Use the 'not' operator for boolean negation or ~int(x) if you really want the bitwise inversion of the underlying int.\n",
      "  if ~isinstance(x, float):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filter time\n",
      "Writing cache...\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    df = filedb_query_updatedb(\n",
    "        verbose=True,\n",
    "        replace_cache=True,\n",
    "        updatedb_path=[\n",
    "            \"interfaces/filedb/data/updatedb/urbisphere_RAW.db\",\n",
    "            \"interfaces/filedb/data/updatedb/urbisphere_L0.db\",            \n",
    "            \"interfaces/filedb/data/updatedb/urbisphere_L1.db\",\n",
    "            \"interfaces/filedb/data/updatedb/urbisphere_L2.db\",            \n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3b9bca-2cb3-463f-8d0b-f238705219e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a76ce3c-b420-4e99-b17a-a94c9e9ecdfd",
   "metadata": {
    "tags": []
   },
   "source": [
    "# DEV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "370ef435-4725-43a3-86d8-26fbb48b8010",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fnmatch_devel():\n",
    "    import fnmatch\n",
    "    import re\n",
    "\n",
    "    regex = fnmatch.translate(\"**/*\")\n",
    "    regex\n",
    "\n",
    "    logger_path = (\n",
    "        df[\"logger_path\"][~df[\"logger_path\"].str.contains(\"/by-upload-date/\")]\n",
    "        .unique()\n",
    "        .tolist()\n",
    "    )\n",
    "\n",
    "    df[df[(\"fullpath\", \"system_path\")].str.contains(\"/by-upload-date/\")].tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312367f9-f340-4182-9347-cfcf4f756de7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fce39820-2bdd-4a9d-b137-a504ac998eb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def exceptions(pattern_match):\n",
    "    # exceptions\n",
    "    exceptions = (\n",
    "        set(pattern_match[(\"PARIS\",)][\"text\"])\n",
    "        - set(pattern_match[(\"PARIS\", \"TIMESTAMP_FILENAME\")][\"text\"])\n",
    "        - set(pattern_match[(\"PARIS\", \"TIMESTAMP_DWL_BG\")][\"text\"])\n",
    "        - set(pattern_match[(\"PARIS\", \"TIMESTAMP_ALC_CL31\")][\"text\"])\n",
    "    )\n",
    "\n",
    "    exceptions = [\n",
    "        f for f in sorted(exceptions) if not Path(f).is_dir() and not \"/dupes/\" in f\n",
    "    ]\n",
    "\n",
    "    len(exceptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e4a868-1ee2-4e26-9d6c-979903f28d5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f61639-067c-4575-9bf3-49fb560dcc7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:status312v1] *",
   "language": "python",
   "name": "conda-env-status312v1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
