{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ef2405-4ad9-4815-8850-a3ddf3894247",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import sys\n",
    "import tempfile\n",
    "import uuid\n",
    "import warnings\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from functools import reduce\n",
    "from pathlib import Path, PurePath\n",
    "\n",
    "import dask\n",
    "import diskcache as dc\n",
    "import netCDF4 as nc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr  # >= 2022.11\n",
    "import zarr\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Overwrite defaults\n",
    "xr.set_options(keep_attrs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece7a3e8-3be0-4b05-bd7e-fce31e4be0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug\n",
    "import inspect\n",
    "import types\n",
    "from typing import cast\n",
    "\n",
    "# this_function_name = cast(types.FrameType, inspect.currentframe()).f_code.co_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a074a08b-052d-42be-b0ef-d818fd5d0092",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rebase_path(path_base=\"urbisphere-dm\", path_root=None):\n",
    "    \"\"\"return abs path of a higher level directory\"\"\"\n",
    "    from pathlib import Path\n",
    "\n",
    "    path_root = Path(\"__file__\").parent.resolve() if not path_root else path_root\n",
    "    path_parts = lambda p: p[0 : (p.index(path_base) + 1 if path_base in p else len(p))]\n",
    "    return str(Path(*[n for n in path_parts(Path(path_root).parts)]))\n",
    "\n",
    "\n",
    "sys.path.append(os.path.join(rebase_path(), \"interfaces/metadb/notebooks/\"))\n",
    "sys.path.append(os.path.join(rebase_path(), \"interfaces/metadb/src/\"))\n",
    "sys.path.append(os.path.join(rebase_path(), \"interfaces/filedb/notebooks/\"))\n",
    "sys.path.append(os.path.join(rebase_path(), \"common/plotly/notebooks/\"))\n",
    "\n",
    "from ipynb.fs.defs.filedb_datastore import filedb_decode, get_data_dict\n",
    "\n",
    "# from ipynb.fs.defs.metadb_query import googlesheet_query as vocabulary_query\n",
    "from ipynb.fs.defs.metadb_vocabulary import (\n",
    "    dataframe_query,\n",
    "    metadb_units_conv,\n",
    "    metadb_vocabulary_query,\n",
    ")\n",
    "\n",
    "# from ipynb.fs.defs.metadb_attributes import metadb_combine_globalattrs\n",
    "from metadb_attributes import metadb_combine_globalattrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a884bab5-0738-4931-ba3e-e787bcc64930",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class StopExecution(Exception):\n",
    "    def _render_traceback_(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "def get_dictlist_permutations(input_subset):\n",
    "    \"\"\"Expand a dict of (strs|lists) into all possible permutations.\"\"\"\n",
    "    import itertools\n",
    "\n",
    "    keys, values = zip(*input_subset.items())\n",
    "    values = [v if isinstance(v, list) else [v] for v in values]\n",
    "    permutations_dicts = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "    return permutations_dicts\n",
    "\n",
    "\n",
    "def get_dictlist_flatten(input_subset, joinstr=\"+\"):\n",
    "    \"\"\"Flatten dict of (strs|lists) into dict of (strs).\"\"\"\n",
    "    import itertools\n",
    "\n",
    "    keys, values = zip(*input_subset.items())\n",
    "    values = [joinstr.join(v) if isinstance(v, list) else (v) for v in values]\n",
    "    return dict(zip(keys, values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14879ec3-b8e4-4e8e-9b3c-8310a9fc76b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_creation_time(d=datetime.utcnow()):\n",
    "    d_str = d.replace(microsecond=0, tzinfo=timezone.utc).isoformat()\n",
    "    return d_str\n",
    "\n",
    "\n",
    "# def get_gattrs(gattrs):\n",
    "#     ga = gattrs[0]\n",
    "#     gs = {}\n",
    "#     if len(ga) > 1:\n",
    "#         for g in gattrs[1:]:\n",
    "#             bo = False\n",
    "#             for k in gs.keys():\n",
    "#                 if k in g and not bo:\n",
    "#                     bo = any(re.compile(m).match(gs[k]) for m in g[k])\n",
    "#                     if bo:\n",
    "#                         for k, v in g.items():\n",
    "#                             if k in ga:\n",
    "#                                 ga[k] = v\n",
    "\n",
    "#     gd = {\n",
    "#         \"version_id\": version[\"id\"],\n",
    "#         \"version_date\": version[\"time\"],\n",
    "#         \"creation_time\": get_creation_time(),\n",
    "#     }\n",
    "#     ga = {k: \"; \".join(v) if isinstance(v, list) else v for k, v in ga.items()}\n",
    "#     ga = {k: v.format(**gd) for k, v in ga.items()}\n",
    "#     return ga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc299a4e-d751-466f-bce7-0f6bda3e4a20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_gattrs(gattrs, sep=\";\\n\"):\n",
    "    ga = gattrs[0]\n",
    "    gs = {}\n",
    "    if len(ga) > 1:\n",
    "        for g in gattrs[1:]:\n",
    "            bo = False\n",
    "            for k in gs.keys():\n",
    "                if k in g and not bo:\n",
    "                    bo = any(re.compile(m).match(gs[k]) for m in g[k])\n",
    "                    if bo:\n",
    "                        for k, v in g.items():\n",
    "                            if k in ga:\n",
    "                                ga[k] = v\n",
    "\n",
    "    ct = get_creation_time()\n",
    "    gd = {\n",
    "        \"version_id\": version[\"id\"],\n",
    "        \"version_time\": version[\"time\"],\n",
    "        \"version_date\": version[\"time\"],\n",
    "        \"creation_time\": ct,\n",
    "        \"creation_date\": ct[0:10],\n",
    "    }\n",
    "    ga = {k: sep.join(v) if isinstance(v, list) else v for k, v in ga.items()}\n",
    "    ga = {k: v.format(**gd) for k, v in ga.items()}\n",
    "    return ga\n",
    "\n",
    "\n",
    "def get_gattrs_pub(gattrs):\n",
    "    publication_gattrs = {}\n",
    "    for ga in gattrs:\n",
    "        if \"production_profile\" in ga:\n",
    "            pga = metadb_publication_query(\n",
    "                ga[\"production_profile\"], publication_name=\"datasets_default\"\n",
    "            )\n",
    "            pga = {k: v for k, v in pga.items() if v != \"\"}\n",
    "            publication_gattrs = {**publication_gattrs, **pga}\n",
    "    return publication_gattrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afaf6600-1d7f-43c7-9dcf-e1b404eba642",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_config(ioconf_file, ioconf_name=None, version_dict=None):\n",
    "    \"\"\"Read configuration file and extract dict that matches version['id'].\"\"\"\n",
    "    import toml\n",
    "    from mergedeep import merge\n",
    "\n",
    "    def read_config(ioconfig_file):\n",
    "        if os.path.exists(ioconfig_file):\n",
    "            with open(ioconfig_file) as f:\n",
    "                ioconf = toml.load(f)\n",
    "            return ioconf\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    # read TOML config file\n",
    "    ioconf = read_config(ioconfig_file)\n",
    "\n",
    "    if isinstance(version_dict, dict):\n",
    "        # lookup of settings\n",
    "        if ioconfig_name:\n",
    "            group = ioconfig_name\n",
    "        else:\n",
    "            group = Path(ioconfig_file).stem\n",
    "\n",
    "        if group in ioconf:\n",
    "            conf_list = [\n",
    "                d\n",
    "                for d in ioconf[group]\n",
    "                if version_dict[\"id\"].startswith(\n",
    "                    d[\"version\"][\"id\"] if \"version\" in d else \"v\"\n",
    "                )\n",
    "            ]\n",
    "            config = merge(*conf_list)\n",
    "        else:\n",
    "            config = {}\n",
    "\n",
    "        return config\n",
    "    else:\n",
    "        return ioconf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedb7a2b-7b33-4d89-8a5a-e37fea10ee62",
   "metadata": {},
   "source": [
    "## Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b5c1c7-4bf4-44be-ae6a-0600fc47694d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sort_prefix(c_list, prefix=None):\n",
    "    if not prefix:\n",
    "        import string\n",
    "\n",
    "        prefix = list(string.ascii_lowercase)\n",
    "    p_list = [list(filter(lambda x: x.startswith(p), c_list)) for p in prefix]\n",
    "    c_set = list(dict.fromkeys([n for m in p_list for n in m]))\n",
    "    c_not = [x for x in c_list if not x in c_set]\n",
    "    return c_set + c_not\n",
    "\n",
    "\n",
    "def reset_coords(ds, sortby=None):\n",
    "    new_coords = sort_prefix(list(ds.coords), prefix=sortby)\n",
    "    new_vars = sort_prefix(list(ds.variables), prefix=sortby)\n",
    "    ds = ds.reset_coords()[new_vars]\n",
    "    ds = ds.set_coords([n for n in new_coords if not n in ds.dims])\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9dbf94d-d0c5-404f-8a12-cd4f57baae2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_reduce(ds, drop_vars=[], drop_dims=[]):\n",
    "    # drop dims\n",
    "    ds = ds.drop_dims([k for k in drop_dims if k in list(ds.dims)])\n",
    "\n",
    "    # drop coordinates/variables\n",
    "    try:\n",
    "        for n in drop_vars:\n",
    "            if n in list(ds.variables):\n",
    "                ds = ds.drop_vars([n])  # drop variables and coordinate variables\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # drop dims from coords\n",
    "    for c in list(ds.coords):\n",
    "        c_dims = list(ds.coords[c].dims)\n",
    "        if len(c_dims) > 1:\n",
    "            # print(c_dims)\n",
    "            c_dim = c.split(\"_\", 1)[0]\n",
    "            ds = ds.assign_coords(\n",
    "                **{\n",
    "                    c: ds[c].isel(\n",
    "                        **{k: 0 for k in [n for n in c_dims if not n == c_dim]},\n",
    "                        drop=True\n",
    "                    )\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30af84c8-e89c-4e00-abd7-dcbe5655c059",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fc6aab-c9a2-4b66-916b-4a2cd6c885ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_merge(ds):\n",
    "    # deducplicate and sort (time)\n",
    "    _, index = np.unique(ds[\"time\"], return_index=True)\n",
    "    ds = ds.isel(time=index)\n",
    "    ds = ds.sortby(\"time\")\n",
    "\n",
    "    # set multi-index\n",
    "    midx = {\n",
    "        n: [k for k in list(ds.coords) if k.startswith(f\"{n}_\")]\n",
    "        for n in [\"system\", \"station\", \"attributes\"]\n",
    "    }\n",
    "\n",
    "    midx = {k: v for k, v in list(midx.items()) if len(v) > 0}\n",
    "\n",
    "    ds = ds.set_index({k: v for k, v in list(midx.items()) if len(v) > 1})\n",
    "    ds = ds.set_coords([v[0] for k, v in list(midx.items()) if len(v) == 1])\n",
    "    return ds\n",
    "\n",
    "\n",
    "def postprocess_merge(ds):\n",
    "    # reset multi-index\n",
    "    idx = [\n",
    "        n\n",
    "        for n in ds.indexes\n",
    "        if isinstance(n, object) and n in ds.dims and n not in [\"time\"]\n",
    "    ]\n",
    "    if idx:\n",
    "        ds = ds.reset_index(idx)\n",
    "\n",
    "    # drop single-index\n",
    "    idx = [n for n in ds.indexes if n not in ds.dims]\n",
    "    if idx:\n",
    "        ds = ds.drop_indexes(idx)\n",
    "\n",
    "    # warning exception\n",
    "    idx = [n for n in ds.indexes if n not in [\"time\"]]\n",
    "    if idx:\n",
    "        logging.warning(\"Not all indexes could be reset.\")\n",
    "\n",
    "    return ds\n",
    "\n",
    "\n",
    "def merge_reduce(ds_list, merge_opts=dict(combine_attrs=\"drop_conflicts\")):\n",
    "    dx = None\n",
    "    for ds in tqdm(ds_list, disable=len(ds_list) < 3, desc=\"merge_reduce\"):\n",
    "        if not dx:\n",
    "            dx = ds\n",
    "        else:\n",
    "            dx = xr.merge([dx, ds], **merge_opts)\n",
    "\n",
    "    return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4053bab-8e38-463c-949c-305b839a6a76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_nc(ds):\n",
    "    # reset single or multi-index\n",
    "    for d in [\"station\", \"system\"]:\n",
    "        if d in ds.dims and d in ds.coords and d in ds.indexes:\n",
    "            if isinstance(ds.indexes[d], pd.MultiIndex):\n",
    "                # reset multi-index\n",
    "                ds = ds.reset_index([d])  # drop multi index to coordinates\n",
    "            else:\n",
    "                # reset single-index\n",
    "                ds = ds.drop_indexes([d])\n",
    "\n",
    "        if any(x.startswith(d) for x in ds.indexes):\n",
    "            # reset single-index, while making no assumptions on the suffix\n",
    "            ds = ds.reset_index([x for x in ds.indexes if x.startswith(d)])\n",
    "\n",
    "        if d in ds.dims and d in ds.coords:\n",
    "            # reset single-index, while making assumptions on the suffix\n",
    "            ds = ds.rename_vars({d: \"{}_group\".format(d)})\n",
    "\n",
    "    return ds\n",
    "\n",
    "\n",
    "def preprocess_store(ds):\n",
    "    # reset single or multi-index\n",
    "    for d in [\"station\", \"system\"]:\n",
    "        if d in ds.dims and d in ds.coords and d in ds.indexes:\n",
    "            if isinstance(ds.indexes[d], pd.MultiIndex):\n",
    "                # reset multi-index\n",
    "                ds = ds.reset_index([d])  # drop multi index to coordinates\n",
    "            else:\n",
    "                # reset single-index\n",
    "                ds = ds.drop_indexes([d])\n",
    "\n",
    "        if d in ds.dims and d in ds.coords:\n",
    "            # reset single-index, while making assumptions on the suffix\n",
    "            ds = ds.rename_vars({d: \"{}_group\".format(d)})\n",
    "\n",
    "    return ds\n",
    "\n",
    "\n",
    "def preprocess_attrs(ds):\n",
    "    for x in list(ds.data_vars):\n",
    "        dat = ds[x].attrs.copy()\n",
    "        ds[x].attrs = {}\n",
    "        ds[x] = ds[x].assign_attrs(\n",
    "            {k: v for k, v in dat.items() if k not in [\"coordinates\"]}\n",
    "        )\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449a208f-333f-4d5c-9f74-8d01f744eccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_files(input_subset, default_subset={}, custom_subset={}, debug=False):\n",
    "    if not default_subset:\n",
    "        default_subset = dict(\n",
    "            path_base=input_path_base,\n",
    "            path=input_path,\n",
    "            file=input_file,\n",
    "        )\n",
    "\n",
    "    def force_list(x):\n",
    "        return x if isinstance(x, list) else [x]\n",
    "\n",
    "    input_files = [\n",
    "        {\n",
    "            \"type\": n[\"file\"][\"type\"],\n",
    "            \"path\": os.path.join(n[\"path_base\"], n[\"path\"], n[\"file\"][\"path\"]),\n",
    "            \"file\": n[\"file\"][\"file\"],\n",
    "        }\n",
    "        for n in get_dictlist_permutations(\n",
    "            dict(\n",
    "                path_base=force_list(default_subset[\"path_base\"]),\n",
    "                path=force_list(default_subset[\"path\"]),\n",
    "                file=default_subset[\"file\"],\n",
    "            )\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # expand input_files\n",
    "    fn_list = [\n",
    "        {\n",
    "            \"type\": input_file[\"type\"],\n",
    "            \"file\": os.path.join(input_file[\"path\"], input_file[\"file\"]).format(\n",
    "                **{**ss, **custom_subset}\n",
    "            ),\n",
    "        }\n",
    "        for input_file in input_files\n",
    "        for ss in get_dictlist_permutations(input_subset)\n",
    "    ]  #\n",
    "    return (input_files, fn_list)\n",
    "\n",
    "\n",
    "def get_output_files(dx, default_subset={}, custom_subset={}):\n",
    "    if dx:\n",
    "        # dataset lookup\n",
    "        s_id = list(dx[\"station_id\"].values) if \"system_id\" in list(dx.coords) else [\"\"]\n",
    "        i_id = list(dx[\"system_id\"].values) if \"system_id\" in list(dx.coords) else [\"\"]\n",
    "        ii_id = list(dx[\"sensor_id\"].values) if \"sensor_id\" in list(dx.coords) else [\"\"]\n",
    "\n",
    "        # modify output subset dict, based on input settings\n",
    "        output_subset = get_dictlist_flatten(input_subset)\n",
    "        output_subset[\"station_id\"] = \"\".join(s_id) if len(s_id) == 1 else \"\"\n",
    "        output_subset[\"system_id\"] = \"+\".join(i_id)\n",
    "        output_subset[\"sensor_id\"] = \"+\".join(ii_id)\n",
    "        output_subset[\"time_bounds\"] = get_time_bounds(query_range)\n",
    "        output_subset = {**output_subset, **default_subset}\n",
    "    else:\n",
    "        output_subset = default_subset\n",
    "\n",
    "    output_subset = {**output_subset, **custom_subset}\n",
    "\n",
    "    if not \"output_files\" in globals():\n",
    "        output_files = {\"path\": output_subset[\"path\"], \"file\": output_subset[\"file\"]}\n",
    "\n",
    "    output_file = os.path.join(output_files[\"path\"], output_files[\"file\"]).format(\n",
    "        **output_subset\n",
    "    )\n",
    "    return output_file\n",
    "\n",
    "\n",
    "def get_time_bounds(query_range):\n",
    "    tr = query_range.strftime(\"%Y%m%dT%H%M%S%z\").tolist()\n",
    "    res = tr[0] if tr[0] == tr[1] else \"{}_{}\".format(*tr)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81eb825-10c9-4b59-b4e6-f08978bb713f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def datasets_fsspec_args(filepath):\n",
    "    \"\"\"generate a compounded url for fsspec, if relevant, plus arguments for xarray\"\"\"\n",
    "    url = \"\"\n",
    "    args = {}\n",
    "\n",
    "    p = Path(filepath).resolve()\n",
    "\n",
    "    # is the path an archive?\n",
    "    if p.suffixes[-1] == \".zip\" and len(p.suffixes) > 1:\n",
    "        url += f\"zip::\"\n",
    "\n",
    "        # define the suffix, in case of a trailing archive suffix\n",
    "        sufx = p.suffixes[-2]\n",
    "    else:\n",
    "        sufx = p.suffixes[-1]\n",
    "\n",
    "    if sufx == \".zarr\":\n",
    "        args[\"engine\"] = \"zarr\"\n",
    "    elif sufx == \".nc\":\n",
    "        args[\"engine\"] = \"netcdf4\"\n",
    "\n",
    "    # add the file path to the url\n",
    "    # url += p.as_uri()\n",
    "    if url != \"\":\n",
    "        url += \"file://\"\n",
    "\n",
    "    url += f\"{p}\"  # local files\n",
    "\n",
    "    return url, args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b720bf0-6010-4c50-a929-450f7307db08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4ceb08-770f-4cb6-bcd7-3959ff471e15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_ds_index(ds):\n",
    "    # equivalent: functionality similar to `preprocess_reduce`\n",
    "    a_dict = {\n",
    "        k: \"\".join(ds[k].squeeze().values.tolist())\n",
    "        for k in [\"system_group\", \"system_name\", \"sensor_name\"]\n",
    "        if k in list(ds.coords)\n",
    "    }\n",
    "    if \"sensor_name\" in a_dict and not \"system_name\" in a_dict:\n",
    "        a_dict[\"system_name\"] = a_dict[\"sensor_name\"]\n",
    "    return a_dict\n",
    "\n",
    "\n",
    "def datastore_units_query(datastore_df):\n",
    "    # unit converstion meta data for each file group\n",
    "    dx = []\n",
    "    for datastore_id, datastore in datastore_df.groupby(\n",
    "        [\n",
    "            (\"encoded\", \"station_id\"),\n",
    "            (\"decoded\", \"system_group\"),\n",
    "            (\"decoded\", \"system_id\"),\n",
    "        ]\n",
    "    ):\n",
    "        ifn_series = datastore.iloc[0]\n",
    "        fn = Path(\n",
    "            *ifn_series[\n",
    "                [(\"location\", \"path_base\"), (\"location\", \"path\"), (\"location\", \"file\")]\n",
    "            ].tolist()\n",
    "        )\n",
    "\n",
    "        store, store_args = datasets_fsspec_args(fn)\n",
    "        with xr.open_dataset(store, **store_args) as ds:\n",
    "            try:\n",
    "                a = get_ds_index(ds)\n",
    "                dx.append(tuple([datastore_id, a]))\n",
    "            except:\n",
    "                # print(ds)\n",
    "                pass\n",
    "\n",
    "    svoc_pat_list = [\n",
    "        {**{k: re.escape(v) for k, v in dict(t).items()}, **{\"original_name\": \".*\"}}\n",
    "        for t in {tuple(d[1].items()) for d in dx}\n",
    "    ]\n",
    "    units_list = metadb_vocabulary_query(\n",
    "        svoc_pat_list=svoc_pat_list,\n",
    "        production_level=\"L1\",\n",
    "    )\n",
    "    units_df = pd.concat(\n",
    "        [\n",
    "            pd.DataFrame(\n",
    "                index=pd.MultiIndex.from_frame(\n",
    "                    pd.DataFrame.from_dict(\n",
    "                        {k: [v] for k, v in list(d[\"query\"].items())}\n",
    "                    )\n",
    "                ),\n",
    "                data={\n",
    "                    (oKey, iKey): values\n",
    "                    for oKey, iDict in d[\"response\"].items()\n",
    "                    for iKey, values in iDict.items()\n",
    "                },\n",
    "            )\n",
    "            for d in units_list\n",
    "        ]\n",
    "    )\n",
    "    return units_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723fc756-49dd-4f69-b8f8-e75324c8daa7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def valid_zip(fn):\n",
    "    import sys\n",
    "    import zipfile\n",
    "\n",
    "    try:\n",
    "        the_zip_file = zipfile.ZipFile(fn)\n",
    "        ret = the_zip_file.testzip()\n",
    "        if ret is not None:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "    except Exception as ex:\n",
    "        print(\"Exception:\", ex)\n",
    "        return False\n",
    "\n",
    "\n",
    "def strip_extension(x):\n",
    "    x = Path(x)\n",
    "    e = []\n",
    "    while x.suffix in {\".zarr\", \".zip\", \".nc\"}:\n",
    "        e.append(x.suffix)\n",
    "        x = x.with_suffix(\"\")\n",
    "    return str(x)  # , \"\".join(e)\n",
    "\n",
    "\n",
    "def datasets_to_zarr_zip(dx, output_file, zipstore_args={}, **zarr_args):\n",
    "    from pathlib import Path\n",
    "\n",
    "    import zarr\n",
    "\n",
    "    # on the safe side, always start with an empty zarr.zip file.\n",
    "    zipstore_args_default = dict(\n",
    "        mode=\"w\",\n",
    "        allowZip64=True,\n",
    "    )\n",
    "    kw = {**zipstore_args_default, **zipstore_args}\n",
    "\n",
    "    if isinstance(dx, xr.Dataset):\n",
    "        dx = {None: dx}\n",
    "\n",
    "    fn = Path(output_file)\n",
    "    if not (fn.suffixes[-2] == \".zarr\" and fn.suffixes[-1] == \".zip\"):\n",
    "        fn = fn.with_suffix(\".zarr.zip\")\n",
    "\n",
    "    for k, ds in dx.items():\n",
    "        store = zarr.storage.ZipStore(fn, **kw)\n",
    "        ds.to_zarr(store, group=k, **zarr_args)\n",
    "        store.close()\n",
    "\n",
    "        # Any further group is to be updated/appended, by default\n",
    "        kw[\"mode\"] = \"a\"\n",
    "\n",
    "    if not valid_zip(fn):\n",
    "        logging.warning(\"File corrupt, removed: `%s`\", str(fn))\n",
    "        fn.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bbe99e-e584-465b-b4dd-9b64b1b3ed27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def datastore_conform(datastore_df, datastore_units):\n",
    "    import logging\n",
    "\n",
    "    import cfunits\n",
    "\n",
    "    cf_attr_dict = {\"production_level\": \"L1\"}\n",
    "    g_attr_dict = get_gattrs(gattrs)\n",
    "\n",
    "    # @dask.delayed\n",
    "    def datastore_conform_func(rowid, fn_series, rebase_p):\n",
    "        dx = []\n",
    "        p_list = []\n",
    "\n",
    "        # file paths\n",
    "        fn = Path(\n",
    "            *fn_series[\n",
    "                [\n",
    "                    (\"location\", \"path_base\"),\n",
    "                    (\"location\", \"path\"),\n",
    "                    (\"location\", \"file\"),\n",
    "                ]\n",
    "            ].tolist()\n",
    "        )\n",
    "        cache_fn = get_output_files(\n",
    "            None,\n",
    "            default_subset={\n",
    "                \"path\": os.path.join(cache_path_base, cache_path),\n",
    "                \"file\": cache_file,\n",
    "            },\n",
    "            custom_subset={\n",
    "                **{n[1]: v for n, v in fn_series.items() if n[0] not in [\"location\"]},\n",
    "                **cf_attr_dict,\n",
    "            },\n",
    "        )\n",
    "\n",
    "        # avoid reduncancy\n",
    "        if Path(cache_fn).is_file():\n",
    "            logging.warning(\"File exists: `%s`\", cache_fn)\n",
    "\n",
    "            # overwrite cache or not\n",
    "            if not query_latest:\n",
    "                p_list.append((rowid, Path(cache_fn)))\n",
    "                logging.warning(\"Skipping file: `%s`\", cache_fn)\n",
    "                return p_list\n",
    "            else:\n",
    "                logging.info(\"Replacing file: `%s`\", cache_fn)\n",
    "\n",
    "        store, store_args = datasets_fsspec_args(fn)\n",
    "        with xr.open_dataset(store, **store_args) as ds:\n",
    "            # ds = ds.drop_dims([k for k in [\"attributes\"] if k in ds.dims])\n",
    "            dx = ds.coords.to_dataset()\n",
    "\n",
    "            try:\n",
    "                s_ind = get_ds_index(ds)\n",
    "            except:\n",
    "                logging.exception(\"Unable to open NetCDF file `%s`.\", fn)\n",
    "\n",
    "            for v_name in list(ds.data_vars):\n",
    "                v_ind = {**s_ind, \"original_name\": v_name}\n",
    "                logging.debug(\"Index : %s\", tuple(v_ind.values()))\n",
    "                if tuple(v_ind.values()) in datastore_units.index:\n",
    "                    for _, c_ds in datastore_units.loc[\n",
    "                        [tuple(v_ind.values())], :\n",
    "                    ].iterrows():\n",
    "                        # print(c_ds)\n",
    "                        c_name = c_ds[(\"convention\", \"name\")]\n",
    "\n",
    "                        # data array for the variable\n",
    "                        da = ds[v_name].copy()\n",
    "\n",
    "                        # unit value conversion\n",
    "                        da.values = da.pipe(\n",
    "                            cfunits.Units.conform,\n",
    "                            from_units=c_ds.loc[\n",
    "                                (\"convention\", \"units_conversion_units_from\")\n",
    "                            ],\n",
    "                            to_units=c_ds.loc[\n",
    "                                (\"convention\", \"units_conversion_units_to\")\n",
    "                            ],\n",
    "                        )\n",
    "\n",
    "                        # assign data array to output dataset\n",
    "                        dx = dx.assign({c_name: da})\n",
    "\n",
    "                        # update variable attributes\n",
    "                        dx[c_name] = dx[c_name].assign_attrs(\n",
    "                            **{\n",
    "                                \"long_name\": c_ds.loc[(\"convention\", \"long_name\")],\n",
    "                                \"standard_name\": c_ds.loc[\n",
    "                                    (\"convention\", \"standard_name\")\n",
    "                                ],\n",
    "                                \"units\": str(\n",
    "                                    c_ds.loc[\n",
    "                                        (\n",
    "                                            \"convention\",\n",
    "                                            \"units_conversion_units_to\",\n",
    "                                        )\n",
    "                                    ]\n",
    "                                ),\n",
    "                                \"cell_methods\": c_ds.loc[\n",
    "                                    (\"convention\", \"cell_methods\")\n",
    "                                ],\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "            # consolidate output dataset\n",
    "            dx = xr.merge([dx[k] for k in sorted(list(dx.data_vars))])\n",
    "\n",
    "            # update attributes\n",
    "            sys.path.append(os.path.join(rebase_p, \"interfaces/metadb/src/\"))\n",
    "            from metadb_attributes import metadb_combine_globalattrs\n",
    "\n",
    "            gattrs_list = [ds.attrs, g_attr_dict]\n",
    "            gattrs_dict = metadb_combine_globalattrs(gattrs_list, module_base=rebase_p)\n",
    "            # dx = assign_attrs(gattrs_dict)\n",
    "            dx.attrs = gattrs_dict\n",
    "            dx = dx.assign_attrs(**cf_attr_dict)\n",
    "\n",
    "            # cleanup\n",
    "            # dx = dx.transpose(*[n for n in dim_list if n in list(dx.dims)])\n",
    "            dx = dx.transpose(\n",
    "                \"time\",\n",
    "                \"station\",\n",
    "                \"system\",\n",
    "                \"sensor\",\n",
    "                \"channel\",\n",
    "                \"cell\",\n",
    "                \"attributes\",\n",
    "                missing_dims=\"ignore\",\n",
    "            )\n",
    "            dx = dx.chunk()\n",
    "            dx = reset_coords(dx, sortby=dim_list)\n",
    "            dx = dx.drop_encoding()\n",
    "\n",
    "            # prepare\n",
    "            # dx = preprocess_nc(dx)\n",
    "\n",
    "            # export to cache\n",
    "            p = Path(cache_fn)\n",
    "            p.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            try:\n",
    "                store, store_args = datasets_fsspec_args(cache_fn)\n",
    "                if store_args[\"engine\"] == \"zarr\" and p.suffix == \".zip\":\n",
    "                    datasets_to_zarr_zip(dx, p)\n",
    "                    p_list.append((rowid, p))\n",
    "                else:\n",
    "                    dx.to_netcdf(str(p))\n",
    "                    p_list.append((rowid, p))\n",
    "            except:\n",
    "                # logging.info(\"Unable to write NetCDF file.\")\n",
    "                funcname = cast(types.FrameType, inspect.currentframe()).f_code.co_name\n",
    "                logging.exception(\n",
    "                    \"`%s`: unable to write NetCDF file in `%s`.\", funcname, str(p)\n",
    "                )\n",
    "\n",
    "        return p_list\n",
    "\n",
    "    # unit converstion meta data for each file group\n",
    "    dx = []\n",
    "    p_list = []\n",
    "    datastore_gb = datastore_df.groupby(\n",
    "        [\n",
    "            (\"encoded\", \"station_id\"),\n",
    "            (\"decoded\", \"system_group\"),\n",
    "            (\"decoded\", \"system_id\"),\n",
    "        ]\n",
    "    )\n",
    "    for datastore_id, datastore in tqdm(datastore_gb, desc=\"Groupby loop\"):\n",
    "        logging.info(\"Group `%s` Count `%s`\", datastore_id, str(datastore.shape[0]))\n",
    "\n",
    "        if datastore.shape[0] < 2:\n",
    "            store_gb = datastore.iterrows()\n",
    "        else:\n",
    "            store_gb = tqdm(\n",
    "                datastore.iterrows(), desc=\"Datastore loop\", total=len(datastore)\n",
    "            )\n",
    "        for rowid, fn_series in store_gb:\n",
    "            p_list.append(datastore_conform_func(rowid, fn_series, rebase_path()))\n",
    "\n",
    "    # dask.compute(p_list)\n",
    "\n",
    "    return p_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcfc348-3f90-41b6-8e1e-ef7b1fb3a84a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def datastore_concat(datastore_df, debug=False):\n",
    "    import logging\n",
    "    from copy import deepcopy\n",
    "\n",
    "    import cfunits\n",
    "\n",
    "    cf_attr_dict = {\"production_level\": \"L1\"}\n",
    "\n",
    "    # unit converstion meta data for each file group\n",
    "    dx = []\n",
    "    dx_keys = []\n",
    "    p_list = []\n",
    "    datastore_gb = datastore_df.groupby(\n",
    "        [\n",
    "            (\"encoded\", \"station_id\"),\n",
    "            (\"decoded\", \"system_group\"),\n",
    "            (\"decoded\", \"system_id\"),\n",
    "        ]\n",
    "    )\n",
    "    for datastore_id, datastore in tqdm(datastore_gb, desc=\"Groupby loop\"):\n",
    "        logging.info(\"Group `%s` Count `%s`\", datastore_id, str(datastore.shape[0]))\n",
    "        file_dx = []\n",
    "        file_attrs = []\n",
    "        file_key = datastore_id\n",
    "        for rowid, fn_series in tqdm(\n",
    "            datastore.iterrows(), desc=\"Datastore loop\", total=len(datastore)\n",
    "        ):\n",
    "            # file paths\n",
    "            fn = Path(\n",
    "                *fn_series[\n",
    "                    [\n",
    "                        (\"location\", \"path_base\"),\n",
    "                        (\"location\", \"path\"),\n",
    "                        (\"location\", \"file\"),\n",
    "                    ]\n",
    "                ].tolist()\n",
    "            )\n",
    "\n",
    "            # avoid reduncancy\n",
    "            if not Path(fn).is_file():\n",
    "                logging.info(\"File exists\")\n",
    "                # continue\n",
    "\n",
    "            try:\n",
    "                store, store_args = datasets_fsspec_args(fn)\n",
    "                ds = xr.open_dataset(store, **store_args, mode=\"r\")\n",
    "                # ds = xr.open_dataset(fn)\n",
    "                reduce_kwarg = (\n",
    "                    query_settings[\"concat_reduce\"]\n",
    "                    if \"concat_reduce\" in query_settings\n",
    "                    else dict(\n",
    "                        drop_vars=[\"system_id\", \"system_name\"], drop_dims=[\"channel\"]\n",
    "                    )\n",
    "                )\n",
    "                if \"attributes\" in ds.dims:\n",
    "                    ds = ds.drop_dims([\"attributes\"])\n",
    "                if \"channel\" in ds.dims:\n",
    "                    ds = ds.mean(dim=\"channel\")\n",
    "                ds = preprocess_reduce(ds, **reduce_kwarg)\n",
    "                if \"time\" in ds.dims and \"time\" in ds.variables:\n",
    "                    file_dx.append(ds)\n",
    "                    file_attrs.append(deepcopy(ds.attrs))\n",
    "            except:\n",
    "                logging.info(\"File could not be opened by `xarray`: '%s'\", fn)\n",
    "\n",
    "        # concatenate\n",
    "        if not file_dx:\n",
    "            continue\n",
    "\n",
    "        dx_dict[file_key] = xr.concat(\n",
    "            file_dx, dim=\"time\", combine_attrs=\"drop_conflicts\"\n",
    "        )\n",
    "\n",
    "        # fix/simplifiy coords, drop dim channel\n",
    "        ds = dx_dict[file_key]\n",
    "\n",
    "        # time\n",
    "        try:\n",
    "            _, time_index = np.unique(ds[\"time\"], return_index=True)\n",
    "            ds = ds.isel(time=time_index)\n",
    "            ds = ds.sortby(\"time\")\n",
    "            ds.time.encoding.update(**enc_conf[\"time\"])\n",
    "        except:\n",
    "            logging.info(\"Skipping `concat` for group `%s`\", str(datastore_id))\n",
    "            continue\n",
    "\n",
    "        # time helper variable\n",
    "        time_bounds = pd.Index(\n",
    "            [\n",
    "                (\n",
    "                    query_range[0]\n",
    "                    if np.any((ds[\"time\"] < query_range[0]).any().values)\n",
    "                    else pd.to_datetime(ds[\"time\"].min().values)\n",
    "                ),\n",
    "                (\n",
    "                    query_range[1]\n",
    "                    if np.any((ds[\"time\"] > query_range[1]).any().values)\n",
    "                    else pd.to_datetime(ds[\"time\"].max().values)\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Slice `time` to query range bounds. Note: excluded here, only within the combine routine.\n",
    "        # ds = ds.sel(time=slice(time_bounds[0],time_bounds[1]))\n",
    "\n",
    "        # update attributes\n",
    "        gattrs_list = file_attrs\n",
    "        gattrs_dict = metadb_combine_globalattrs(gattrs_list)\n",
    "        ds.attrs = deepcopy(gattrs_dict)\n",
    "\n",
    "        # chunks\n",
    "        # ds = ds.chunk({\"time\": 3600, \"station\": 1, \"system\": 1})\n",
    "\n",
    "        # debug\n",
    "        # ds.to_netcdf('/tmp/{}_{}_{}.nc'.format(*datastore_id))\n",
    "\n",
    "        # pre-export cleanup\n",
    "        try:\n",
    "            ds = preprocess_attrs(ds)\n",
    "            ds = preprocess_merge(ds)\n",
    "            ds = preprocess_nc(ds)\n",
    "            # ds = ds.transpose(*[n for n in dim_list if n in list(ds.dims)])\n",
    "            ds = ds.transpose(\n",
    "                \"time\",\n",
    "                \"station\",\n",
    "                \"system\",\n",
    "                \"sensor\",\n",
    "                \"channel\",\n",
    "                \"cell\",\n",
    "                \"attributes\",\n",
    "                missing_dims=\"ignore\",\n",
    "            )\n",
    "            ds = ds.chunk()\n",
    "            ds = reset_coords(ds, sortby=dim_list)\n",
    "        except Exception as Argument:\n",
    "            logging.exception(\n",
    "                \"Unable to restructure `datastore_concat` results for `%s`.\",\n",
    "                str(file_key),\n",
    "            )\n",
    "            logging.info(\"%s\", Argument)\n",
    "\n",
    "        # store concatenated result in (temporary) file\n",
    "        if nc_temp:\n",
    "            nc_path = nc_temp.name\n",
    "            cache_fn = get_output_files(\n",
    "                ds,\n",
    "                default_subset={\n",
    "                    \"path\": os.path.join(nc_path, cache_path),\n",
    "                    \"file\": cache_file,\n",
    "                },\n",
    "                custom_subset=reduce(\n",
    "                    lambda a, b: {**a, **b},\n",
    "                    [\n",
    "                        {\n",
    "                            n[1]: v\n",
    "                            for n, v in fn_series.items()\n",
    "                            if n[0] not in [\"location\"]\n",
    "                        },\n",
    "                        cf_attr_dict,\n",
    "                        {\"time_bounds\": get_time_bounds(time_bounds)},\n",
    "                    ],\n",
    "                ),\n",
    "            )\n",
    "\n",
    "            # export to cache, force this stage to be NetCDF4 to shorten time\n",
    "            p = Path(cache_fn)\n",
    "            if p.suffix == \".zip\":\n",
    "                p = p.with_suffix(\"\")\n",
    "            if p.suffix == \".zarr\":\n",
    "                p = p.with_suffix(\".nc\")\n",
    "\n",
    "            p.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            try:\n",
    "                store, store_args = datasets_fsspec_args(str(p))\n",
    "                if store_args[\"engine\"] == \"zarr\" and p.suffix == \".zip\":\n",
    "                    datasets_to_zarr_zip(ds, store)\n",
    "                elif store_args[\"engine\"] == \"zarr\" and p.suffix == \".zarr\":\n",
    "                    ds.to_zarr(str(store))\n",
    "                else:\n",
    "                    with warnings.catch_warnings():\n",
    "                        warnings.simplefilter(\"ignore\")\n",
    "                        ds.to_netcdf(str(p))\n",
    "\n",
    "                if debug:\n",
    "                    p_debug = Path(os.path.join(\"../data/tmp/\", p.name))\n",
    "                    shutil.copy(\n",
    "                        p,\n",
    "                        p_debug,\n",
    "                    )\n",
    "                p_list.append((rowid, p))\n",
    "            except:\n",
    "                # logging.exception(\"Unable to write NetCDF file `%s`.\",str((rowid, p)))\n",
    "                funcname = cast(types.FrameType, inspect.currentframe()).f_code.co_name\n",
    "                logging.exception(\n",
    "                    \"`%s`: unable to write NetCDF file in `%s`.\", funcname, str(p)\n",
    "                )\n",
    "\n",
    "    return p_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2970200c-c284-4915-8ebe-abb703ec07d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def datastore_combine(datastore_df):\n",
    "    import logging\n",
    "    import uuid\n",
    "    from copy import deepcopy\n",
    "\n",
    "    import cfunits\n",
    "\n",
    "    def store_export(ds, p, store_path=None, store_mode=\"a\"):\n",
    "        \"\"\"helper fuction for export of datasets\"\"\"\n",
    "        p = Path(p)\n",
    "\n",
    "        # DEBUG: export to cache, force this stage to be NetCDF4\n",
    "        # if p.suffix == \".zip\":\n",
    "        #    p = p.with_suffix('')\n",
    "        # if p.suffix == \".zarr\":\n",
    "        #    p = p.with_suffix('.nc')\n",
    "\n",
    "        p.parent.mkdir(parents=True, exist_ok=True)\n",
    "        try:\n",
    "            store, store_args = datasets_fsspec_args(str(p))\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\")\n",
    "                if store_args[\"engine\"] == \"zarr\" and p.suffix == \".zip\":\n",
    "                    datasets_to_zarr_zip(\n",
    "                        {store_path: ds},\n",
    "                        p,\n",
    "                        zipstore_args={\"mode\": store_mode},\n",
    "                        zarr_version=2,\n",
    "                    )\n",
    "                elif p.suffix == \".nc\":\n",
    "                    ds.to_netcdf(path=str(p), mode=store_mode, group=store_path)\n",
    "        except:\n",
    "            # logging.info(\"Unable to write NetCDF file.\")\n",
    "            funcname = cast(types.FrameType, inspect.currentframe()).f_code.co_name\n",
    "            logging.exception(\n",
    "                \"`%s`: unable to write to file in `%s`.\", funcname, str(p)\n",
    "            )\n",
    "\n",
    "    def store_pathname(dx, datastore_df, time_bounds, fn_series):\n",
    "        globloc_str = (\n",
    "            dx.attrs[\"network\"]\n",
    "            if \"network\" in dx.attrs\n",
    "            else datastore_df[(\"pattern\", \"global_location\")]\n",
    "            .str.split(\".\")\n",
    "            .apply(lambda x: \".\".join(x[0:-1]))\n",
    "            .unique()\n",
    "            .tolist()[0]\n",
    "        )\n",
    "\n",
    "        nc_attr_dict = {\n",
    "            \"production_level\": dx.attrs[\"production_level\"],\n",
    "            \"global_location\": globloc_str,\n",
    "            \"station_id\": \"\",\n",
    "            \"sensor_id\": \"\",\n",
    "        }\n",
    "\n",
    "        output_fn = get_output_files(\n",
    "            dx,\n",
    "            default_subset={\n",
    "                \"path\": os.path.join(output_path_base, output_path),\n",
    "                \"file\": output_file,\n",
    "            },\n",
    "            custom_subset=reduce(\n",
    "                lambda a, b: {**a, **b},\n",
    "                [\n",
    "                    {n[1]: v for n, v in fn_series.items() if n[0] not in [\"location\"]},\n",
    "                    nc_attr_dict,\n",
    "                    {\"time_bounds\": get_time_bounds(time_bounds)},\n",
    "                ],\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        return output_fn\n",
    "\n",
    "    def store_copy(p, output_fn):\n",
    "        if p.is_file():\n",
    "            try:\n",
    "                logging.info(\"Source     : `%s`\", str(p))\n",
    "                output_p = Path(output_fn).with_suffix(\"\".join(p.suffixes))\n",
    "                logging.info(\"Destination: `%s`\", str(output_p))\n",
    "                output_p.parent.mkdir(parents=True, exist_ok=True)\n",
    "                shutil.copy(\n",
    "                    p,\n",
    "                    output_p,\n",
    "                )\n",
    "\n",
    "                # double check output strucure\n",
    "                if p.suffixes == [\".zarr\", \".zip\"]:\n",
    "                    logging.info(\"Summary report\")\n",
    "                    store, store_args = datasets_fsspec_args(str(p))\n",
    "                    dz = zarr.open(store, mode=\"r\")\n",
    "                    print(dz.tree())  # instead of display()\n",
    "\n",
    "                return output_p\n",
    "            except:\n",
    "                # logging.info(\"Unable to write NetCDF file.\")\n",
    "                funcname = cast(types.FrameType, inspect.currentframe()).f_code.co_name\n",
    "                logging.exception(\n",
    "                    \"`%s`: unable to write to file in `%s`.\", funcname, str(p)\n",
    "                )\n",
    "\n",
    "    # parameters\n",
    "    cf_attr_dict = {\"production_level\": \"L1\"}\n",
    "\n",
    "    # unit converstion meta data for each file group\n",
    "    dx_keys = []\n",
    "    dx = []\n",
    "    p_list = []\n",
    "    datastore_gb = datastore_df.groupby(\n",
    "        [\n",
    "            (\"encoded\", \"station_id\"),\n",
    "            (\"decoded\", \"system_group\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # store `time` concatenated results in (temporary) files\n",
    "    if nc_temp:\n",
    "        store_path = nc_temp.name\n",
    "        store_file = \"{}.{}\".format(\n",
    "            uuid.uuid4().hex, input_subset[\"extension\"]\n",
    "        )  # nc or zarr.zip\n",
    "        store_temp = Path(*[store_path, store_file])\n",
    "        store_mode = \"w\"\n",
    "\n",
    "        logging.info(\"Temporary file `%s`\", store_file)\n",
    "\n",
    "        nc_attrs = []\n",
    "\n",
    "        for datastore_id, datastore in tqdm(datastore_gb, desc=\"Groupby loop\"):\n",
    "            file_dx = []\n",
    "            file_attrs = []\n",
    "            file_key = datastore_id\n",
    "\n",
    "            logging.info(\"Group `%s` Count `%s`\", datastore_id, str(datastore.shape[0]))\n",
    "\n",
    "            for rowid, fn_series in tqdm(\n",
    "                datastore.iterrows(), desc=\"Datastore loop\", total=len(datastore)\n",
    "            ):\n",
    "                # file paths\n",
    "                fn = Path(\n",
    "                    *fn_series[\n",
    "                        [\n",
    "                            (\"location\", \"path_base\"),\n",
    "                            (\"location\", \"path\"),\n",
    "                            (\"location\", \"file\"),\n",
    "                        ]\n",
    "                    ].tolist()\n",
    "                )\n",
    "\n",
    "                try:\n",
    "                    store, store_args = datasets_fsspec_args(fn)\n",
    "                    ds = xr.open_dataset(store, **store_args)\n",
    "                    file_dx.append(ds)\n",
    "                    file_attrs.append(deepcopy(ds.attrs))\n",
    "                except:\n",
    "                    logging.info(\"File could not be opened by `xarray`: '%s'\", fn)\n",
    "\n",
    "            # validate units before merge (todo)\n",
    "\n",
    "            # combine, merge, store results in cache\n",
    "            try:\n",
    "                ds = xr.merge(\n",
    "                    [preprocess_merge(d) for d in file_dx],\n",
    "                    combine_attrs=\"drop_conflicts\",\n",
    "                )\n",
    "                ds = postprocess_merge(ds)\n",
    "\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            # time coordinate needs to sorted, unique\n",
    "            try:\n",
    "                _, time_index = np.unique(ds[\"time\"], return_index=True)\n",
    "                ds = ds.isel(time=time_index)\n",
    "                ds = ds.sortby(\"time\")\n",
    "                ds.time.encoding.update(**enc_conf[\"time\"])\n",
    "            except:\n",
    "                logging.info(\"Skipping `concat` for group `%s`\", str(datastore_id))\n",
    "                continue\n",
    "\n",
    "            # trim `time` to query bounds\n",
    "            ds = ds.sel(time=slice(query_range[0], query_range[1]))\n",
    "\n",
    "            # chunks\n",
    "            # ds = ds.chunk({\"time\": 3600, \"station\": 1, \"system\": 1})\n",
    "\n",
    "            # update attributes\n",
    "            gattrs_list = file_attrs\n",
    "            gattrs_dict = metadb_combine_globalattrs(gattrs_list)\n",
    "            ds.attrs = gattrs_dict\n",
    "            nc_attrs.append(deepcopy(gattrs_dict))\n",
    "\n",
    "            # collect coordinates\n",
    "            dx_keys.append(file_key)\n",
    "            dx_dict[file_key] = ds.coords.to_dataset()\n",
    "\n",
    "            # pre-export cleanup\n",
    "            ds = preprocess_attrs(ds)\n",
    "            ds = preprocess_merge(ds)\n",
    "            ds = postprocess_merge(ds)\n",
    "            # ds = ds.transpose(*[n for n in dim_list if n in list(ds.dims)])\n",
    "            ds = ds.transpose(\n",
    "                \"time\",\n",
    "                \"station\",\n",
    "                \"system\",\n",
    "                \"sensor\",\n",
    "                \"channel\",\n",
    "                \"cell\",\n",
    "                \"attributes\",\n",
    "                missing_dims=\"ignore\",\n",
    "            )\n",
    "            ds = ds.chunk()\n",
    "            ds = reset_coords(ds, sortby=dim_list)\n",
    "            ds = ds.drop_encoding()\n",
    "\n",
    "            # export to cache\n",
    "            store_root = file_key[0]\n",
    "            store_export(\n",
    "                ds,\n",
    "                store_temp,\n",
    "                store_path=store_root,\n",
    "                store_mode=store_mode,\n",
    "            )\n",
    "\n",
    "            # update mode for subequent loop items\n",
    "            store_mode = \"a\"\n",
    "\n",
    "        # merge coorindates\n",
    "        dx = xr.merge(\n",
    "            [preprocess_merge(dx_dict[k]) for k in dx_keys if k in list(dx_dict)],\n",
    "            compat=\"override\",\n",
    "        )\n",
    "\n",
    "        # merge attributes\n",
    "        gattrs_list = nc_attrs\n",
    "        gattrs_dict = metadb_combine_globalattrs(gattrs_list)\n",
    "        dx.attrs = deepcopy(gattrs_dict)\n",
    "\n",
    "        # time\n",
    "        try:\n",
    "            _, time_index = np.unique(dx[\"time\"], return_index=True)\n",
    "            dx = dx.isel(time=time_index)\n",
    "            dx = dx.sortby(\"time\")\n",
    "            dx.time.encoding.update(**enc_conf[\"time\"])\n",
    "        except:\n",
    "            logging.exception(\"Unable to sort `time`.\")\n",
    "\n",
    "        # time string helper variable\n",
    "        time_bounds = pd.Index(\n",
    "            [\n",
    "                (\n",
    "                    query_range[0]\n",
    "                    if np.any((dx[\"time\"] < query_range[0]).any().values)\n",
    "                    else pd.to_datetime(dx[\"time\"].min().values)\n",
    "                ),\n",
    "                (\n",
    "                    query_range[1]\n",
    "                    if np.any((dx[\"time\"] > query_range[1]).any().values)\n",
    "                    or query_latest\n",
    "                    else pd.to_datetime(dx[\"time\"].max().values)\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        # clip `time` ...needed?\n",
    "        dx = dx.sel(time=slice(time_bounds[0], time_bounds[1]))\n",
    "\n",
    "        # prepare export\n",
    "        dx = preprocess_merge(dx)\n",
    "        dx = postprocess_merge(dx)\n",
    "        dx = dx.drop_encoding()\n",
    "\n",
    "        # export merged result to cache\n",
    "        store_root = datastore_df[(\"decoded\", \"location_city\")].unique().tolist()[0]\n",
    "        store_export(\n",
    "            dx,\n",
    "            store_temp,\n",
    "            store_path=store_root,\n",
    "            store_mode=store_mode,\n",
    "        )\n",
    "\n",
    "        # final destination output, copy from cache\n",
    "        logging.info(\"Copy `combine` store to final output destination.\")\n",
    "        store_output = store_pathname(dx, datastore_df, time_bounds, fn_series)\n",
    "        store_output = store_copy(store_temp, store_output)\n",
    "\n",
    "        return store_temp, store_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741ca299-eee3-4324-8d5c-5903bd91f3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def datastore_cube(input_file_list, cache_file_list=[], output_file_list=[]):\n",
    "\n",
    "    def get_store_temporary():\n",
    "        if nc_temp:\n",
    "            store_path = nc_temp.name\n",
    "        else:\n",
    "            store_path = Path(\"/tmp/\")\n",
    "\n",
    "        store_file = \"{}.{}\".format(\n",
    "            uuid.uuid4().hex, input_subset[\"extension\"]\n",
    "        )  # nc or zarr.zip\n",
    "        store_temp = Path(*[store_path, store_file])\n",
    "\n",
    "        logging.info(\"Temporary file `%s`\", store_temp)\n",
    "        return store_temp\n",
    "\n",
    "    def get_store_groups(fn, data_vars=[\"ta\"], groups=None):\n",
    "        \"\"\"read and subset helper function\"\"\"\n",
    "        store, store_args = datasets_fsspec_args(str(fn))\n",
    "\n",
    "        store_groups = [\n",
    "            n for n in list(zarr.open(store, mode=\"r\").group_keys()) if len(n) != 2\n",
    "        ]\n",
    "\n",
    "        if groups and isinstance(groups, list):\n",
    "            store_groups = [n for n in store_groups if n in groups]\n",
    "\n",
    "        if not store_groups:\n",
    "            store_groups = [None]\n",
    "\n",
    "        ds_dict = {\n",
    "            n: xr.open_dataset(store, **store_args, group=n, mode=\"r\")\n",
    "            for n in store_groups\n",
    "        }\n",
    "\n",
    "        if data_vars == []:\n",
    "            data_vars = list(\n",
    "                dict.fromkeys(\n",
    "                    [\n",
    "                        item\n",
    "                        for sublist in [list(ds.data_vars) for ds in ds_dict.values()]\n",
    "                        for item in sublist\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "\n",
    "        if data_vars:\n",
    "            ds_dict = {\n",
    "                n: d[[v for v in data_vars if v in list(d.data_vars)]]\n",
    "                for n, d in ds_dict.items()\n",
    "                if any([v in list(d.data_vars) for v in data_vars])\n",
    "            }\n",
    "        else:\n",
    "            ds_dict = {n: d.coords.to_dataset() for n, d in ds_dict.items()}\n",
    "\n",
    "        return ds_dict, data_vars, store_groups\n",
    "\n",
    "    def cache_store_groups(dx, cache_path=\"tmp/tmp_dev/\"):\n",
    "\n",
    "        import uuid\n",
    "\n",
    "        store_list = []\n",
    "        mode = \"w\"\n",
    "        for gp, ds in dx.items():\n",
    "            if isinstance(ds, xr.Dataset):\n",
    "                ds = preprocess_store(ds)\n",
    "                ds = ds.transpose(\n",
    "                    \"time\",\n",
    "                    \"station\",\n",
    "                    \"system\",\n",
    "                    \"sensor\",\n",
    "                    \"channel\",\n",
    "                    \"cell\",\n",
    "                    \"attributes\",\n",
    "                    missing_dims=\"ignore\",\n",
    "                )\n",
    "                ds = ds.chunk({\"time\": \"auto\"})\n",
    "                ds = ds.drop_encoding()\n",
    "\n",
    "                # export\n",
    "                cache_file = os.path.join(cache_path, f\"{gp}_{uuid.uuid4().hex}.zarr\")\n",
    "                store = Path(cache_file)\n",
    "                store_args = dict(group=gp, mode=mode)\n",
    "                ds.to_zarr(store, **store_args)\n",
    "\n",
    "                # file mode\n",
    "                if Path(cache_file).is_dir():\n",
    "                    mode = \"a\"\n",
    "\n",
    "                # report\n",
    "                store_list.append((store, gp))\n",
    "\n",
    "        return store_list\n",
    "\n",
    "    def combine_store_groups(ifn, data_vars=[], cache=None):\n",
    "\n",
    "        # lazyload\n",
    "        dc_dict, _, _ = get_store_groups(ifn, None, None)\n",
    "        ds_dict, data_vars, store_groups = get_store_groups(ifn, data_vars, None)\n",
    "\n",
    "        dc_list = [\n",
    "            v for k, v in dc_dict.items() if isinstance(v, xr.Dataset)\n",
    "        ]  # only coordinates\n",
    "        ds_list = [\n",
    "            v.close() for k, v in ds_dict.items() if isinstance(v, xr.Dataset)\n",
    "        ]  # if variables\n",
    "\n",
    "        # coordindates\n",
    "        dc = merge_reduce([preprocess_merge(d.copy()) for d in dc_list])\n",
    "\n",
    "        # global attributes\n",
    "\n",
    "        # data_vars\n",
    "        dx = {}\n",
    "        for dv in tqdm(data_vars, desc=\"data_vars\"):\n",
    "            dx[dv] = None\n",
    "            for sg in tqdm(store_groups, desc=\"store groups\"):\n",
    "                if not isinstance(dx[dv], xr.Dataset):\n",
    "                    # _, dx = xr.align(preprocess_merge(dc),preprocess_merge(ds),join='left')\n",
    "                    dx[dv] = dc.copy()\n",
    "\n",
    "                ds_dict, _, _ = get_store_groups(ifn, [dv], [sg])\n",
    "                ds_list = [\n",
    "                    v for k, v in ds_dict.items() if v and isinstance(v, xr.Dataset)\n",
    "                ]\n",
    "                for ds in ds_list:\n",
    "                    dx[dv] = merge_reduce([dx[dv], preprocess_merge(ds)])\n",
    "                    ds.close()\n",
    "\n",
    "            if cache:\n",
    "                dx[dv] = cache_store_groups({dv: dx[dv]}, cache_path=cache)\n",
    "                ds_list = [d.close() for d in ds_list]\n",
    "\n",
    "        return dx\n",
    "\n",
    "    def merge_store_groups_cache(cache_file_list, idx=None, replace=False):\n",
    "        \"\"\"read and reduce helper function\"\"\"\n",
    "        if not isinstance(cache_file_list, list):\n",
    "            bool_single = True\n",
    "            cache_file_list = [cache_file_list]\n",
    "        else:\n",
    "            bool_single = False\n",
    "\n",
    "        # list cache\n",
    "        if not idx:\n",
    "            idx = {}\n",
    "            for cfn in cache_file_list:\n",
    "                for store in sorted(glob.glob(cfn + \"/*.zarr\")):\n",
    "                    if not cfn in idx:\n",
    "                        idx[cfn] = {}\n",
    "                    gp = \"_\".join(Path(store).name.split(\"_\")[:-1])\n",
    "                    idx[cfn][gp] = [(store, gp)]\n",
    "\n",
    "        # merge, write\n",
    "        ds_list = []\n",
    "        for cfn in idx.keys():\n",
    "\n",
    "            dx = []\n",
    "            for vn, pn in idx[cfn].items():\n",
    "                for vfn, gp in pn:\n",
    "                    store, store_args = datasets_fsspec_args(str(vfn))\n",
    "\n",
    "                    ds = xr.open_dataset(store, **store_args, group=gp, mode=\"r\")\n",
    "                    # ds = ds.drop_encoding()\n",
    "                    ds = ds.chunk()\n",
    "                    dx.append(ds)\n",
    "\n",
    "                    # print(f\"{vn} {ds['system_id'].values}\")\n",
    "\n",
    "            # try:\n",
    "            #    ds = merge_reduce([preprocess_merge(ds) for ds in dx])\n",
    "            # except:\n",
    "            #    return ds\n",
    "            ds = merge_reduce([preprocess_merge(ds) for ds in dx])\n",
    "\n",
    "            if isinstance(ds, xr.Dataset):\n",
    "                print(\"merge\")\n",
    "                ds = postprocess_merge(ds)\n",
    "                print(\"sort\")\n",
    "                ds = ds.sortby(\"station_id\")\n",
    "                print(\"encoding\")\n",
    "                ds = ds.drop_encoding()\n",
    "                print(\"chunk\")\n",
    "                ds = ds.chunk({\"time\": 2000000, \"station\": 100, \"system\": 1})\n",
    "                print(\"transpose\")\n",
    "                ds = ds.transpose(\n",
    "                    \"time\",\n",
    "                    \"station\",\n",
    "                    \"system\",\n",
    "                    \"sensor\",\n",
    "                    \"channel\",\n",
    "                    \"cell\",\n",
    "                    \"attributes\",\n",
    "                    missing_dims=\"ignore\",\n",
    "                )\n",
    "                print(ds.chunksizes)\n",
    "                print(\"final\")\n",
    "                ds_list.append(ds)\n",
    "\n",
    "        if bool_single:\n",
    "            return ds_list[0]\n",
    "        else:\n",
    "            return ds_list\n",
    "\n",
    "    def get_store_temporary():\n",
    "        if nc_temp:\n",
    "            store_path = nc_temp.name\n",
    "        else:\n",
    "            store_path = Path(\"/tmp/\")\n",
    "\n",
    "        store_file = \"{}.{}\".format(\n",
    "            uuid.uuid4().hex, input_subset[\"extension\"]\n",
    "        )  # nc or zarr.zip\n",
    "        store_temp = Path(*[store_path, store_file])\n",
    "\n",
    "        logging.info(\"Temporary file `%s`\", store_temp)\n",
    "        return store_temp\n",
    "\n",
    "    idx = {}\n",
    "\n",
    "    if not isinstance(cache_file_list, list):\n",
    "        store_input_list = [cache_file_list]\n",
    "\n",
    "    cache_path = nc_temp.name\n",
    "    cache_file_list = [\n",
    "        str(PurePath(cache_path, strip_extension(Path(fn).stem)))\n",
    "        for fn in input_file_list\n",
    "    ]\n",
    "\n",
    "    if not output_file_list:\n",
    "        output_path = \"data/L1/\"\n",
    "        output_file_list = [\n",
    "            str(PurePath(output_path, Path(fn).name)) for fn in input_file_list\n",
    "        ]\n",
    "        output_file_list = [\n",
    "            get_store_temporary() if Path(fn).is_file() else fn\n",
    "            for fn in output_file_list\n",
    "        ]\n",
    "        # output_file_list = [get_store_temporary() for n in cache_file_list]\n",
    "\n",
    "    # prepare cache\n",
    "    for ifn, cfn in zip(input_file_list, cache_file_list):\n",
    "        idx[cfn] = combine_store_groups(ifn, data_vars=[], cache=cfn)\n",
    "\n",
    "    store_file_list = []\n",
    "    for ifn, cfn, ofn in zip(input_file_list, cache_file_list, output_file_list):\n",
    "        ds = merge_store_groups_cache(cfn)\n",
    "\n",
    "        store_root = list(set([n[0:2] for n in ds.station_id.values.tolist()]))[0]\n",
    "        #shutil.copy(ifn, ofn)\n",
    "        datasets_to_zarr_zip(\n",
    "            {store_root: ds},\n",
    "            ofn,\n",
    "            zipstore_args={\"mode\": \"w\"},\n",
    "            mode=\"w\",\n",
    "        )\n",
    "        store_file_list.append(ofn)\n",
    "\n",
    "    return tuple([store_file_list, output_file_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59085252-a12b-4fd0-9590-b1b4ea5d434e",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946f3a63-a5b0-4def-9c4a-4594f0089f1e",
   "metadata": {},
   "source": [
    "## Static Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac5a16d-d7b8-43fa-8ebc-e2356b132f34",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Version\n",
    "version = {\n",
    "    \"id\": \"v1.0.0\",\n",
    "    \"time\": \"2023-03-29\",\n",
    "}  # first version.\n",
    "version = {\n",
    "    \"id\": \"v1.0.1\",\n",
    "    \"time\": \"2023-09-27\",\n",
    "}  # updated version.\n",
    "version = {\n",
    "    \"id\": \"v1.0.2\",\n",
    "    \"time\": \"2025-02-21\",\n",
    "}  # updated version.\n",
    "\n",
    "\n",
    "# Configuration file for input / output files\n",
    "ioconfig_name = \"datasets_conjoin\"\n",
    "try:\n",
    "    ioconfig_file = \"../conf/{}.toml\".format(ioconfig_name)\n",
    "    if not Path(ioconfig_file).exists():\n",
    "        raise\n",
    "except:\n",
    "    ioconfig_file = \"conf/{}.toml\".format(ioconfig_name)\n",
    "\n",
    "# ----- Papermill injection below this cell -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c3c7f8-30dc-4824-9d9c-c2a856022033",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# input/output config\n",
    "ioconf = parse_config(ioconfig_file, ioconfig_name, version)\n",
    "\n",
    "# validate config (to do)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4e7b39-0158-497a-9998-3de144ca7ed8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Note: the approach to set global helper variables should be revised. \n",
    "But was/is used in combination with papermill automation.\n",
    "\"\"\"\n",
    "\n",
    "# set global variables\n",
    "query_from = ioconf[\"query\"][\"start\"]\n",
    "query_to = None if not \"end\" in ioconf[\"query\"] else ioconf[\"query\"][\"end\"]\n",
    "query_period = ioconf[\"query\"][\"period\"]\n",
    "query_index = ioconf[\"query\"][\"system_index\"]\n",
    "query_latest = ioconf[\"query\"][\"latest\"]\n",
    "query_cache = ioconf[\"query\"][\"cache\"]\n",
    "query_dask = ioconf[\"query\"][\"dask\"]\n",
    "query_tasks = ioconf[\"query\"][\"tasks\"]\n",
    "query_settings = (\n",
    "    {} if not \"settings\" in ioconf[\"query\"] else ioconf[\"query\"][\"settings\"]\n",
    ")\n",
    "\n",
    "input_path_base = ioconf[\"input\"][\"path_base\"]\n",
    "input_path = ioconf[\"input\"][\"path\"]\n",
    "input_file = ioconf[\"input\"][\"file\"]\n",
    "input_subset = ioconf[\"input\"][\"subset\"]\n",
    "cache_path_base = ioconf[\"cache\"][\"path_base\"]\n",
    "cache_path = ioconf[\"cache\"][\"path\"]\n",
    "cache_file = ioconf[\"cache\"][\"file\"]\n",
    "output_path_base = ioconf[\"output\"][\"path_base\"]\n",
    "output_path = ioconf[\"output\"][\"path\"]\n",
    "output_file = ioconf[\"output\"][\"file\"]\n",
    "\n",
    "log_path = ioconf[\"logging\"][\"path\"]\n",
    "log_file = ioconf[\"logging\"][\"file\"]\n",
    "log_format = ioconf[\"logging\"][\"format\"]\n",
    "log_filemode = (\n",
    "    \"a\" if not \"filemode\" in ioconf[\"logging\"] else ioconf[\"logging\"][\"filemode\"]\n",
    ")\n",
    "\n",
    "gattrs = ioconf[\"gattrs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d15ad43-dfa4-42d3-ada3-d1457cb9ab6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fde73306-db07-4ecd-bf9e-5a64e4fef346",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Logging configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d610e76-612f-4e70-940a-1ae657b20b89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create logger\n",
    "import logging\n",
    "import logging.handlers\n",
    "from pprint import pformat\n",
    "\n",
    "logging.basicConfig(\n",
    "    encoding=\"utf-8\",\n",
    "    format=log_format,\n",
    "    level=logging.INFO,\n",
    "    # Declare handlers\n",
    "    handlers=[\n",
    "        logging.FileHandler(\n",
    "            os.path.join(log_path, log_file).format(version_id=version[\"id\"]),\n",
    "            mode=log_filemode,\n",
    "        ),\n",
    "        # logging.StreamHandler(sys.stdout),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71b0f06-caef-4eb8-b9b1-7f6930991d1f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dynamic Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8fa490-8355-4bf6-8e96-126a4672d4a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# summarize\n",
    "logging.info(\"`ioconf` file: %s\", ioconfig_file)\n",
    "logging.info(\n",
    "    \"`ioconf` dict:\\n# start of item\\n%s\\n# end of item\\n\",\n",
    "    pformat(ioconf, sort_dicts=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915a01b9-f76b-4bdf-bdf4-1d63857a25f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reset structure\n",
    "dim_list = [\"time\", \"station\", \"system\", \"sensor\", \"channel\", \"cell\", \"attributes\"]\n",
    "enc_conf = {\n",
    "    \"time\": {\n",
    "        \"units\": \"nanoseconds since 1970-01-01 00:00:00\",  # \" +0000\"\n",
    "        \"calendar\": \"proleptic_gregorian\",\n",
    "    }\n",
    "}\n",
    "nc_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3145e4e1-bf53-492b-a39a-56963314f48f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if query_cache:\n",
    "    # cache is stored in the temp\n",
    "    temporary_path_base = \"tmp/tmp_{}\".format(uuid.uuid4().hex)\n",
    "    Path(temporary_path_base).mkdir(parents=True, exist_ok=True)\n",
    "    logging.info(\"Temporary folder created: `%s`\", temporary_path_base)\n",
    "\n",
    "    # cache\n",
    "    temporary_cache = PurePath(temporary_path_base, \"cache\")\n",
    "    dx_dict = dc.Cache(temporary_cache)\n",
    "    dx_dict.clear()\n",
    "    dx_dict.reset(\"size_limit\", int(12e9))  # in bytes\n",
    "    dx_dict.reset(\"cull_limit\", 0)\n",
    "\n",
    "    # tmp files\n",
    "    temporary_tmp = PurePath(temporary_path_base, \"temp\")\n",
    "    Path(temporary_tmp).mkdir(parents=True, exist_ok=True)\n",
    "    nc_temp = tempfile.TemporaryDirectory(dir=temporary_tmp, suffix=None)\n",
    "    temporary_path = nc_temp.name\n",
    "\n",
    "    # path redirections\n",
    "    logging.info(\"Temporary subfolder created (cache): `%s`\", temporary_cache)\n",
    "    logging.info(\"Temporary subfolder created (temp): `%s`\", temporary_tmp)\n",
    "    logging.info(\"Temporary subfolder created (temp/*): `%s`\", temporary_path)\n",
    "\n",
    "    def temp_cleanup(temporary_path_base, temporary_path, nc_temp, dx_dict):\n",
    "        logging.info(\"Cleanup routines:\")\n",
    "        try:\n",
    "            logging.info(\"Closing Cache.\")\n",
    "            logging.info(\" Cache volume (current): '{}'\".format(str(dx_dict.volume())))\n",
    "            dx_dict.clear()\n",
    "            logging.info(\" Cache volume (cleared): '{}'\".format(str(dx_dict.volume())))\n",
    "            dx_dict.close()\n",
    "            logging.info(\"Cache closed.\")\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            nc_temp.cleanup()\n",
    "            logging.info(\"Temporary subfolder removed (temp/*): `%s`\", temporary_path)\n",
    "        finally:\n",
    "            shutil.rmtree(temporary_path, ignore_errors=True)\n",
    "\n",
    "        try:\n",
    "            shutil.rmtree(temporary_path_base)\n",
    "            logging.info(\"Temporary folder removed: `%s`\", temporary_path_base)\n",
    "        finally:\n",
    "            shutil.rmtree(nc_temp.name, ignore_errors=True)\n",
    "\n",
    "    # register cleanup upon exit\n",
    "    import atexit\n",
    "\n",
    "    atexit.register(temp_cleanup, temporary_path_base, temporary_path, nc_temp, dx_dict)\n",
    "\n",
    "else:\n",
    "    dx_dict = {}\n",
    "    nc_temp = None\n",
    "\n",
    "nc_list = []\n",
    "nc_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c1f51f-ce7b-4eac-8694-6f97d47e2f49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# time related helper variables\n",
    "if not query_to and query_period:\n",
    "    query_to = pd.to_datetime(query_from) + pd.tseries.frequencies.to_offset(\n",
    "        query_period\n",
    "    )\n",
    "    if any(query_period.endswith(n) for n in [\"M\", \"Y\"]):\n",
    "        query_to = query_to + pd.tseries.frequencies.to_offset(\"1D\")\n",
    "query_range = pd.to_datetime([query_from, query_to])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3b27f4-9786-4910-bae4-d961be94bb4a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99501d1-c1ef-4b69-bc51-d830f443d32a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    if query_dask:\n",
    "        from dask.distributed import Client, LocalCluster\n",
    "\n",
    "        # Create cluster\n",
    "        # cluster = LocalCluster(n_workers=1, processes=False, threads_per_worker=1, memory_limit=\"6GB\")\n",
    "        # client = Client(cluster)\n",
    "        # client\n",
    "        # the client has now been started\n",
    "        # logging.info(\"Starting a new `dask` cluster and client.\")\n",
    "\n",
    "    else:\n",
    "        from dask.distributed import Client\n",
    "\n",
    "        #\n",
    "        logging.info(\"Using an existing `dask` client, e.g., in jupyterlab.\")\n",
    "        client = Client(\"tcp://127.0.0.1:39743\")\n",
    "        client\n",
    "\n",
    "        # dask `enabled` is required\n",
    "        query_dask = True\n",
    "\n",
    "    ## Convert L0 to L1 (vocabulary only)\n",
    "    if \"conform\" in query_tasks:\n",
    "        logging.info(\"Task `conform`\")\n",
    "        # input file patterns\n",
    "        _, fn_list = get_input_files(input_subset, custom_subset=input_subset)\n",
    "\n",
    "        # decode input files\n",
    "        logging.info(\"Query source file database.\")\n",
    "        datastore_df = pd.concat(\n",
    "            [pd.DataFrame(filedb_decode(fn)) for fn in set(n[\"file\"] for n in fn_list)],\n",
    "            axis=0,\n",
    "        )\n",
    "\n",
    "        # input file query\n",
    "        logging.info(\"Subset source file database.\")\n",
    "        datastore_df = datastore_df.loc[\n",
    "            (datastore_df[(\"decoded\", \"time_start\")] >= query_range[0])\n",
    "            & (datastore_df[(\"decoded\", \"time_end\")] <= query_range[1]),\n",
    "            :,\n",
    "        ]\n",
    "\n",
    "        if len(datastore_df) > 0:\n",
    "            # lookup vocabulary\n",
    "            logging.info(\"Query vocabulary database.\")\n",
    "            datastore_units = datastore_units_query(datastore_df)\n",
    "\n",
    "            logging.info(\"Apply vocabulary translations.\")\n",
    "            p_list = datastore_conform(datastore_df, datastore_units.sort_index())\n",
    "        else:\n",
    "            logging.info(\n",
    "                \"No source files found in query subset of the source file database.\"\n",
    "            )\n",
    "        nc_dict[\"conform\"] = p_list\n",
    "\n",
    "    ## Concatenate DataSets in each path, along dimension `time`.\n",
    "    if \"concat\" in query_tasks:\n",
    "        logging.info(\"Task `concat`\")\n",
    "\n",
    "        # input file patterns\n",
    "        default_subset = dict(\n",
    "            path_base=cache_path_base,\n",
    "            path=input_path,\n",
    "            file=input_file,\n",
    "        )\n",
    "        _, fn_list = get_input_files(\n",
    "            input_subset,\n",
    "            default_subset=default_subset,\n",
    "            custom_subset={**input_subset, **{\"production_level\": \"L1\"}},\n",
    "        )\n",
    "\n",
    "        # decode input files\n",
    "        logging.info(\"Query source file database.\")\n",
    "        datastore_df = pd.concat(\n",
    "            [pd.DataFrame(filedb_decode(fn)) for fn in set(n[\"file\"] for n in fn_list)],\n",
    "            axis=0,\n",
    "        )\n",
    "\n",
    "        # input file query\n",
    "        logging.info(\"Subset source file database.\")\n",
    "        datastore_df = datastore_df.loc[\n",
    "            (datastore_df[(\"decoded\", \"time_start\")] >= query_range[0])\n",
    "            & (datastore_df[(\"decoded\", \"time_end\")] <= query_range[1]),\n",
    "            :,\n",
    "        ]\n",
    "\n",
    "        if len(datastore_df) > 0:\n",
    "            # lookup vocabulary\n",
    "            logging.info(\"Concatenate grouped database.\")\n",
    "            p_list = datastore_concat(datastore_df)\n",
    "        else:\n",
    "            logging.info(\n",
    "                \"No source files found in query subset of the source file database.\"\n",
    "            )\n",
    "        nc_dict[\"concat\"] = p_list\n",
    "\n",
    "    ### Merge concatenated DataSets along Index `station` and `system`.\n",
    "    if \"combine\" in query_tasks:\n",
    "        logging.info(\"Task `combine`\")\n",
    "        nc_dict[\"combine\"] = list(\n",
    "            # set(glob.glob(pathname=\"tmp/tmp*/**/*.nc\",recursive=True)) + glob.glob(\"../data/tmp/*.nc\"))\n",
    "            # set(glob.glob(pathname=\"{}/**/*.(nc|zarr.zip)\".format(nc_temp.name), recursive=True))\n",
    "            set(\n",
    "                str(p)\n",
    "                for p in Path(nc_temp.name).glob(\"**/*.*\")\n",
    "                if p.suffix in {\".zip\", \".nc\"}\n",
    "            )\n",
    "        )\n",
    "\n",
    "        datastore_df = pd.concat(\n",
    "            [\n",
    "                pd.DataFrame(filedb_decode(fn))\n",
    "                for fn in set(str(n) for n in nc_dict[\"combine\"])\n",
    "            ],\n",
    "            axis=0,\n",
    "        )\n",
    "        datastore_df = datastore_df.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "        store_temp, store_output = datastore_combine(datastore_df)\n",
    "\n",
    "    ### Merge DataSets along Index `station` and `system` and `time`, and variables.\n",
    "    if \"conflate\" in query_tasks:\n",
    "        logging.info(\"Task `conflate`\")\n",
    "        if \"store_output\" in locals():\n",
    "            nc_dict[\"conflate\"] = [Path(store_output)]\n",
    "        elif \"store_temp\" in locals():\n",
    "            nc_dict[\"conflate\"] = [Path(store_temp)]\n",
    "        else:\n",
    "            nc_dict[\"conflate\"] = list(\n",
    "                set(\n",
    "                    str(p)\n",
    "                    for p in Path(nc_temp.name).glob(\"*.*\")\n",
    "                    if p.suffix in {\".zip\"}\n",
    "                )\n",
    "            )\n",
    "        store_file_list, output_file_list = datastore_cube(nc_dict[\"conflate\"])\n",
    "\n",
    "    ### Cleanup\n",
    "    if query_cache:\n",
    "        # temp_cleanup()\n",
    "        # no cleanup action here, look for the `atexit` instantation.\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56c338f-2c57-4d97-804b-95592908d9cf",
   "metadata": {},
   "source": [
    "# Temp files removed on (clean) exit in python (not jupyter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec4a7e8-84d1-42ef-9aba-3670b6f385c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "24e8a64f-20f2-48bc-a2fe-82d10af93738",
   "metadata": {
    "tags": []
   },
   "source": [
    "# DEV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8baecac-e16d-4d19-9ec9-bd4e848f9aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def datastore_combine_nc_to_zarr(fn):\n",
    "\n",
    "    p = Path(fn)\n",
    "    # root\n",
    "    with xr.open_dataset(p) as ds:\n",
    "        t = p.with_suffix(\".zarr\")\n",
    "        ds.to_zarr(t)\n",
    "\n",
    "    # groups\n",
    "    groups = list(nc.Dataset(p).groups.keys())\n",
    "    for group in groups:\n",
    "        print(group)\n",
    "        with xr.open_dataset(p, group=group) as dx:\n",
    "            t = p.with_suffix(\".zarr\")\n",
    "            dx.to_zarr(t, group=group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86137b4d-f382-43f4-a6c9-805775fce100",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_ds(nc_ds, nc_mode=\"w\", group=None):\n",
    "    # fix\n",
    "    nc_ds = nc_ds.chunk({\"station\": 1, \"system\": 1, \"time\": 3600})\n",
    "\n",
    "    # reconstruct the dim and cooridnate order accoring to dimensions.\n",
    "    nc_ds = preprocess_nc(nc_ds)\n",
    "    nc_ds = nc_ds.transpose(*[n for n in dim_list if n in list(nc_ds.dims)])\n",
    "    nc_ds = reset_coords(nc_ds, sortby=dim_list)\n",
    "\n",
    "    nc_dict = dattrs(nc_ds)\n",
    "    nc_path = \"../data/L0/\"\n",
    "    nc_file = \"urbisphere_set({global_location},{system_group},{time_bounds})_version({version}).{extension}\"\n",
    "    print(\"Output file: '{}'\".format(os.path.join(nc_path, nc_file).format(**nc_dict)))\n",
    "\n",
    "    print(nc_ds)\n",
    "\n",
    "    nc_ds.to_netcdf(os.path.join(nc_path, nc_file).format(**nc_dict), mode=nc_mode)\n",
    "    # nc_ds.to_zarr(os.path.join(nc_path, nc_file).format(**nc_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bb9fdb-d0fd-415a-a8f4-8ad95df8d4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def datasets_conjoin_cube_qc_1():\n",
    "    # input file patterns\n",
    "    default_subset = dict(\n",
    "        path_base=cache_path_base,\n",
    "        path=input_path,\n",
    "        file=input_file,\n",
    "    )\n",
    "    _, fn_list = get_input_files(\n",
    "        input_subset,\n",
    "        default_subset=default_subset,\n",
    "        custom_subset={**input_subset, **{\"production_level\": \"L1\"}},\n",
    "    )\n",
    "\n",
    "    # decode input files\n",
    "    logging.info(\"Query source file database.\")\n",
    "    datastore_df = pd.concat(\n",
    "        [pd.DataFrame(filedb_decode(fn)) for fn in set(n[\"file\"] for n in fn_list)],\n",
    "        axis=0,\n",
    "    )\n",
    "\n",
    "    # input file query\n",
    "    logging.info(\"Subset source file database.\")\n",
    "    datastore_df = datastore_df.loc[\n",
    "        (datastore_df[(\"decoded\", \"time_start\")] >= query_range[0])\n",
    "        & (datastore_df[(\"decoded\", \"time_end\")] <= query_range[1]),\n",
    "        :,\n",
    "    ]\n",
    "\n",
    "    subset = []\n",
    "    for idg, group in datastore_df.groupby([(\"pattern\", \"global_location\")]):\n",
    "        print(idg)\n",
    "        for idx, item in tqdm(group.iterrows()):\n",
    "            fn = os.path.join(\n",
    "                *(\n",
    "                    item[\n",
    "                        [\n",
    "                            (\"location\", \"path_base\"),\n",
    "                            (\"location\", \"path\"),\n",
    "                            (\"location\", \"file\"),\n",
    "                        ]\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "            store, store_args = datasets_fsspec_args(fn)\n",
    "            ds = xr.open_dataset(store, **store_args)\n",
    "            if any(\n",
    "                ds[\"system_id\"] == item\n",
    "                for item in [\n",
    "                    \"10000000\",\n",
    "                ]\n",
    "            ):\n",
    "                subset.append(fn)\n",
    "            ds.close()\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8622e9-1ef0-44ba-b41b-41e58d740402",
   "metadata": {},
   "outputs": [],
   "source": [
    "def datasets_conjoin_cube_qc_2(fn):\n",
    "\n",
    "    # fn = \"tmp/tmp_bc3811dcdd4e414cb3d8d2dd3488fa4f/temp/tmpftfjpo7t/f2b71cf1150942b0b3e09a3be9cda500.zarr.zip\"\n",
    "    if Path(fn).suffix == \".zip\":\n",
    "        groups = list(zarr.open(fn).group_keys())\n",
    "    elif Path(fn).suffix == \".nc\":\n",
    "        groups = list(nc.Dataset(fn).groups.keys())\n",
    "\n",
    "    for g in groups:\n",
    "        store, store_args = datasets_fsspec_args(fn)\n",
    "        dx = {g: xr.open_dataset(store, group=g, **store_args)}\n",
    "        print({k: v[\"system_id\"].values.tolist() for k, v in dx.items()})\n",
    "        dx[g].close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96a8eeb-9379-4585-80f9-1df4a41e7f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def datasets_conjoin_cube_qc_3(fn, variable=\"ta\"):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    mono_font = {\"fontname\": \"monospace\", \"fontsize\": 8}\n",
    "\n",
    "    # data\n",
    "    store, store_args = datasets_fsspec_args(str(fn))\n",
    "    ds = xr.open_dataset(store, **store_args, group=None, mode=\"r\")\n",
    "    da = ds[variable].mean(dim=\"system\").resample(time=\"h\").mean()\n",
    "\n",
    "    ## Figure, overview\n",
    "    fig_title = str(da.time[0].values.astype(\"datetime64[Y]\"))\n",
    "    fig_network = ds.attrs[\"network\"]\n",
    "    fig_name = (\n",
    "        f\"tmp_datasets_conjoin_cube_qc_3_\"\n",
    "        + f\"set({fig_network},AWS_{variable},{fig_title})_\"\n",
    "        + f\"version({ pd.to_datetime('today').strftime(\"%Y%m%d\")  }).png\"\n",
    "    )\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 7))\n",
    "    da.plot(\n",
    "        x=\"time\",\n",
    "        y=\"station_id\",\n",
    "        vmin=-5,\n",
    "        vmax=35,\n",
    "        ax=ax,  # or figsize=[8, 8],\n",
    "        yincrease=False,\n",
    "        ylim=[-0.5, da.sizes[\"station\"] - 0.5],\n",
    "        xlim=pd.to_datetime(\n",
    "            [f\"{fig_title}{n}\" for n in [\"-01-01 00:00:00\", \"-12-31 23:59:59\"]]\n",
    "        ),\n",
    "        cbar_kwargs={\"shrink\": 0.5},\n",
    "    )\n",
    "    ax.set_yticks(ax.get_yticks())\n",
    "    ax.set_yticklabels(ax.get_yticklabels(), **mono_font)\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "    fig.savefig(fig_name, dpi=\"figure\")\n",
    "    # plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f81db2a-9fe1-4730-abf3-6043cc1bc4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def datasets_conjoin_cube_qc_4(fn, variable=\"ta\"):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    mono_font = {\"fontname\": \"monospace\", \"fontsize\": 8}\n",
    "\n",
    "    # data\n",
    "    store, store_args = datasets_fsspec_args(str(fn))\n",
    "    ds = xr.open_dataset(store, **store_args, group=None, mode=\"r\")\n",
    "    da = ds[variable].resample(time=\"h\").mean()\n",
    "\n",
    "    ## Figure, overview\n",
    "    fig_title = str(da.time[0].values.astype(\"datetime64[Y]\"))\n",
    "    fig_network = ds.attrs[\"network\"]\n",
    "    fig_name = (\n",
    "        f\"tmp_datasets_conjoin_cube_qc_4_\"\n",
    "        + f\"set({fig_network},AWS_{variable},{fig_title})_\"\n",
    "        + f\"version({ pd.to_datetime('today').strftime(\"%Y%m%d\")  }).png\"\n",
    "    )\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 7))\n",
    "    da.plot(\n",
    "        x=\"time\",\n",
    "        y=\"system_id\",\n",
    "        vmin=-5,\n",
    "        vmax=35,\n",
    "        ax=ax,  # or figsize=[8, 8],\n",
    "        yincrease=False,\n",
    "        ylim=[-0.5, da.sizes[\"system\"] - 0.5],\n",
    "        xlim=pd.to_datetime(\n",
    "            [f\"{fig_title}{n}\" for n in [\"-01-01 00:00:00\", \"-12-31 23:59:59\"]]\n",
    "        ),\n",
    "        cbar_kwargs={\"shrink\": 0.5},\n",
    "    )\n",
    "    ax.set_yticks(ax.get_yticks())\n",
    "    ax.set_yticklabels(ax.get_yticklabels(), **mono_font)\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "    fig.savefig(fig_name, dpi=\"figure\")\n",
    "    # plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e980d8e-1e55-401c-aba5-744811977683",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    for store_file in store_file_list:\n",
    "        # datasets_conjoin_cube_qc_3(store_file)\n",
    "        # datasets_conjoin_cube_qc_4(store_file)\n",
    "        pass\n",
    "except:\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
