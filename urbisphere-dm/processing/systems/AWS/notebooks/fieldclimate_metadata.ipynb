{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9289d668-7441-4dae-a677-46def4cb0b5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b49c8cba-7aa9-4e2d-9ff0-1e0ffd9849bf",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Introduction\n",
    "\n",
    "Created on Tue May 5 15:33:36 2022\n",
    "\n",
    "Modified on Tue Aug 3 12:59:23 2022\n",
    "\n",
    "Modified on Mon Dec 12 16:48:02 2022\n",
    "\n",
    "Modified on Mon Sep 25 14:01:34 2023\n",
    "\n",
    "Modified on Fri Oct 11 10:59:01 2024\n",
    "\n",
    "@author: zeeman-m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e053921-e2c8-496a-97a4-6045cff42227",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "420c12f2-b42c-4dbe-8812-020a0474e4e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# FieldcClimate connection\n",
    "from datetime import datetime\n",
    "\n",
    "import requests\n",
    "from Crypto.Hash import HMAC, SHA256\n",
    "from dateutil.tz import tzlocal\n",
    "from requests.auth import AuthBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc45f566-813e-462e-98b9-34422e8ee8f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Other requirements\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import toml\n",
    "import xarray as xr\n",
    "from mergedeep import merge\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c446aa4-2a05-4312-8f7f-296610d40a05",
   "metadata": {},
   "source": [
    "# Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8490a238-329b-42a1-a5c6-50b07c1dba74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rebase_path(path_base=\"urbisphere-dm\", path_root=None):\n",
    "    \"\"\"return abs path of a higher level directory\"\"\"\n",
    "    from pathlib import Path\n",
    "\n",
    "    path_root = Path(\"__file__\").parent.resolve() if not path_root else path_root\n",
    "    path_parts = lambda p: p[0 : (p.index(path_base) + 1 if path_base in p else len(p))]\n",
    "    return str(Path(*[n for n in path_parts(Path(path_root).parts)]))\n",
    "\n",
    "\n",
    "sys.path.append(os.path.join(rebase_path(), \"interfaces/metadb/notebooks/\"))\n",
    "sys.path.append(os.path.join(rebase_path(), \"processing/datasets/pub/notebook/\"))\n",
    "from ipynb.fs.defs.metadb_publications import metadb_publication_query\n",
    "from ipynb.fs.full.metadb_query import (\n",
    "    inventardb_query_deployment as get_metadata_inventory_deployment,\n",
    ")\n",
    "from ipynb.fs.full.metadb_query import metadb_query as metadb_query_subset_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8af6e1de-be51-431f-ba52-54b745a05bff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_metadata_metadb_deployment(query_city, query_time):\n",
    "    subset = {\"system\": {\"i_model\": [\"LoRAIN\",\"nMETOS100+\"]}}\n",
    "    if query_city and query_city != \"\":\n",
    "        subset[\"station\"] = {\n",
    "            \"y_code\": (query_city if isinstance(query_city, list) else [query_city])\n",
    "        }\n",
    "    if len(query_range) == 2:\n",
    "        subset[\"time\"] = dict(zip([\"start\", \"end\"], query_range.astype(str).tolist()))\n",
    "\n",
    "    metadb = metadb_query_subset_table(subset=subset)\n",
    "    return metadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74e9920-0dd0-4fe5-8d81-359523c44b1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2a39fe-58f8-4638-9cb1-fa91fc891d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_config(ioconf_file, ioconf_name=None, version_dict=None, merge_additive = ['gattrs']):\n",
    "    \"\"\"Read configuration file and extract dict that matches version['id'].\"\"\"\n",
    "    import toml\n",
    "    from mergedeep import merge, Strategy\n",
    "    from collections import ChainMap\n",
    "\n",
    "    def read_config(ioconfig_file):\n",
    "        if os.path.exists(ioconfig_file):\n",
    "            with open(ioconfig_file) as f:\n",
    "                ioconf = toml.load(f)\n",
    "            return ioconf\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    # read TOML config file\n",
    "    ioconf = read_config(ioconfig_file)\n",
    "\n",
    "    if isinstance(version_dict, dict):\n",
    "        # lookup of settings\n",
    "        if ioconfig_name:\n",
    "            group = ioconfig_name\n",
    "        else:\n",
    "            group = Path(ioconfig_file).stem\n",
    "\n",
    "        if group in ioconf:\n",
    "            # replace merge\n",
    "            conf_list = [\n",
    "                d\n",
    "                for d in ioconf[group]\n",
    "                if version_dict[\"id\"].startswith(d[\"version\"][\"id\"])\n",
    "            ]\n",
    "            config = merge(*conf_list)\n",
    "            \n",
    "            # additive merge\n",
    "            deep_conf_list = [\n",
    "                d\n",
    "                for d in ioconf[group]\n",
    "                if version_dict[\"id\"] == (d[\"version\"][\"id\"])\n",
    "            ]            \n",
    "            deep_config = merge(*deep_conf_list,strategy=Strategy.TYPESAFE_ADDITIVE)    \n",
    "            \n",
    "            for g in merge_additive:\n",
    "                if g in config and g in deep_config:\n",
    "                    if isinstance(deep_config[g],list):\n",
    "                        config[g] = [dict(ChainMap(*reversed(deep_config[g])))]\n",
    "        else:\n",
    "            config = {}\n",
    "\n",
    "        return config\n",
    "    else:\n",
    "        return ioconf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8797bf9f-a2d0-4664-9844-eb2c2871813c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gattrs(gattrs, data_subset, sep=\";\\n\"):\n",
    "    ga = gattrs[0]\n",
    "    gs = {}\n",
    "    if len(ga) > 1:\n",
    "        for g in gattrs[1:]:\n",
    "            bo = False\n",
    "            for k in gs.keys():\n",
    "                if k in g and not bo:\n",
    "                    bo = any(re.compile(m).match(gs[k]) for m in g[k])\n",
    "                    if bo:\n",
    "                        for k, v in g.items():\n",
    "                            if k in ga:\n",
    "                                ga[k] = v\n",
    "\n",
    "    ct = get_creation_time()\n",
    "    gd = {\n",
    "        \"version_id\": version[\"id\"],\n",
    "        \"version_time\": version[\"time\"],\n",
    "        \"version_date\": version[\"time\"],\n",
    "        \"creation_time\": ct,\n",
    "        \"creation_date\": ct[0:10],\n",
    "    }\n",
    "    ga = {k: sep.join(v) if isinstance(v, list) else v for k, v in ga.items()}\n",
    "    ga = {k: v.format(**gd) for k, v in ga.items()}\n",
    "    return ga\n",
    "\n",
    "\n",
    "def get_gattrs_pub(gattrs):\n",
    "    publication_gattrs = {}\n",
    "    for ga in gattrs:\n",
    "        if \"production_profile\" in ga:\n",
    "            pga = metadb_publication_query(\n",
    "                ga[\"production_profile\"], publication_name=\"datasets_default\"\n",
    "            )\n",
    "            # pga = {k:v for k,v in pga.items() if v != ''}\n",
    "            publication_gattrs = {**publication_gattrs, **pga}\n",
    "    return publication_gattrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236237a1-a8ef-4306-bfcf-ebac60af7869",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3051fe-df13-4078-a516-a28b611c2669",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_creation_time(d=datetime.utcnow()):\n",
    "    d_str = d.replace(microsecond=0, tzinfo=timezone.utc).isoformat()\n",
    "    return d_str\n",
    "\n",
    "\n",
    "def get_time_bounds(query_range):\n",
    "    tr = query_range.strftime(\"%Y%m%dT%H%M%S%z\").tolist()\n",
    "    res = tr[0] if tr[0] == tr[1] else \"{}_{}\".format(*tr)\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_global_attributes(data_subset):\n",
    "    from copy import deepcopy\n",
    "\n",
    "    ga = deepcopy(get_gattrs(gattrs, data_subset))\n",
    "    pga = deepcopy(get_gattrs_pub(gattrs))\n",
    "\n",
    "    for k in ga.keys():\n",
    "        if not k in pga:\n",
    "            pga[k] = ga[k]\n",
    "        if k in pga and ga[k] != \"\":\n",
    "            if pga[k] == \"\":\n",
    "                pga[k] = ga[k]\n",
    "    return pga\n",
    "\n",
    "\n",
    "def get_global_reference():\n",
    "    crs = OrderedDict(\n",
    "        origin_time=\"nanoseconds since 1970-01-01 00:00:00 +0000\",\n",
    "        origin_lon=0.0,\n",
    "        origin_lat=0.0,\n",
    "        origin_utm_x=0.0,\n",
    "        origin_utm_y=0.0,\n",
    "        origin_x=0.0,\n",
    "        origin_y=0.0,\n",
    "        origin_z=0.0,  # or \"station: h\"\n",
    "        origin_h=\"meters above mean sea level\",  # or \"meters above Normaal Amsterdams Peil\"\n",
    "        origin_azimuth=0.0,\n",
    "    )\n",
    "    return crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ba201b-d2ff-45f4-9059-cc45baa684ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "81a32655-6133-46ba-a2f8-73155d753163",
   "metadata": {},
   "source": [
    "## Connection to Fieldclimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e70315-a1ac-4fa5-a710-ac0896d2a8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified from source: https://api.fieldclimate.com/v2/docs#authentication-hmac\n",
    "\n",
    "\n",
    "# Class to perform HMAC encoding\n",
    "class AuthHmacMetosGet(AuthBase):\n",
    "    # Creates HMAC authorization header for Metos REST service GET request.\n",
    "    def __init__(self, apiRoute, publicKey, privateKey):\n",
    "        self._publicKey = publicKey\n",
    "        self._privateKey = privateKey\n",
    "        self._method = \"GET\"\n",
    "        self._apiRoute = apiRoute\n",
    "\n",
    "    def __call__(self, request):\n",
    "        dateStamp = datetime.utcnow().strftime(\"%a, %d %b %Y %H:%M:%S GMT\")\n",
    "        request.headers[\"Date\"] = dateStamp\n",
    "        msg = (self._method + self._apiRoute + dateStamp + self._publicKey).encode(\n",
    "            encoding=\"utf-8\"\n",
    "        )\n",
    "        h = HMAC.new(self._privateKey.encode(encoding=\"utf-8\"), msg, SHA256)\n",
    "        signature = h.hexdigest()\n",
    "        request.headers[\"Authorization\"] = \"hmac \" + self._publicKey + \":\" + signature\n",
    "        return request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164e4de9-aaff-49f1-983e-421b02480a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fieldclimate_metadata(\n",
    "    apiConf,\n",
    "    meta_conf={\n",
    "        \"user\": \"/user\",\n",
    "        \"user_stations\": \"/user/stations\",\n",
    "        \"system_groups\": \"/system/groups\",\n",
    "        \"system_sensors\": \"/system/sensors\",\n",
    "    },\n",
    "):\n",
    "    \"\"\"Get Selected Observatory Meta Data via REST API\"\"\"\n",
    "    response = OrderedDict()\n",
    "    for k, apiRoute in meta_conf.items():\n",
    "        auth = AuthHmacMetosGet(apiRoute, apiConf[\"publicKey\"], apiConf[\"privateKey\"])\n",
    "        logging.info(\"Fieldclimate API Route:  %s\", apiRoute)\n",
    "        response[k] = requests.get(\n",
    "            apiConf[\"apiURI\"] + apiRoute,\n",
    "            headers={\"Accept\": \"application/json\"},\n",
    "            auth=auth,\n",
    "            timeout=(3.05, 27), # connection, read            \n",
    "        )\n",
    "    return response\n",
    "\n",
    "\n",
    "def get_fieldclimate_metadata_dataframe(response, section=\"user_stations\"):\n",
    "    \"\"\"Convert selected response into pandas dataframe\"\"\"\n",
    "\n",
    "    if isinstance(response[section], requests.models.Response):\n",
    "        response_json = response[section].json()\n",
    "    else:\n",
    "        response_json = response[section]\n",
    "\n",
    "    if not isinstance(response_json, list):\n",
    "        df = pd.json_normalize(response_json)\n",
    "    else:\n",
    "        df_list = []\n",
    "        for k in response_json:\n",
    "            df = pd.json_normalize(k)\n",
    "            df_list.append(df)\n",
    "        df = pd.concat(df_list)\n",
    "\n",
    "    # filters\n",
    "    if section in [\"user_stations\"]:\n",
    "        df = df.sort_values([\"name.custom\"])\n",
    "    elif section in [\"system_groups\", \"system_sensors\"]:\n",
    "        df.columns = df.columns.str.split(\".\", expand=True)\n",
    "        df = df.loc[0].unstack(level=0).transpose()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcc248c-4670-4164-ba69-3d302a290c2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483fb94b-40dd-427b-8d6c-230e46708989",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context(\n",
    "    \"display.max_rows\",\n",
    "    None,\n",
    "    \"display.max_columns\",\n",
    "    None,\n",
    "    \"display.precision\",\n",
    "    3,\n",
    "):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75c8fea-ba7e-4acb-b42a-d3ab1bfb12bf",
   "metadata": {},
   "source": [
    "## Fieldclimate Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9991dada-b740-4907-9e76-6d6d240fae24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fieldclimate_cell_methods(aggr, cell_dim=\"time\"):\n",
    "    lut = {\"max\": \"maximum\", \"min\": \"minimum\", \"avg\": \"mean\"}\n",
    "    if aggr in [\n",
    "        \"sum\",\n",
    "        \"mean\",\n",
    "        \"maximum\",\n",
    "        \"minimum\",\n",
    "        \"mid_range\",\n",
    "        \"standard_deviation\",\n",
    "        \"variance\",\n",
    "        \"mode\",\n",
    "        \"median\",\n",
    "    ]:\n",
    "        res = OrderedDict(\n",
    "            cell_methods=\"{}: {}\".format(cell_dim, aggr),\n",
    "            cell_bounds_domain={\"{}_{}\".format(cell_dim, \"bounds\"): [0, 1]},\n",
    "        )\n",
    "    elif aggr in lut.keys():\n",
    "        res = OrderedDict(\n",
    "            cell_methods=\"{}: {}\".format(cell_dim, lut[aggr]),\n",
    "            cell_bounds_domain={\"{}_{}\".format(cell_dim, \"bounds\"): [0, 1]},\n",
    "        )\n",
    "    elif aggr in [\"point\"]:\n",
    "        res = OrderedDict(\n",
    "            cell_methods=\"{}: {}\".format(cell_dim, aggr),\n",
    "            cell_bounds_domain={\"{}_{}\".format(cell_dim, \"bounds\"): [0, 0]},\n",
    "        )\n",
    "    elif aggr in [\"last\"]:\n",
    "        res = OrderedDict(\n",
    "            cell_methods=\"{}: {}\".format(cell_dim, \"point\"),\n",
    "            cell_bounds_domain={\"{}_{}\".format(cell_dim, \"bounds\"): [1, 1]},\n",
    "        )\n",
    "    else:\n",
    "        res = OrderedDict()\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_fieldclimate_cell_bounds(domain, cell_bounds_domain):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8dcaa97-b3b7-4290-bc38-05167a7b0c5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dda5626-5b84-4275-8a16-a5fc76647b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fieldclimate_station(apiConf, apiQueryList):\n",
    "    \"\"\"Get Selected Observatory Data via REST API\"\"\"\n",
    "    response = OrderedDict()\n",
    "    for apiQuery in tqdm(apiQueryList):\n",
    "        for k, apiRoute in apiQuery.items():\n",
    "            # print(k)\n",
    "            auth = AuthHmacMetosGet(\n",
    "                apiRoute, apiConf[\"publicKey\"], apiConf[\"privateKey\"]\n",
    "            )\n",
    "            response[k] = requests.get(\n",
    "                apiConf[\"apiURI\"] + apiRoute,\n",
    "                headers={\"Accept\": \"application/json\"},\n",
    "                auth=auth,\n",
    "                timeout=(3.05, 27), # connection, read                \n",
    "            )\n",
    "    return response\n",
    "\n",
    "\n",
    "def get_fieldclimate_station_queries(\n",
    "    station_ids,\n",
    "    time_range,\n",
    "    time_offset=[\"0S\", \"-1S\"],\n",
    "    data_groups=[\"1\"],  # ['4','7','8','5','3','9','6','1','2']\n",
    "    convert_to_unix_timestamp=True,\n",
    "):\n",
    "    \"\"\"Convert selected response into pandas dataframe\"\"\"\n",
    "    query_list = []\n",
    "    query_str = \"/data/{STATION_ID}/{DATA_GROUP}/from/{FROM_UNIX_TIMESTAMP}/to/{TO_UNIX_TIMESTAMP}\"\n",
    "    query_id_str = \"{STATION_ID}_{FROM_UNIX_TIMESTAMP}_{DATA_GROUP}\"\n",
    "\n",
    "    time_from, time_to = time_range\n",
    "    time_from_offset, time_to_offset = time_offset\n",
    "\n",
    "    # station ids\n",
    "    if isinstance(station_ids, str):\n",
    "        station_ids = [station_ids]\n",
    "\n",
    "    if not data_groups:\n",
    "        data_groups = [\"\"]\n",
    "\n",
    "    # time variable.\n",
    "    time_span = {\n",
    "        \"start\": pd.to_datetime(time_from, format=\"ISO8601\")\n",
    "        + pd.Timedelta(time_from_offset),\n",
    "        \"end\": pd.to_datetime(time_to, format=\"ISO8601\") + pd.Timedelta(time_to_offset),\n",
    "    }\n",
    "    time_delta = list(time_span.values())[1] - list(time_span.values())[0]\n",
    "\n",
    "    # split the queries by time range (max 1 day)\n",
    "    if time_delta < pd.Timedelta(\"1D\"):\n",
    "        time_list = [\n",
    "            dict(\n",
    "                zip(\n",
    "                    [\"FROM_UNIX_TIMESTAMP\", \"TO_UNIX_TIMESTAMP\"],\n",
    "                    list(time_span.values()),\n",
    "                )\n",
    "            )\n",
    "        ]\n",
    "    else:\n",
    "        time_days = pd.date_range(**time_span, freq=\"1D\").tolist()\n",
    "        time_list = [\n",
    "            dict(zip([\"FROM_UNIX_TIMESTAMP\", \"TO_UNIX_TIMESTAMP\"], l))\n",
    "            for l in list(zip(time_days, time_days[1:] + [(time_span[\"end\"])]))\n",
    "        ]\n",
    "        if time_list[-1][\"FROM_UNIX_TIMESTAMP\"] == time_list[-1][\"TO_UNIX_TIMESTAMP\"]:\n",
    "            time_list = time_list[:-1]\n",
    "\n",
    "    # convert or debug\n",
    "    if convert_to_unix_timestamp:\n",
    "        for i in range(len(time_list)):\n",
    "            for k, v in time_list[i].items():\n",
    "                time_list[i][k] = (v - pd.Timestamp(\"1970-01-01\")) // pd.Timedelta(\"1s\")\n",
    "\n",
    "    # loop\n",
    "    for sid in station_ids:\n",
    "        for dg in data_groups:\n",
    "            for tr in time_list:\n",
    "                query_dict = {**{\"STATION_ID\": sid, \"DATA_GROUP\": dg}, **tr}\n",
    "                query = (query_str.format(**query_dict)).replace(\"//\", \"/\")\n",
    "                query_id = query_id_str.format(**query_dict)\n",
    "                query_list.append({query_id: query})\n",
    "\n",
    "    return query_list\n",
    "\n",
    "\n",
    "def get_fieldclimate_station_dataframe(response_dict):\n",
    "    df_list = []\n",
    "    for response in response_dict:\n",
    "        for k, v in response:\n",
    "            df = pd.json_normalize(v.json())\n",
    "            df_list.append(df)\n",
    "    df = pd.concat(df_list).sort_values([\"name.custom\"]).set_index([\"name.custom\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea922f95-5864-4af5-95fa-cb9ccd6ddbd9",
   "metadata": {},
   "source": [
    "## Query response handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b040fe1-e9b9-4542-8e40-4add3e88e481",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_fieldclimate_station_datasets_org(\n",
    "    response_dict, meta_stations, meta_deployment\n",
    "):\n",
    "    import re\n",
    "\n",
    "    def get_deployment(meta_deployment, station, timestamp):\n",
    "        ts = pd.to_datetime(float(timestamp), unit=\"s\", origin=\"unix\")\n",
    "\n",
    "        res = (\n",
    "            {}\n",
    "        )  # meta_deployment.loc[:,['model.manufacturer_name','model.model_name','model.model_description']].iloc[0].to_dict()\n",
    "\n",
    "        depl = meta_deployment.loc[\n",
    "            (\n",
    "                (meta_deployment[\"sensor.sensor_serial\"] == station)\n",
    "                & (meta_deployment[\"location.location_startdate\"] <= ts)\n",
    "                & (meta_deployment[\"location.location_enddate\"] > ts)\n",
    "            ),\n",
    "            :,\n",
    "        ]\n",
    "\n",
    "        if not depl.empty:\n",
    "            res = {**res, **depl.iloc[0].to_dict()}\n",
    "\n",
    "        return res\n",
    "\n",
    "    def evaluate_attributes(d):\n",
    "        \"\"\"Encoding of list and dicts to json.\"\"\"\n",
    "        import json\n",
    "\n",
    "        res = {\n",
    "            \"attributes_id\": [],\n",
    "            \"attributes_key\": [],\n",
    "            \"attributes_type\": [],\n",
    "            \"attributes_val\": [],\n",
    "        }\n",
    "        for i, j in d.items():\n",
    "            for k, v in j.items():\n",
    "                res[\"attributes_id\"].append(i)\n",
    "                res[\"attributes_key\"].append(k)\n",
    "                res[\"attributes_type\"].append(str(type(v).__name__))\n",
    "                if isinstance(v, list) or isinstance(v, dict):\n",
    "                    v = json.dumps(v)\n",
    "                else:\n",
    "                    v = str(v)\n",
    "                    v = json.dumps(v)\n",
    "\n",
    "                res[\"attributes_val\"].append(v)\n",
    "        return res\n",
    "\n",
    "    ds_dict = OrderedDict()\n",
    "\n",
    "    for ind, response in response_dict.items():\n",
    "        # print(ind)\n",
    "\n",
    "        # processing identifiers\n",
    "        station, timestamp, group = ind.split(\"_\")\n",
    "\n",
    "        if not station in ds_dict:\n",
    "            ds_dict[station] = OrderedDict()\n",
    "\n",
    "        if not timestamp in ds_dict[station]:\n",
    "            ds_dict[station][timestamp] = []\n",
    "\n",
    "        res_json = response.json()\n",
    "        res_meta = meta_stations[meta_stations.index == station].reset_index().loc[0]\n",
    "        # res_locs = (\n",
    "        #    meta_locations[meta_locations[\"Serial No.\"] == station].reset_index().loc[0]\n",
    "        # )\n",
    "        res_locs = get_deployment(meta_deployment, station, timestamp)\n",
    "        res_refs = get_global_reference()\n",
    "        res_glob = get_global_attributes(output_subset)\n",
    "\n",
    "        for var in res_json[\"data\"]:\n",
    "            # filters\n",
    "            dattr = OrderedDict(\n",
    "                [\n",
    "                    (k, v)\n",
    "                    for k, v in var.items()\n",
    "                    if not isinstance(v, list) and not isinstance(v, dict)\n",
    "                ]\n",
    "            )\n",
    "            # attributes\n",
    "            dattr = OrderedDict(\n",
    "                [\n",
    "                    (\"global\", OrderedDict(res_refs)),\n",
    "                    (\"station\", OrderedDict(res_locs)),\n",
    "                    (\"system\", OrderedDict(res_meta)),\n",
    "                    (\n",
    "                        \"cell\",\n",
    "                        OrderedDict(\n",
    "                            [(k, v) for k, v in var.items() if k not in [\"values\"]]\n",
    "                        ),\n",
    "                    ),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            # flatten attributes, convert values to json\n",
    "            ddattr = evaluate_attributes(dattr)\n",
    "\n",
    "            # remove meaningless variables? (e.g. Midnight)\n",
    "            if not \"name_original\" in list(var.keys()):\n",
    "                # print('SKIPPED: {}'.format(dattr['name']))\n",
    "                continue\n",
    "\n",
    "            if not res_locs:\n",
    "                # if station is not defined in metadata, skip\n",
    "                continue\n",
    "\n",
    "            # extraction of meaningful attributes:\n",
    "            station_res = re.findall(\n",
    "                r\"(Station.*?)\\s?\\((FR.*?)\\)\",\n",
    "                str(res_locs.copy()[\"location.location_text\"]),\n",
    "            )\n",
    "\n",
    "            station_id = station_res[0][1] if station_res else \"\"\n",
    "            station_name = (\n",
    "                str(station_res[0][0]).replace(\"Station Freiburg \", \"\")\n",
    "                if station_res\n",
    "                else \"\"\n",
    "            )\n",
    "\n",
    "            cell_attrs = [\n",
    "                get_fieldclimate_cell_methods(m) for m in var[\"values\"].keys()\n",
    "            ]\n",
    "            # print(cell_attrs)\n",
    "            cell_methods = [\n",
    "                n[\"cell_methods\"] if \"cell_methods\" in n.keys() else \"\"\n",
    "                for n in cell_attrs\n",
    "            ]\n",
    "\n",
    "            var_name = dattr[\"cell\"][\"name_original\"].replace(\" \", \"_\").replace(\",\", \"\")\n",
    "            var_long_name = dattr[\"cell\"][\"name_original\"]\n",
    "            var_units = dattr[\"cell\"][\"unit\"]\n",
    "\n",
    "            # Data variables\n",
    "            data_vars = OrderedDict(\n",
    "                [\n",
    "                    (\n",
    "                        \"{}\".format(var_name),\n",
    "                        (\n",
    "                            [\"system\", \"cell\", \"time\"],\n",
    "                            [[v for k, v in var[\"values\"].items()]],\n",
    "                            {\"long_name\": var_long_name, \"units\": var_units},\n",
    "                        ),\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "            attributes_values = (\n",
    "                \"attributes_values\".format(\"\"),\n",
    "                (\n",
    "                    [\"station\", \"system\", \"channel\", \"attributes\"],\n",
    "                    [[[ddattr[\"attributes_val\"]] * 1]],\n",
    "                    {\n",
    "                        \"description\": \"Attributes combined from meta data and input-file headers.\"\n",
    "                    },\n",
    "                ),\n",
    "            )\n",
    "\n",
    "            # Coordinates\n",
    "            coord_time = pd.to_datetime(res_json[\"dates\"], format=\"ISO8601\")\n",
    "            coords = {\n",
    "                \"time\": (\n",
    "                    [\"time\"],\n",
    "                    coord_time,\n",
    "                    {\n",
    "                        \"long_name\": \"time\",\n",
    "                        \"standard_name\": \"time\",\n",
    "                        # \"calendar\": \"proleptic_gregorian\",  # xarray error\n",
    "                        # \"units\": \"microseconds since 1970-01-01 00:00:00 +0000\", # xarray error\n",
    "                    },\n",
    "                ),\n",
    "                \"station_id\": ([\"station\"], [str(station_id)], OrderedDict()),\n",
    "                \"station_name\": ([\"station\"], [str(station_name)], {}),\n",
    "                # \"station_group\": ([\"station\"], [str(\"PL\")], {}),\n",
    "                \"station_lat\": (\n",
    "                    [\"station\"],\n",
    "                    [float(dattr[\"station\"][\"station.station_lat\"])],\n",
    "                    {},\n",
    "                ),\n",
    "                \"station_lon\": (\n",
    "                    [\"station\"],\n",
    "                    [float(dattr[\"station\"][\"station.station_lon\"])],\n",
    "                    {},\n",
    "                ),\n",
    "                \"system_id\": ([\"system\"], [str(station)], {}),\n",
    "                \"system_name\": ([\"system\"], [str(res_meta[\"info.device_name\"])], {}),\n",
    "                \"system_group\": (\n",
    "                    [\"system\"],\n",
    "                    [str(\"AWS\")],\n",
    "                    {},\n",
    "                ),\n",
    "                \"sensor_id\": ([\"sensor\"], [str(dattr[\"cell\"][\"group\"])], {}),\n",
    "                \"channel_id\": ([\"channel\"], [str(dattr[\"cell\"][\"ch\"])], {}),\n",
    "                \"cell_type\": ([\"cell\"], list(cell_methods), {}),\n",
    "                \"attributes_id\": ([\"attributes\"], ddattr[\"attributes_key\"], {}),\n",
    "                \"attributes_group\": ([\"attributes\"], ddattr[\"attributes_id\"], {}),\n",
    "                \"attributes_type\": ([\"attributes\"], ddattr[\"attributes_type\"], {}),\n",
    "            }\n",
    "\n",
    "            # coords back to tuple\n",
    "            coords = [(k, v) for k, v in coords.items()]\n",
    "\n",
    "            # add attributes_values as coord or data_vars?\n",
    "            coords.append(attributes_values)\n",
    "\n",
    "            # Dataset (xarray)\n",
    "            ds = xr.Dataset(\n",
    "                data_vars=data_vars,\n",
    "                coords=dict(coords),\n",
    "                attrs=res_glob,\n",
    "            )\n",
    "\n",
    "            # Dimensions sort order\n",
    "            ds = ds.transpose(\n",
    "                \"time\",\n",
    "                \"station\",\n",
    "                \"system\",\n",
    "                \"sensor\",\n",
    "                \"channel\",\n",
    "                \"cell\",\n",
    "                \"attributes\",\n",
    "                missing_dims=\"ignore\",\n",
    "            )\n",
    "\n",
    "            # Encoding\n",
    "            encoder_list = [\n",
    "                n for n in output_encoder if n[\"name\"] in list(ds.variables)\n",
    "            ]\n",
    "            for encoder in encoder_list:\n",
    "                n = encoder[\"name\"]\n",
    "                enc = {k: v for k, v in encoder.items() if k not in [\"name\"]}\n",
    "                ds[n].encoding.update(**enc)\n",
    "                logging.debug(\"Encoding updated for `%s`: `%s`\", n, str(enc))\n",
    "\n",
    "            ds_dict[station][timestamp].append(ds)\n",
    "\n",
    "    return ds_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d495ad3-4603-4f08-9f9c-e67de88ea5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fieldclimate_station_datasets(response_dict, meta_stations, meta_deployment):\n",
    "    import re\n",
    "\n",
    "    def get_deployment(meta_deployment, system_id, timestamp):\n",
    "        ts = pd.to_datetime(float(timestamp), unit=\"s\", origin=\"unix\")\n",
    "\n",
    "        # for reference only, <v1.0.4 keys\n",
    "        translation_dict = {\n",
    "            \"sensor.sensor_serial\": (\"id\", \"system_id\"),\n",
    "            \"location.location_startdate\": (\"configuration\", \"d_start_date\"),\n",
    "            \"location.location_enddate\": (\"configuration\", \"d_end_date\"),\n",
    "            \"location.location_text\": (\"id\", \"station_name\"),\n",
    "            \"station.station_lat\": (\"id\", \"system_lat\"),\n",
    "            \"station.station_lon\": (\"id\", \"system_lon\"),\n",
    "        }\n",
    "\n",
    "        res = {}\n",
    "\n",
    "        dep = meta_deployment.loc[\n",
    "            (\n",
    "                (meta_deployment[(\"id\", \"system_id\")] == system_id)\n",
    "                & (meta_deployment[(\"configuration\", \"d_start_date\")] <= ts)\n",
    "                & (meta_deployment[(\"configuration\", \"d_end_date\")] > ts)\n",
    "            ),\n",
    "            :,\n",
    "        ]\n",
    "\n",
    "        if not dep.empty:\n",
    "            dep_subset = pd.concat(\n",
    "                [\n",
    "                    dep.iloc[[0]].loc(axis=1)[\"id\", :],\n",
    "                    dep.iloc[[0]].loc(axis=1)[\n",
    "                        \"configuration\", [\"d_id\", \"c_id\", \"i_id\", \"m_id\", \"s_id\"]\n",
    "                    ],\n",
    "                ],\n",
    "                axis=1,\n",
    "            ).droplevel(axis=1, level=0)\n",
    "            # ALT: dep_subset.columns = dep_subset.columns.to_flat_index().str.join('_')\n",
    "\n",
    "            res = {**res, **dep_subset.iloc[0].to_dict()}\n",
    "\n",
    "        return res\n",
    "\n",
    "    def evaluate_attributes(d):\n",
    "        \"\"\"Encoding of list and dicts to json.\"\"\"\n",
    "        import json\n",
    "\n",
    "        res = {\n",
    "            \"attributes_id\": [],\n",
    "            \"attributes_key\": [],\n",
    "            \"attributes_type\": [],\n",
    "            \"attributes_val\": [],\n",
    "        }\n",
    "        for i, j in d.items():\n",
    "            for k, v in j.items():\n",
    "                res[\"attributes_id\"].append(i)\n",
    "                res[\"attributes_key\"].append(k)\n",
    "                res[\"attributes_type\"].append(str(type(v).__name__))\n",
    "                if isinstance(v, list) or isinstance(v, dict):\n",
    "                    v = json.dumps(v)\n",
    "                else:\n",
    "                    v = str(v)\n",
    "                    v = json.dumps(v)\n",
    "\n",
    "                res[\"attributes_val\"].append(v)\n",
    "        return res\n",
    "\n",
    "    ds_dict = OrderedDict()\n",
    "\n",
    "    for ind, response in response_dict.items():\n",
    "        # print(ind)\n",
    "        # processing identifiers\n",
    "        system_id, timestamp, group = ind.split(\"_\")\n",
    "\n",
    "        if not system_id in ds_dict:\n",
    "            ds_dict[system_id] = OrderedDict()\n",
    "\n",
    "        if not timestamp in ds_dict[system_id]:\n",
    "            ds_dict[system_id][timestamp] = []\n",
    "\n",
    "        if isinstance(response, requests.models.Response):\n",
    "            res_dict = response.json()\n",
    "        elif isinstance(response, dict):\n",
    "            res_dict = response  # dict\n",
    "\n",
    "        if not 'data' in res_dict.keys():\n",
    "            if 'message' in res_dict.keys():\n",
    "                logging.warning(\"No 'data' in response: `%s`: `%s`\", ind, str(res_dict['message']))\n",
    "            continue\n",
    "        \n",
    "        res_meta = meta_stations[meta_stations.index == system_id].reset_index().loc[0]\n",
    "        res_locs = get_deployment(meta_deployment, system_id, timestamp)\n",
    "        res_refs = get_global_reference()\n",
    "        res_glob = get_global_attributes(output_subset)\n",
    "\n",
    "        for var in res_dict[\"data\"]:\n",
    "            # filters\n",
    "            dattr = OrderedDict(\n",
    "                [\n",
    "                    (k, v)\n",
    "                    for k, v in var.items()\n",
    "                    if not isinstance(v, list) and not isinstance(v, dict)\n",
    "                ]\n",
    "            )\n",
    "            # attributes\n",
    "            dattr = OrderedDict(\n",
    "                [\n",
    "                    (\"global\", OrderedDict(res_refs)),\n",
    "                    (\"station\", OrderedDict(res_locs)),\n",
    "                    (\"system\", OrderedDict(res_meta)),\n",
    "                    (\n",
    "                        \"cell\",\n",
    "                        OrderedDict(\n",
    "                            [(k, v) for k, v in var.items() if k not in [\"values\"]]\n",
    "                        ),\n",
    "                    ),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            # flatten attributes, convert values to json\n",
    "            ddattr = evaluate_attributes(dattr)\n",
    "\n",
    "            # remove meaningless variables? (e.g. Midnight)\n",
    "            if not \"name_original\" in list(var.keys()):\n",
    "                # print('SKIPPED: {}'.format(dattr['name']))\n",
    "                continue\n",
    "\n",
    "            if not res_locs:\n",
    "                # if station is not defined in metadata, skip\n",
    "                continue\n",
    "\n",
    "            # extraction of meaningful attributes:\n",
    "            station_id = dattr[\"station\"][\"station_id\"]\n",
    "            station_name = dattr[\"station\"][\"station_name\"]\n",
    "\n",
    "            system_name = str(dattr[\"station\"][\"system_name\"])\n",
    "            system_name_alt = str(dattr[\"system\"][\"info.device_name\"])\n",
    "\n",
    "            if system_name != system_name_alt and not ds_dict[system_id]:\n",
    "                logging.warning(\n",
    "                    \"Name mismatch (system_id,system_name,device_name): (`%s`,`%s`,`%s`)\",\n",
    "                    system_id,\n",
    "                    system_name,\n",
    "                    system_name_alt,\n",
    "                )\n",
    "\n",
    "            cell_attrs = [\n",
    "                get_fieldclimate_cell_methods(m) for m in var[\"values\"].keys()\n",
    "            ]\n",
    "            # print(cell_attrs)\n",
    "            cell_methods = [\n",
    "                n[\"cell_methods\"] if \"cell_methods\" in n.keys() else \"\"\n",
    "                for n in cell_attrs\n",
    "            ]\n",
    "\n",
    "            var_name = dattr[\"cell\"][\"name_original\"].replace(\" \", \"_\").replace(\",\", \"\")\n",
    "            var_long_name = dattr[\"cell\"][\"name_original\"]\n",
    "            var_units = dattr[\"cell\"][\"unit\"]\n",
    "\n",
    "            # Data variables\n",
    "            data_vars = OrderedDict(\n",
    "                [\n",
    "                    (\n",
    "                        \"{}\".format(var_name),\n",
    "                        (\n",
    "                            [\"system\", \"cell\", \"time\"],\n",
    "                            [[v for k, v in var[\"values\"].items()]],\n",
    "                            {\"long_name\": var_long_name, \"units\": var_units},\n",
    "                        ),\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "            attributes_values = (\n",
    "                \"attributes_values\".format(\"\"),\n",
    "                (\n",
    "                    [\"station\", \"system\", \"channel\", \"attributes\"],\n",
    "                    [[[ddattr[\"attributes_val\"]] * 1]],\n",
    "                    {\n",
    "                        \"description\": \"Attributes combined from meta data and input-file headers.\"\n",
    "                    },\n",
    "                ),\n",
    "            )\n",
    "\n",
    "            # Coordinates\n",
    "            coord_time = pd.to_datetime(res_dict[\"dates\"], format=\"ISO8601\")\n",
    "            coords = {\n",
    "                \"time\": (\n",
    "                    [\"time\"],\n",
    "                    coord_time,\n",
    "                    {\n",
    "                        \"long_name\": \"time\",\n",
    "                        \"standard_name\": \"time\",\n",
    "                        # \"calendar\": \"proleptic_gregorian\",  # xarray error\n",
    "                        # \"units\": \"microseconds since 1970-01-01 00:00:00 +0000\", # xarray error\n",
    "                    },\n",
    "                ),\n",
    "                \"station_id\": (\n",
    "                    [\"station\"],\n",
    "                    [str(dattr[\"station\"][\"station_id\"])],\n",
    "                    {},\n",
    "                ),\n",
    "                \"station_name\": (\n",
    "                    [\"station\"],\n",
    "                    [str(dattr[\"station\"][\"station_name\"])],\n",
    "                    {},\n",
    "                ),\n",
    "                \"station_lat\": (\n",
    "                    [\"station\"],\n",
    "                    [float(dattr[\"station\"][\"station_lat\"])],\n",
    "                    {},\n",
    "                ),\n",
    "                \"station_lon\": (\n",
    "                    [\"station\"],\n",
    "                    [float(dattr[\"station\"][\"station_lon\"])],\n",
    "                    {},\n",
    "                ),\n",
    "                \"station_height\": (\n",
    "                    [\"station\"],\n",
    "                    [float(dattr[\"station\"][\"station_height\"])],\n",
    "                    {},\n",
    "                ),\n",
    "                \"system_id\": (\n",
    "                    [\"system\"],\n",
    "                    [str(dattr[\"station\"][\"system_id\"])],  ### attention\n",
    "                    {},\n",
    "                ),\n",
    "                \"system_name\": (\n",
    "                    [\"system\"],\n",
    "                    [str(dattr[\"station\"][\"system_name\"])],  ### attention\n",
    "                    {},\n",
    "                ),\n",
    "                \"system_group\": (\n",
    "                    [\"system\"],\n",
    "                    [str(\"AWS\")],\n",
    "                    {},\n",
    "                ),\n",
    "                \"sensor_id\": (\n",
    "                    [\"sensor\"],\n",
    "                    [str(dattr[\"cell\"][\"group\"])],\n",
    "                    {},\n",
    "                ),\n",
    "                \"channel_id\": (\n",
    "                    [\"channel\"],\n",
    "                    [str(dattr[\"cell\"][\"ch\"])],\n",
    "                    {},\n",
    "                ),\n",
    "                \"cell_type\": (\n",
    "                    [\"cell\"],\n",
    "                    list(cell_methods),  ### atttenion\n",
    "                    {},\n",
    "                ),\n",
    "                \"attributes_id\": (\n",
    "                    [\"attributes\"],\n",
    "                    ddattr[\"attributes_key\"],\n",
    "                    {},\n",
    "                ),\n",
    "                \"attributes_group\": (\n",
    "                    [\"attributes\"],\n",
    "                    ddattr[\"attributes_id\"],\n",
    "                    {},\n",
    "                ),\n",
    "                \"attributes_type\": (\n",
    "                    [\"attributes\"],\n",
    "                    ddattr[\"attributes_type\"],\n",
    "                    {},\n",
    "                ),\n",
    "            }\n",
    "\n",
    "            # coords back to tuple\n",
    "            coords = [(k, v) for k, v in coords.items()]\n",
    "\n",
    "            # add attributes_values as coord or data_vars?\n",
    "            coords.append(attributes_values)\n",
    "\n",
    "            # Dataset (xarray)\n",
    "            ds = xr.Dataset(\n",
    "                data_vars=data_vars,\n",
    "                coords=dict(coords),\n",
    "                attrs=res_glob,\n",
    "            )\n",
    "\n",
    "            # Dimensions sort order\n",
    "            ds = ds.transpose(\n",
    "                \"time\",\n",
    "                \"station\",\n",
    "                \"system\",\n",
    "                \"sensor\",\n",
    "                \"channel\",\n",
    "                \"cell\",\n",
    "                \"attributes\",\n",
    "                missing_dims=\"ignore\",\n",
    "            )\n",
    "\n",
    "            # Encoding\n",
    "            encoder_list = [\n",
    "                n for n in output_encoder if n[\"name\"] in list(ds.variables)\n",
    "            ]\n",
    "            for encoder in encoder_list:\n",
    "                n = encoder[\"name\"]\n",
    "                enc = {k: v for k, v in encoder.items() if k not in [\"name\"]}\n",
    "                ds[n].encoding.update(**enc)\n",
    "                logging.debug(\"Encoding updated for `%s`: `%s`\", n, str(enc))\n",
    "\n",
    "            ds_dict[system_id][timestamp].append(ds)\n",
    "\n",
    "    return ds_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3460d527-c953-4937-8e5e-c7c91faedde6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f849db0f-8c73-4209-9311-8af1f145489f",
   "metadata": {},
   "source": [
    "## Further Data management "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8cd6183a-03d2-4094-9107-b9539b59a144",
   "metadata": {},
   "outputs": [],
   "source": [
    "def datasets_to_zarr_zip(dx, output_file, zipstore_args={}, **zarr_args):\n",
    "    from pathlib import Path\n",
    "\n",
    "    import zarr\n",
    "\n",
    "    # on the safe side, always start with an empty zarr.zip file.\n",
    "    zipstore_args_default = dict(\n",
    "        mode=\"w\",\n",
    "        allowZip64=True,\n",
    "    )\n",
    "    kw = {**zipstore_args_default, **zipstore_args}\n",
    "\n",
    "    if isinstance(dx, xr.Dataset):\n",
    "        dx = {None: dx}\n",
    "\n",
    "    fn = Path(output_file).with_suffix(\".zarr.zip\")\n",
    "    for k, ds in dx.items():\n",
    "        store = zarr.storage.ZipStore(fn, **kw)\n",
    "        ds.to_zarr(store, group=k, **zarr_args)\n",
    "        store.close()\n",
    "\n",
    "        # Any further group is to be updated/appended, by default\n",
    "        kw[\"mode\"] = \"a\"\n",
    "\n",
    "\n",
    "def export_fieldclimate_query_RAW(\n",
    "    export_dict,\n",
    "    output_path=\"/tmp/Fieldclimate/RAW/\",\n",
    "    output_file=\"PESSL_FieldClimate_QueryResponses_%Y%m%dT%H%M%S+0000\",\n",
    "    output_dict={},\n",
    "):\n",
    "    \"\"\"Export all queries to an archive file.\"\"\"\n",
    "    import io\n",
    "    import json\n",
    "    import os\n",
    "    import tarfile\n",
    "    import time\n",
    "    from pathlib import Path\n",
    "\n",
    "    # archive\n",
    "    fa = os.path.join(output_path, output_file).format(**output_dict)\n",
    "    fm = time.time()\n",
    "\n",
    "    logging.info(\"Query RAW file `%s`\", fa)\n",
    "\n",
    "    # path\n",
    "    Path(fa).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # add query response as files to archive\n",
    "    with tarfile.open(fa, \"w:gz\") as tar:\n",
    "        for ind, response in export_dict.items():\n",
    "            # processing identifiers\n",
    "            # station_id, time_timestamp, cell_group = ind.split(\"_\")\n",
    "            data_dict = (\n",
    "                response.json()\n",
    "                if isinstance(response, requests.models.Response)\n",
    "                else response\n",
    "            )\n",
    "            data = json.dumps(data_dict).encode(\"utf8\")\n",
    "            info = tarfile.TarInfo(name=\"{}.json\".format(ind))\n",
    "            info.size = len(data)\n",
    "            info.mtime = fm\n",
    "\n",
    "            tar.addfile(info, io.BytesIO(data))\n",
    "\n",
    "    return Path(fa).is_file()\n",
    "\n",
    "\n",
    "def import_fieldclimate_query_RAW(\n",
    "    input_path=\"/tmp/Fieldclimate/RAW/\",\n",
    "    input_file=\"PESSL_FieldClimate_QueryResponses_%Y%m%dT%H%M%S+0000\",\n",
    "    input_dict={},\n",
    "    include_all=False,\n",
    "):\n",
    "    \"\"\"Export all queries to an archive file.\"\"\"\n",
    "    import io\n",
    "    import json\n",
    "    import os\n",
    "    import tarfile\n",
    "    import time\n",
    "    from pathlib import Path\n",
    "\n",
    "    # archive\n",
    "    fa_pattern = os.path.join(input_path, input_file).format(**input_dict)\n",
    "    fa_list = sorted(glob.glob(fa_pattern, recursive=True), reverse=True)\n",
    "\n",
    "    if fa_list:\n",
    "        fa = sorted(fa_list, reverse=True)  # assuming date folder sorting key\n",
    "        logging.info(\"Found RAW files, N=%s\", len(fa_list))\n",
    "        logging.info(\"Reading RAW file `%s`\", fa)\n",
    "    else:\n",
    "        fa = []\n",
    "        logging.info(\"No RAW files found, at `%s`\", fa_pattern)\n",
    "\n",
    "    query_list = []\n",
    "    for fa in fa_list:\n",
    "        meta_response_keys = [\n",
    "            \"user\",\n",
    "            \"user_stations\",\n",
    "            \"system_groups\",\n",
    "            \"system_sensors\",\n",
    "        ]\n",
    "        meta_response = {}\n",
    "        response_dict = {}\n",
    "        meta_stations = None\n",
    "\n",
    "        if not Path(fa).is_file():\n",
    "            logging.info(\"File not found\", fa)\n",
    "        else:\n",
    "            # extract query response as files in archive\n",
    "            with tarfile.open(fa, \"r:gz\") as tar:\n",
    "                for member in tar.getmembers():\n",
    "                    f = tar.extractfile(member)\n",
    "                    data = f.read()\n",
    "                    response = json.loads(data.decode(\"utf8\"))\n",
    "                    ind = str(Path(member.name).with_suffix(\"\"))\n",
    "                    if ind in meta_response_keys:\n",
    "                        meta_response[ind] = response\n",
    "                    else:\n",
    "                        response_dict[ind] = response\n",
    "\n",
    "            meta_stations = get_fieldclimate_metadata_dataframe(\n",
    "                meta_response, section=\"user_stations\"\n",
    "            )\n",
    "\n",
    "            # remove the test station.\n",
    "            meta_stations = meta_stations[\n",
    "                ~meta_stations[\"name.original\"].str.startswith(\"03A0D07B\")\n",
    "            ]\n",
    "\n",
    "            # sort by unique identifier (station_id)\n",
    "            meta_stations = meta_stations.reset_index().set_index(\"name.original\")\n",
    "\n",
    "            query_list.append(tuple([fa, meta_response, response_dict, meta_stations]))\n",
    "\n",
    "            if query_list and not include_all:\n",
    "                break\n",
    "\n",
    "    return query_list\n",
    "\n",
    "\n",
    "def export_fieldclimate_station_datasets_L0(\n",
    "    dx,\n",
    "    output_path=\"/tmp/Fieldclimate/L0/\",\n",
    "):\n",
    "    \"\"\"Export each Fieldclimate query to a separate NetCDF\"\"\"\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "\n",
    "    logging.info(\"Query L0 files `%s`\", output_path)\n",
    "\n",
    "    for i in list(dx.keys()):\n",
    "        # merge on channels\n",
    "        for j in list(dx[i].keys()):\n",
    "            for k, v in enumerate(dx[i][j]):\n",
    "                if isinstance(v, xr.Dataset):\n",
    "                    # print(v)\n",
    "                    fp = os.path.join(output_path, i)  # output path + station\n",
    "                    Path(fp).mkdir(parents=True, exist_ok=True)\n",
    "                    fn = os.path.join(fp, \"{}_{}_{}.nc\".format(i, j, k))\n",
    "                    try:\n",
    "                        v.to_netcdf(fn)\n",
    "                    except (TypeError, OSError):\n",
    "                        logging.info(\"Query L0 file `%s` could not be written.\", fn)\n",
    "                        pass\n",
    "                        # print(fn)\n",
    "\n",
    "\n",
    "def export_fieldclimate_station_datasets_L0_merged(\n",
    "    ds,\n",
    "    output_path=\"/tmp/Fieldclimate/L0/\",\n",
    "    output_file=\"urbisphere_set({global_location},{system_group},{time_bounds})_version({version}).nc\",\n",
    "    output_dict={},\n",
    "    output_extension=\".zarr.zip\",\n",
    "):\n",
    "    from pathlib import Path\n",
    "\n",
    "    if query_latest:\n",
    "        tr = [x.strftime(\"%Y%m%dT%H%M%S\") for x in list(query_range)]\n",
    "    else:\n",
    "        tr = [\n",
    "            pd.to_datetime(str(ds.time.min().values), format=\"ISO8601\").strftime(\n",
    "                \"%Y%m%dT%H%M%S\"\n",
    "            ),\n",
    "            pd.to_datetime(str(ds.time.max().values), format=\"ISO8601\").strftime(\n",
    "                \"%Y%m%dT%H%M%S\"\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "    if not \"time_bounds\" in output_dict:\n",
    "        output_dict[\"time_bounds\"] = tr[0] if tr[0] == tr[1] else \"{}_{}\".format(*tr)\n",
    "\n",
    "    for k in [\"station_group\", \"system_group\", \"system_type\"]:\n",
    "        if k in ds.coords:\n",
    "            output_dict[k] = str(ds[k].values[0]).replace(\"\\xa0\", \" \").replace(\" \", \"\")\n",
    "        else:\n",
    "            output_dict[k] = \"\"\n",
    "\n",
    "    fn = os.path.join(output_path, output_file).format(**output_dict)\n",
    "\n",
    "    # input(\"\\nPress Enter to continue...\")\n",
    "\n",
    "    Path(fn).parent.mkdir(parents=True, exist_ok=True)\n",
    "    try:\n",
    "        if output_extension == \".zarr.zip\":\n",
    "            logging.info(\"L0 file `%s`\", Path(fn).with_suffix(output_extension))\n",
    "            datasets_to_zarr_zip(ds, fn)\n",
    "            \n",
    "            ## Temporary duplicate exports for UniWeather (2024-11-27)        \n",
    "            if \"scratch\" in Path(fn).parent.parts and \"L0\" in Path(fn).parent.parts and version[\"id\"] == \"v1.0.5\":\n",
    "                fn_latest = Path(fn.replace('v1.0.5','latest'))\n",
    "                fn_latest.parent.mkdir(parents=True, exist_ok=True)\n",
    "                logging.info(\"+++ L0 file `%s`\", fn_latest)\n",
    "                ds.to_netcdf(fn)                   \n",
    "        else:\n",
    "            logging.info(\"L0 file `%s`\", fn)\n",
    "            ds.to_netcdf(fn)\n",
    "    except (TypeError, OSError):\n",
    "        logging.info(\"L0 file `%s` could not be written.\", fn)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38667bb-3c4a-48b8-89d1-a9cc46c2d970",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_fieldclimate_station_datasets(dx, filters=[]):\n",
    "    \"\"\"Drop empty query results\"\"\"\n",
    "    for i in list(dx.keys()):\n",
    "        # filter retrieval\n",
    "        dx[i] = {k: v for k, v in dx[i].items() if not v == []}\n",
    "\n",
    "        # drop empty stations\n",
    "        if not dx[i]:\n",
    "            logging.info(\"Station skipped: %s\", i)\n",
    "            dx.pop(i, None)\n",
    "            continue\n",
    "\n",
    "        for f in filters:\n",
    "            fk = list(f.keys())[0]\n",
    "            fv = list(f.values())[0]\n",
    "            dx[i] = {k: v for k, v in dx[i].items() for vv in v if vv[fk] != fv}\n",
    "            if not dx[i]:\n",
    "                logging.info(\"Station dropped: %s\", i)\n",
    "                dx.pop(i, None)\n",
    "                continue\n",
    "\n",
    "    return dx\n",
    "\n",
    "\n",
    "def merge_fieldclimate_station_datasets(dx):\n",
    "    \"\"\"Merge a dict of Xarray datasets into one.\"\"\"\n",
    "    for i in list(dx.keys()):\n",
    "        # drop unneeded dimensions (min, max, ...):\n",
    "        # - cell, currently only pick the first column (to be revised)\n",
    "        for j in list(dx[i].keys()):\n",
    "            for n in range(len(dx[i][j])):\n",
    "                for k in [0]:\n",
    "                    # the cell_type coordinate is later removed, here we copy the\n",
    "                    # info to the cell_methods attiribute for the selected variable.\n",
    "                    if \"cell_type\" in dx[i][j][n].coords:\n",
    "                        dv = [\n",
    "                            i\n",
    "                            for i in dx[i][j][n].data_vars\n",
    "                            if not i.endswith(\"_attributes\")\n",
    "                        ]\n",
    "                        for d in dv:\n",
    "                            dx[i][j][n][d].attrs[\"cell_methods\"] = (\n",
    "                                dx[i][j][n].coords[\"cell_type\"].values[k]\n",
    "                            )\n",
    "                    dx[i][j][n] = dx[i][j][n].isel(cell=k)\n",
    "\n",
    "        # merge on channels\n",
    "        for j in list(dx[i].keys()):\n",
    "            dx[i][j] = xr.merge(dx[i][j], compat=\"override\")\n",
    "\n",
    "            # Drop duplicates in the time coordinate. A result of inclusive boundaries for the retrieval by time.\n",
    "            dx[i][j].drop_duplicates(\"time\")\n",
    "            unique_time, unique_index = np.unique(dx[i][j].time, return_index=True)\n",
    "            dx[i][j] = dx[i][j].isel(time=unique_index)\n",
    "\n",
    "        # merge on time\n",
    "        dx[i] = xr.merge(dx[i].values())\n",
    "\n",
    "    # concat on station\n",
    "    dx = xr.concat(dx.values(), \"station\")\n",
    "\n",
    "    # remove unusable coordinates and dimensions\n",
    "    dx = dx.drop_dims([\"sensor\"]).drop(\"cell_type\")\n",
    "    # dx.attrs[\"production_date\"] = get_creation_time()\n",
    "\n",
    "    # Dimensions sort order\n",
    "    dx = dx.transpose(\n",
    "        \"time\",\n",
    "        \"station\",\n",
    "        \"system\",\n",
    "        \"sensor\",\n",
    "        \"channel\",\n",
    "        \"cell\",\n",
    "        \"attributes\",\n",
    "        missing_dims=\"ignore\",\n",
    "    )\n",
    "\n",
    "    # Encoding\n",
    "    encoder_list = [n for n in output_encoder if n[\"name\"] in list(dx.variables)]\n",
    "    for encoder in encoder_list:\n",
    "        n = encoder[\"name\"]\n",
    "        enc = {k: v for k, v in encoder.items() if k not in [\"name\"]}\n",
    "        dx[n].encoding.update(**enc)\n",
    "        logging.info(\"Encoding updated for `%s`: `%s`\", n, str(enc))\n",
    "\n",
    "    return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5c6212-1aa2-4773-9657-3acf6e3a5231",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fieldclimate_raw(apiConf, debug=False):\n",
    "    ## fieldclimate metadata\n",
    "    meta_response = get_fieldclimate_metadata(apiConf)\n",
    "\n",
    "    # process\n",
    "    if debug:\n",
    "        meta_groups = get_fieldclimate_metadata_dataframe(\n",
    "            meta_response, section=\"system_groups\"\n",
    "        )\n",
    "        meta_sensors = get_fieldclimate_metadata_dataframe(\n",
    "            meta_response, section=\"system_sensors\"\n",
    "        )\n",
    "\n",
    "    # stations listing as dataframe\n",
    "    meta_stations = get_fieldclimate_metadata_dataframe(\n",
    "        meta_response, section=\"user_stations\"\n",
    "    )\n",
    "\n",
    "    # remove the test station.\n",
    "    meta_stations = meta_stations[\n",
    "        ~meta_stations[\"name.original\"].str.startswith(\"03A0D07B\")\n",
    "    ]\n",
    "\n",
    "    # sort by unique identifier (station_id)\n",
    "    meta_stations = meta_stations.reset_index().set_index(\"name.original\")\n",
    "\n",
    "    ## fieldclimate metadata, extract data and metadata for each station\n",
    "    # prepare REST queries\n",
    "    query_stations = meta_stations.index.to_list()\n",
    "    query_list = get_fieldclimate_station_queries(\n",
    "        query_stations, query_range, convert_to_unix_timestamp=True\n",
    "    )\n",
    "\n",
    "    if debug:\n",
    "        display(query_list)\n",
    "\n",
    "    # get REST responses\n",
    "    response_dict = get_fieldclimate_station(apiConf, query_list)\n",
    "\n",
    "    if query_cache:\n",
    "        pass\n",
    "\n",
    "    # export temporary files (json)\n",
    "    res = export_fieldclimate_query_RAW(\n",
    "        OrderedDict(list(meta_response.items()) + list(response_dict.items())),\n",
    "        output_file=cache_file,\n",
    "        output_path=os.path.join(cache_path_base, cache_path),\n",
    "        output_dict=cache_subset,\n",
    "    )\n",
    "\n",
    "    return (meta_response, response_dict, meta_stations)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def fieldclimate_raw_subset(meta_response, response_dict, meta_deployment):\n",
    "    import re\n",
    "\n",
    "    def get_deployment(meta_deployment, system_id, timestamp):\n",
    "        ts = pd.to_datetime(float(timestamp), unit=\"s\", origin=\"unix\")\n",
    "\n",
    "        res = {}\n",
    "\n",
    "        dep = meta_deployment.loc[\n",
    "            (\n",
    "                (meta_deployment[(\"id\", \"system_id\")] == system_id)\n",
    "                & (meta_deployment[(\"configuration\", \"d_start_date\")] <= ts)\n",
    "                & (meta_deployment[(\"configuration\", \"d_end_date\")] > ts)\n",
    "            ),\n",
    "            :,\n",
    "        ]\n",
    "\n",
    "        if not dep.empty:\n",
    "            dep_subset = pd.concat(\n",
    "                [\n",
    "                    dep.iloc[[0]].loc(axis=1)[\"id\", :],\n",
    "                    dep.iloc[[0]].loc(axis=1)[\n",
    "                        \"configuration\", [\"d_id\", \"c_id\", \"i_id\", \"m_id\", \"s_id\"]\n",
    "                    ],\n",
    "                ],\n",
    "                axis=1,\n",
    "            ).droplevel(axis=1, level=0)\n",
    "            # ALT: dep_subset.columns = dep_subset.columns.to_flat_index().str.join('_')\n",
    "\n",
    "            res = {**res, **dep_subset.iloc[0].to_dict()}\n",
    "\n",
    "        return res\n",
    "\n",
    "    def get_meta_response_redacted(meta_response):\n",
    "        import operator\n",
    "        from copy import deepcopy\n",
    "        from functools import reduce  # forward compatibility for Python 3\n",
    "\n",
    "        k = [\n",
    "            (\"user\", \"username\"),\n",
    "            (\"user\", \"info\", \"*\"),\n",
    "            (\"user\", \"api_access\", \"hmac\", \"*\"),\n",
    "            (\"user\", \"api_access\", \"oauth2\", \"*\"),\n",
    "            (\"user_stations\", \"*\", \"networking\", \"simid\"),\n",
    "            (\"user_stations\", \"*\", \"networking\", \"imei\"),\n",
    "            (\"user_stations\", \"*\", \"networking\", \"imsi\"),\n",
    "        ]\n",
    "        v = \"***\"\n",
    "        d = deepcopy(meta_response)\n",
    "\n",
    "        def find_key_nonrecursive(adict, key):\n",
    "            stack = [adict]\n",
    "            while stack:\n",
    "                d = stack.pop()\n",
    "                if key in d:\n",
    "                    return d[key]\n",
    "                for v in d.values():\n",
    "                    if isinstance(v, dict):\n",
    "                        stack.append(v)\n",
    "                    if isinstance(v, list):\n",
    "                        stack += v\n",
    "\n",
    "        def get_by_path(root, items):\n",
    "            \"\"\"Access a nested object in root by item sequence.\"\"\"\n",
    "            return reduce(operator.getitem, items, root)\n",
    "\n",
    "        def set_by_path(root, items, value):\n",
    "            \"\"\"Set a value in a nested object in root by item sequence.\"\"\"\n",
    "            get_by_path(root, items[:-1])[items[-1]] = value\n",
    "\n",
    "        def get_path_list(d, path):\n",
    "            kk = []\n",
    "            if path[-1] == \"*\":\n",
    "                kd = list(n for n in path[0:-1])\n",
    "                try:\n",
    "                    nd = get_by_path(d, kd)\n",
    "                except KeyError:\n",
    "                    nd = {}\n",
    "                    pass \n",
    "                if isinstance(nd, dict):\n",
    "                    for n, m in nd.items():\n",
    "                        kk.append(tuple(kd + [n]))\n",
    "                if isinstance(nd, list):\n",
    "                    if len(nd) > 0:\n",
    "                        for n, m in enumerate(nd):\n",
    "                            kk.append(tuple(kd + [n]))\n",
    "                    else:\n",
    "                        # kk.append(tuple(kd + [0]))\n",
    "                        pass  # no need to replace an empty array.                        \n",
    "            elif path[1] == \"*\":\n",
    "                if isinstance(d[path[0]], list):\n",
    "                    for n, v in enumerate(d[path[0]]):\n",
    "                        kk.append(tuple([path[0], n, *path[2:]]))\n",
    "            else:\n",
    "                kk.append(path)\n",
    "\n",
    "            return kk\n",
    "\n",
    "        nk = [tuple(n) for k in k for n in get_path_list(d, k)]\n",
    "        for n in nk:\n",
    "            try:\n",
    "                set_by_path(d, n, v)\n",
    "            except KeyError:\n",
    "                pass            \n",
    "            \n",
    "\n",
    "        return d\n",
    "\n",
    "    meta_response_subset = get_meta_response_redacted(meta_response)\n",
    "\n",
    "    response_dict_subset = OrderedDict()\n",
    "\n",
    "    for ind, response in response_dict.items():\n",
    "        # print(ind)\n",
    "        # processing identifiers\n",
    "        system_id, timestamp, group = ind.split(\"_\")\n",
    "\n",
    "        if isinstance(response, requests.models.Response):\n",
    "            res_dict = response.json()\n",
    "        elif isinstance(response, dict):\n",
    "            res_dict = response  # dict\n",
    "\n",
    "        res_locs = get_deployment(meta_deployment, system_id, timestamp)\n",
    "\n",
    "        if res_locs:\n",
    "            # if station is defined in metadata, keep\n",
    "            response_dict_subset[ind] = response\n",
    "\n",
    "    return (meta_response_subset, response_dict_subset)\n",
    "\n",
    "\n",
    "def fieldclimate_raw_filtered(\n",
    "    input_filename, meta_response, response_dict, meta_deployment\n",
    "):\n",
    "    meta_response_subset, response_dict_subset = fieldclimate_raw_subset(\n",
    "        meta_response, response_dict, meta_deployment\n",
    "    )\n",
    "\n",
    "    cache_output_subset = {\n",
    "        **cache_subset,\n",
    "        **{k: v for k, v in output_subset.items() if k in [\"global_location\"]},\n",
    "    }\n",
    "\n",
    "    cache_output_file = str(\n",
    "        Path(input_filename).relative_to(\n",
    "            rebase_path(\n",
    "                path_base=\"FieldclimateAPI\", path_root=Path(input_filename).parent\n",
    "            )\n",
    "        )\n",
    "    ).replace(cache_subset[\"global_location\"], output_subset[\"global_location\"])\n",
    "\n",
    "    res = export_fieldclimate_query_RAW(\n",
    "        OrderedDict(\n",
    "            list(meta_response_subset.items()) + list(response_dict_subset.items())\n",
    "        ),\n",
    "        output_file=cache_output_file,\n",
    "        output_path=os.path.join(output_path_base, output_path),\n",
    "        output_dict=cache_output_subset,\n",
    "    )\n",
    "\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4dad9f-ae9f-4f16-bf96-ecfca7ee67f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fieldclimate_l0(response_dict, meta_stations, meta_deployment):\n",
    "    # convert responses to DataSets\n",
    "    dx = get_fieldclimate_station_datasets(\n",
    "        response_dict, meta_stations, meta_deployment\n",
    "    )\n",
    "    # filter for empty reponses\n",
    "    dx = filter_fieldclimate_station_datasets(dx)\n",
    "\n",
    "    if not query_latest:\n",
    "        mod_output_subset = {\n",
    "            **output_subset,\n",
    "            **{\n",
    "                \"version_id\": version[\"id\"],\n",
    "                \"production_level\": \"L0\",\n",
    "                # \"path_destination\": \"by-serialnr/Germany/Freiburg\",\n",
    "                \"global_location\": \"anywhere\",\n",
    "                # \"station_id\": \"Tier2/Fieldclimate\",\n",
    "                \"time_query\": \"\",\n",
    "            },\n",
    "        }\n",
    "\n",
    "        # export L0 files (netcdf)\n",
    "        res = export_fieldclimate_station_datasets_L0(\n",
    "            dx,\n",
    "            output_path=os.path.join(cache_path_base, cache_path).format(\n",
    "                **mod_output_subset\n",
    "            ),\n",
    "        )\n",
    "    return dx\n",
    "\n",
    "\n",
    "def fieldclimate_l0_combine(dx):\n",
    "    # filter: only deployed systems\n",
    "    dx = filter_fieldclimate_station_datasets(dx.copy(), filters=[{\"station_name\": \"\"}])\n",
    "\n",
    "    # merge DataSets into one\n",
    "    logging.info(\n",
    "        \"Align and merge coords and data_vars for all stations into a new xr.Dataset\"\n",
    "    )\n",
    "    ds = merge_fieldclimate_station_datasets(dx)\n",
    "\n",
    "    # export\n",
    "    if not query_latest:\n",
    "        mod_output_subset = {\n",
    "            **output_subset,\n",
    "            **{\n",
    "                \"version_id\": version[\"id\"],\n",
    "                \"production_level\": \"L0\",\n",
    "                # \"path_destination\": \"by-serialnr/Germany/Freiburg\",\n",
    "                # \"global_location\": \"de.freiburg\",\n",
    "                # \"station_id\": \"Tier2/Fieldclimate\",\n",
    "                \"time_query\": \"\",\n",
    "                #\"sensor_id\": \"LoRAIN\",\n",
    "                #\"system_id\": \"LoRAIN\",\n",
    "            },\n",
    "        }\n",
    "        export_fieldclimate_station_datasets_L0_merged(\n",
    "            ds,\n",
    "            output_path=os.path.join(cache_path_base, cache_path),\n",
    "            output_file=cache_file,\n",
    "            output_dict=mod_output_subset,\n",
    "        )\n",
    "\n",
    "    # split station and system\n",
    "    logging.info(\"Extract single stations from the merged xr.Dataset.\")\n",
    "    for i, g in ds.groupby(\"station\", squeeze=False):\n",
    "        for j, gg in g.groupby(\"system\", squeeze=False):\n",
    "            logging.info(\"Station `%s` System `%s`\", i, j)\n",
    "            output_dict = {\n",
    "                **output_subset,\n",
    "                **{\n",
    "                    \"version_id\": version[\"id\"],\n",
    "                    \"production_level\": \"L0\",\n",
    "                    # \"path_destination\": \"by-location/Germany/Freiburg\",\n",
    "                    # \"global_location\": \"de.freiburg\",\n",
    "                    \"station_id\": gg[\"station_id\"].values[0],\n",
    "                    \"system_id\": gg[\"system_id\"].values[0][0],\n",
    "                    \"sensor_id\": gg[\"system_id\"].values[0][0],  ## attention\n",
    "                },\n",
    "            }\n",
    "            export_fieldclimate_station_datasets_L0_merged(\n",
    "                gg,\n",
    "                output_path=os.path.join(output_path_base, output_path),\n",
    "                output_file=output_file,\n",
    "                output_dict=output_dict,\n",
    "            )\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fb828c-5a29-408e-8c10-a74478ce4b19",
   "metadata": {},
   "source": [
    "# Configuration\n",
    "\n",
    "## Static configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092d791a-afa4-4607-99d9-95eada07352c",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Version\n",
    "version = {\n",
    "    \"id\": \"v1.0.0\",\n",
    "    \"time\": \"2022-05-05\",\n",
    "}  # first version.\n",
    "version = {\n",
    "    \"id\": \"v1.0.1\",\n",
    "    \"time\": \"2022-08-03\",\n",
    "}  # update version.\n",
    "version = {\n",
    "    \"id\": \"v1.0.2\",\n",
    "    \"time\": \"2022-12-12\",\n",
    "}  # updated version, configuration files\n",
    "version = {\n",
    "    \"id\": \"v1.0.3\",\n",
    "    \"time\": \"2023-02-16\",\n",
    "}  # updated version, configuration files\n",
    "version = {\n",
    "    \"id\": \"v1.0.4\",\n",
    "    \"time\": \"2023-09-25\",\n",
    "}  # updated version, configuration files\n",
    "version = {\n",
    "    \"id\": \"v1.0.5\",\n",
    "    \"time\": \"2024-08-24\",\n",
    "}  # updated version, configuration files\n",
    "\n",
    "\n",
    "# Configuration file for input / output files\n",
    "ioconfig_name = \"fieldclimate_metadata\"\n",
    "try:\n",
    "    ioconfig_file = \"../conf/{}.toml\".format(ioconfig_name)\n",
    "    if not Path(ioconfig_file).exists():\n",
    "        raise\n",
    "except:\n",
    "    ioconfig_file = \"conf/{}.toml\".format(ioconfig_name)\n",
    "# ----- Papermill injection below this cell -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ec1200-9b25-4644-8498-953cdcd18d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input/output config\n",
    "ioconf = parse_config(ioconfig_file, ioconfig_name, version)\n",
    "\n",
    "# validate config (to do)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce01e5c3-414d-4abe-a2f7-a0b2228dbf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Note: the approach to set global helper variables should be revised. \n",
    "But was/is used in combination with papermill automation.\n",
    "\"\"\"\n",
    "\n",
    "# set global variables\n",
    "query_from = ioconf[\"query\"][\"start\"]\n",
    "query_to = None\n",
    "query_period = ioconf[\"query\"][\"period\"]\n",
    "query_index = ioconf[\"query\"][\"system_index\"]\n",
    "query_cache = ioconf[\"query\"][\"cache\"]\n",
    "query_latest = ioconf[\"query\"][\"latest\"]\n",
    "query_tasks = ioconf[\"query\"][\"tasks\"]\n",
    "query_key_file = ioconf[\"query\"][\"key_file\"]\n",
    "query_city = None if not \"city\" in ioconf[\"query\"] else ioconf[\"query\"][\"city\"]\n",
    "\n",
    "input_path_base = ioconf[\"input\"][\"path_base\"]\n",
    "input_path = None if not \"path\" in ioconf[\"input\"] else ioconf[\"input\"][\"path\"]\n",
    "# input_file = ioconf[\"input\"][\"file\"]\n",
    "# input_subset = ioconf[\"input\"][\"subset\"]\n",
    "cache_path_base = ioconf[\"cache\"][\"path_base\"]\n",
    "cache_path = ioconf[\"cache\"][\"path\"]\n",
    "cache_file = ioconf[\"cache\"][\"file\"]\n",
    "output_path_base = ioconf[\"output\"][\"path_base\"]\n",
    "output_path = ioconf[\"output\"][\"path\"]\n",
    "output_file = ioconf[\"output\"][\"file\"]\n",
    "output_subset = ioconf[\"output\"][\"subset\"]\n",
    "output_encoder = ioconf[\"output\"][\"encoder\"]\n",
    "\n",
    "log_path = ioconf[\"logging\"][\"path\"]\n",
    "log_file = ioconf[\"logging\"][\"file\"]\n",
    "log_format = ioconf[\"logging\"][\"format\"]\n",
    "log_filemode = (\n",
    "    \"a\" if not \"filemode\" in ioconf[\"logging\"] else ioconf[\"logging\"][\"filemode\"]\n",
    ")\n",
    "\n",
    "gattrs = ioconf[\"gattrs\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4ec2af-305c-464d-b11b-b352b830a132",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Logging Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310c4482-a97c-4357-9b33-49866670d0b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create logger\n",
    "import logging\n",
    "import logging.handlers\n",
    "from pprint import pformat\n",
    "\n",
    "logging.basicConfig(\n",
    "    encoding=\"utf-8\",\n",
    "    format=log_format,\n",
    "    level=logging.INFO,\n",
    "    # Declare handlers\n",
    "    handlers=[\n",
    "        logging.FileHandler(\n",
    "            os.path.join(log_path, log_file).format(version_id=version[\"id\"]),\n",
    "            mode=log_filemode,\n",
    "        ),\n",
    "        logging.StreamHandler(sys.stdout),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5279487e-3526-4d9e-9159-c4a8015de8db",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dynamic Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16b638a-a6c8-4636-a5a9-99cb6b2c0fe9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# time related helper variables\n",
    "if not query_to and query_period:\n",
    "    query_to = (\n",
    "        pd.to_datetime(query_from, format=\"ISO8601\") + pd.to_timedelta(query_period)\n",
    "    ).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "query_range = pd.to_datetime([query_from, query_to], format=\"ISO8601\")\n",
    "\n",
    "# meta data query\n",
    "# meta_deployment = get_metadata_inventory_deployment()\n",
    "meta_deployment = get_metadata_metadb_deployment(query_city, query_range)\n",
    "\n",
    "\n",
    "# cache settings\n",
    "cache_subset = {\n",
    "    **output_subset,\n",
    "    **{\n",
    "        \"version_id\": version[\"id\"],\n",
    "        \"production_level\": \"RAW\",\n",
    "        \"global_location\": \"anywhere\",\n",
    "        # \"station_id\": \"Tier2/Fieldclimate\",\n",
    "        \"system_name\": \"LoRAIN\",\n",
    "        \"time_bounds\": get_time_bounds(query_range),\n",
    "        \"time_query\": {\n",
    "            \"v1.0.4\": datetime.utcnow().strftime(\"%Y-%m-%d\"),\n",
    "            \"v1.0.5\": datetime.utcnow().strftime(\"dupes/by-upload-date/%Y-%m-%d\"),\n",
    "        }[version[\"id\"]],\n",
    "        \"extension\": \"tar.gz\",\n",
    "    },\n",
    "}\n",
    "\n",
    "input_cache_subset = {**cache_subset, **{\"time_query\": \"**\", \"version_id\": \"v1.0.?\"}}\n",
    "\n",
    "# Fieldclimate Connection Settings\n",
    "with open(query_key_file, \"r\") as keyfile:\n",
    "    apiKeys = json.load(keyfile)\n",
    "\n",
    "apiConf = {\n",
    "    **dict(apiURI=input_path_base),  # Endpoint of the API\n",
    "    **apiKeys,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7648dad5-9fd2-4c74-a316-76ea5799908a",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fffdb3-0580-4756-b09e-6e7251ba00f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    logging.info(\"Configuration context: `query_range`: %s\", query_range.tolist())\n",
    "    logging.info(\"Configuration context: `query_latest`: %s\", query_latest)\n",
    "    logging.info(\"Configuration context: `query_tasks`: %s\", query_tasks)\n",
    "    logging.info(\"Configuration context: `query_city`: %s\", query_city)\n",
    "    logging.info(\"Configuration context: `query_cache`: %s\", query_cache)\n",
    "    logging.info(\"Configuration context: `input_path_base`: %s\", input_path_base)\n",
    "    logging.info(\"Configuration context: `input_path`: %s\", input_path)\n",
    "    logging.info(\"Configuration context: `cache_path_base`: %s\", cache_path_base)\n",
    "    logging.info(\"Configuration context: `cache_path`: %s\", cache_path)\n",
    "    # query\n",
    "    if \"query\" in query_tasks:\n",
    "        logging.info(\"Task group `query`\")\n",
    "        meta_response, response_dict, meta_stations = fieldclimate_raw(apiConf)\n",
    "        query_list = [tuple([meta_response, response_dict, meta_stations])]\n",
    "    else:\n",
    "        # delay the processing, until a schedule alignment solution has been found\n",
    "        if query_latest:\n",
    "            time.sleep(60)\n",
    "\n",
    "    # cleanup data protection strings from raw responses\n",
    "    if \"clean\" in query_tasks:\n",
    "        logging.info(\"Task group `clean`\")\n",
    "        if not \"query\" in query_tasks:\n",
    "            pass\n",
    "        \n",
    "        query_list = import_fieldclimate_query_RAW(\n",
    "            input_file=cache_file,\n",
    "            input_path=os.path.join(\n",
    "                cache_path_base, input_path if input_path else cache_path\n",
    "            ),\n",
    "            input_dict=input_cache_subset,\n",
    "        )\n",
    "\n",
    "        dc = None\n",
    "        for cache_filename, meta_response, response_dict, meta_stations in query_list:\n",
    "            if not dc:\n",
    "                dc = fieldclimate_raw_filtered(\n",
    "                    cache_filename, meta_response, response_dict, meta_deployment\n",
    "                )\n",
    "\n",
    "    # convert\n",
    "    if \"convert\" in query_tasks:\n",
    "        logging.info(\"Task group `convert`\")\n",
    "        if not \"query\" in query_tasks:\n",
    "            pass\n",
    "\n",
    "        query_list = import_fieldclimate_query_RAW(\n",
    "            input_file=cache_file,\n",
    "            input_path=os.path.join(\n",
    "                cache_path_base, input_path if input_path else cache_path\n",
    "            ),\n",
    "            input_dict=input_cache_subset,\n",
    "        )\n",
    "\n",
    "        dx = None\n",
    "        for cache_filename, meta_response, response_dict, meta_stations in query_list:\n",
    "            if not dx:\n",
    "                dx = fieldclimate_l0(response_dict, meta_stations, meta_deployment)\n",
    "\n",
    "    # combine\n",
    "    if \"combine\" in query_tasks:\n",
    "        logging.info(\"Task group `combine`\")\n",
    "        if dx:\n",
    "            ddx = fieldclimate_l0_combine(dx)\n",
    "        else:\n",
    "            logging.warning(\"Warning: No data to process.\")\n",
    "\n",
    "    logging.info(\"End.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a83b9ac-d65f-44f7-b8ef-e80d06b91587",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f29be81-ee3d-466e-963f-3eb0de33843d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8595f5-8a99-492d-9255-433d00838a0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7daef1a7-4f9b-4eea-848e-51014ea1d8f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
