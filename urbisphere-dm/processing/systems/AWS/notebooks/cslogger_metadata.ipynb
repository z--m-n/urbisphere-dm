{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21897add-e87f-409b-b982-71e6f69cb0a5",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Created on Sat Jul 23 15:11:15 2022\n",
    "\n",
    "Modified on Fri Nov 25 13:37:01 2022\n",
    "v1.0.2 - code clean-up\n",
    "\n",
    "@author: zeeman-m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53a0c67c-ece9-4bc8-be3a-d5a702e3c7e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.core.options.set_options at 0x10e25e4d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Other requirements\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "import diskcache\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly as py\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "import toml\n",
    "import xarray as xr\n",
    "from plotly.subplots import make_subplots\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "xr.set_options(keep_attrs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a12f6f87-54a2-406e-b85c-248a24193e70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rebase_path(path_base=\"urbisphere-dm\", path_root=None):\n",
    "    \"\"\"return abs path of a higher level directory\"\"\"\n",
    "    from pathlib import Path\n",
    "\n",
    "    path_root = Path(\"__file__\").parent.resolve() if not path_root else path_root\n",
    "    path_parts = lambda p: p[0 : (p.index(path_base) + 1 if path_base in p else len(p))]\n",
    "    return str(Path(*[n for n in path_parts(Path(path_root).parts)]))\n",
    "\n",
    "\n",
    "sys.path.append(os.path.join(rebase_path(), \"interfaces/metadb/notebooks/\"))\n",
    "from ipynb.fs.full.metadb_query import metadb_query as metadb_query_subset_table\n",
    "from ipynb.fs.full.metadb_query import metadb_sql_query as metadb_sql_query\n",
    "from ipynb.fs.full.metadb_query import metadb_sql_response as metadb_sql_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb1106f-c966-4baa-bbda-7fcdfdfaedfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StopExecution(Exception):\n",
    "    def _render_traceback_(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "def get_dictlist_permutations(input_subset):\n",
    "    \"\"\"Expand a dict of (strs|lists) into all possible permutations.\"\"\"\n",
    "    import itertools\n",
    "\n",
    "    keys, values = zip(*input_subset.items())\n",
    "    values = [v if isinstance(v, list) else [v] for v in values]\n",
    "    permutations_dicts = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "    return permutations_dicts\n",
    "\n",
    "\n",
    "def get_dictlist_flatten(input_subset, joinstr=\"+\"):\n",
    "    \"\"\"Flatten dict of (strs|lists) into dict of (strs).\"\"\"\n",
    "    import itertools\n",
    "\n",
    "    keys, values = zip(*input_subset.items())\n",
    "    values = [joinstr.join(v) if isinstance(v, list) else (v) for v in values]\n",
    "    return dict(zip(keys, values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888fcbdb-68e5-4681-8b8c-b8747c59566a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_config(ioconf_file, ioconf_name=None, version_dict=None):\n",
    "    \"\"\"Read configuration file and extract dict that matches version['id'].\"\"\"\n",
    "    import toml\n",
    "    from mergedeep import merge\n",
    "\n",
    "    def read_config(ioconfig_file):\n",
    "        if os.path.exists(ioconfig_file):\n",
    "            with open(ioconfig_file) as f:\n",
    "                ioconf = toml.load(f)\n",
    "            return ioconf\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    # read TOML config file\n",
    "    ioconf = read_config(ioconfig_file)\n",
    "\n",
    "    if isinstance(version_dict, dict):\n",
    "        # lookup of settings\n",
    "        if ioconfig_name:\n",
    "            group = ioconfig_name\n",
    "        else:\n",
    "            group = Path(ioconfig_file).stem\n",
    "\n",
    "        if group in ioconf:\n",
    "            conf_list = [\n",
    "                d\n",
    "                for d in ioconf[group]\n",
    "                if version_dict[\"id\"].startswith(\n",
    "                    d[\"version\"][\"id\"] if \"version\" in d else \"v\"\n",
    "                )\n",
    "            ]\n",
    "            config = merge(*conf_list)\n",
    "        else:\n",
    "            config = {}\n",
    "\n",
    "        return config\n",
    "    else:\n",
    "        return ioconf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e2f326-d11c-43c7-aefd-c69808f1849e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gattrs(gattrs, data_subset):\n",
    "    ga = gattrs[0]\n",
    "    gs = {\n",
    "        \"station_id\": data_subset[\"station\"][\"station_id\"],\n",
    "        \"system_id\": data_subset[\"system\"][\"system_id\"],\n",
    "    }\n",
    "    if len(ga) > 1:\n",
    "        for g in gattrs[1:]:\n",
    "            bo = False\n",
    "            for k in gs.keys():\n",
    "                if k in g and not bo:\n",
    "                    bo = any(re.compile(m).match(gs[k]) for m in g[k])\n",
    "                    if bo:\n",
    "                        for k, v in g.items():\n",
    "                            if k in ga:\n",
    "                                ga[k] = v\n",
    "\n",
    "    gd = {\n",
    "        \"version_id\": version[\"id\"],\n",
    "        \"version_date\": version[\"time\"],\n",
    "        \"creation_time\": get_creation_time(),\n",
    "    }\n",
    "    ga = {k: \"; \".join(v) if isinstance(v, list) else v for k, v in ga.items()}\n",
    "    ga = {k: v.format(**gd) for k, v in ga.items()}\n",
    "    return ga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cbb1dc-79d9-4a7b-ae6f-aa74a6239334",
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_args(query_index):\n",
    "    metadb = metadb_query_subset_table(meta_data_subset)\n",
    "    print(\"Available:\")\n",
    "    print(\n",
    "        metadb.loc[:, (\"id\", slice(None))]\n",
    "        # .drop_duplicates([(\"id\", \"station_id\")], keep=\"first\")\n",
    "        .loc[:, \"id\"]\n",
    "        .sort_values([\"station_id\", \"sensor_id\"])\n",
    "        # .drop([\"sensor_id\"], axis=1)\n",
    "        .set_index(\"station_id\")\n",
    "        .to_markdown()\n",
    "    )\n",
    "\n",
    "    # evaluate index\n",
    "    metadb.index = pd.Index(metadb.groupby((\"id\", \"system_id\")).ngroup())\n",
    "\n",
    "    if isinstance(query_index, str):\n",
    "        query_index = (\n",
    "            metadb.reset_index()\n",
    "            .index[metadb[\"id\"][\"system_id\"] == query_index]\n",
    "            .tolist()\n",
    "        )\n",
    "        if len(query_index) > 0:\n",
    "            query_index = query_index  # [-1]\n",
    "        else:\n",
    "            print(\"`query_index` outside scope of Meta Data DB table\")\n",
    "            sys.exit()\n",
    "\n",
    "    if isinstance(query_index, int):\n",
    "        if query_index > metadb.shape[0]:\n",
    "            print(\"`query_index` outside scope of Meta Data DB table\")\n",
    "            sys.exit()\n",
    "\n",
    "    # subset\n",
    "    if query_index is not None:\n",
    "        if not isinstance(query_index,list):\n",
    "            query_index = [query_index]        \n",
    "        metadb = metadb.iloc[[query_index], :]\n",
    "\n",
    "    print(\"\\nSelection:\")\n",
    "    print(\n",
    "        metadb.loc[:, (\"id\", slice(None))]\n",
    "        # .drop_duplicates([(\"id\", \"station_id\")], keep=\"first\")\n",
    "        .loc[:, \"id\"]\n",
    "        .sort_values([\"station_id\", \"sensor_id\"])\n",
    "        # .drop([\"sensor_id\"], axis=1)\n",
    "        .set_index(\"station_id\")\n",
    "        .to_markdown()\n",
    "    )\n",
    "\n",
    "    # input stations\n",
    "    input_stations = []  # manual entries\n",
    "    input_stations.extend(\n",
    "        [\n",
    "            {\n",
    "                \"system_name\": k[(\"id\", \"system_name\")],\n",
    "                \"system_id\": k[(\"id\", \"system_id\")],\n",
    "                \"sensor_id\": k[(\"id\", \"sensor_id\")],\n",
    "            }\n",
    "            for k in metadb.loc[\n",
    "                :,\n",
    "                [\n",
    "                    (\"id\", \"system_name\"),\n",
    "                    (\"id\", \"system_id\"),\n",
    "                    (\"id\", \"sensor_id\"),\n",
    "                ],\n",
    "            ].to_dict(orient=\"records\")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return query_index, input_stations, metadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8dabc3-a739-4832-9783-83f54671a78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_files(input_subset, data_subset, debug=False):\n",
    "    input_files = [\n",
    "        {\n",
    "            \"type\": n[\"file\"][\"type\"],\n",
    "            \"path\": os.path.join(n[\"path_base\"], n[\"path\"], n[\"file\"][\"path\"]),\n",
    "            \"file\": n[\"file\"][\"file\"],\n",
    "        }\n",
    "        for n in get_dictlist_permutations(\n",
    "            dict(\n",
    "                path_base=(\n",
    "                    input_path_base\n",
    "                    if isinstance(input_path_base, list)\n",
    "                    else [input_path_base]\n",
    "                ),\n",
    "                path=(input_path if isinstance(input_path, list) else [input_path]),\n",
    "                file=input_file,\n",
    "            )\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # expand input_files\n",
    "    fn_list = [\n",
    "        {\n",
    "            \"type\": input_file[\"type\"],\n",
    "            \"file\": os.path.join(input_file[\"path\"], input_file[\"file\"]).format(\n",
    "                **{**ss, **data_subset[\"system\"]}\n",
    "            ),\n",
    "        }\n",
    "        for input_file in input_files\n",
    "        for ss in get_dictlist_permutations(input_subset)\n",
    "        if input_file[\"type\"] in query_tasks\n",
    "    ]  #\n",
    "\n",
    "    def validate_filepath(fi):\n",
    "        m_str = None\n",
    "        t_str = None\n",
    "        t_fmt = None\n",
    "        valid = False\n",
    "        \n",
    "        # check if matches dupes path patterns\n",
    "        if not m_str and not valid:\n",
    "            m_pat = \"\\/(dupes\\\\/by-upload-date)+\\/\"\n",
    "            m_str = re.findall(m_pat, fi)            \n",
    "            if m_str:   \n",
    "                m_str = m_str[0]\n",
    "                valid = True        \n",
    "        \n",
    "        # check for dates, valid date range\n",
    "        if not t_str and not valid:\n",
    "            t_pat = \".*?_(\\d{4}-\\d{2}-\\d{2})+\\.\\w{3}$\"\n",
    "            t_str = re.findall(t_pat, fi)\n",
    "            if t_str:\n",
    "                t_str = t_str[0]            \n",
    "                t_fmt = \"%Y-%m-%d\"\n",
    "        \n",
    "        if not t_str and not valid:\n",
    "            t_pat = \"\\/(\\d{4}\\/\\d{3})+\\/\"\n",
    "            t_str = re.findall(t_pat, fi)\n",
    "            if t_str:\n",
    "                t_str = t_str[0]\n",
    "                t_fmt = \"%Y/%j\"\n",
    "            \n",
    "        if t_str and t_fmt and not valid:\n",
    "            fd = pd.to_datetime(t_str, format=t_fmt)\n",
    "            if (\n",
    "                fd >= data_subset[\"time_range\"].start\n",
    "                and fd <= data_subset[\"time_range\"].stop\n",
    "            ):\n",
    "                valid = True\n",
    "        \n",
    "        return(valid)\n",
    "        \n",
    "        \n",
    "    fns_list = {}\n",
    "    for fn in fn_list:\n",
    "        fns = glob.glob(fn[\"file\"])\n",
    "        for fi in fns:\n",
    "            # try:\n",
    "            #     # retrieve timestamp from file name\n",
    "            #     t_pat = \".*?_(\\d{4}-\\d{2}-\\d{2})+\\.\\w{3}$\"\n",
    "            #     t_str = re.findall(t_pat, fi)[0]\n",
    "            #     fd = pd.to_datetime(t_str, format=\"%Y-%m-%d\")\n",
    "            # except:\n",
    "            #     # retrieve timestamp from file path\n",
    "            #     t_pat = \"\\/(\\d{4}\\/\\d{3})+\\/\"\n",
    "            #     t_str = re.findall(t_pat, fi)[0]\n",
    "            #     fd = pd.to_datetime(t_str, format=\"%Y/%j\")\n",
    "            # try:\n",
    "            #     if (\n",
    "            #         fd >= data_subset[\"time_range\"].start\n",
    "            #         and fd <= data_subset[\"time_range\"].stop\n",
    "            #     ):\n",
    "            #         if not fn[\"type\"] in fns_list:\n",
    "            #             fns_list[fn[\"type\"]] = []\n",
    "            #         fns_list[fn[\"type\"]] = sorted([fi] + fns_list[fn[\"type\"]])\n",
    "            # except:\n",
    "            #     logging.debug(\"Skipped file %s\", fn)\n",
    "            try:\n",
    "                if validate_filepath(fi):\n",
    "                    if not fn[\"type\"] in fns_list:\n",
    "                        fns_list[fn[\"type\"]] = []\n",
    "                    fns_list[fn[\"type\"]] = sorted([fi] + fns_list[fn[\"type\"]])        \n",
    "            except:\n",
    "                logging.debug(\"Skipped file %s\", fn)                    \n",
    "\n",
    "    if debug:\n",
    "        return input_files, fn_list, fns_list\n",
    "    else:\n",
    "        return fns_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d36733-7fb4-41d2-85d3-7f336a78f9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_input_files(input_subset, data_subset, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8820158-f230-4efa-8330-871206697a84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae62f56-dbae-407a-8f28-c736e2b66136",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee8aae7-0489-41bf-9d2f-1dc4a5b32658",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_toa5_header(filename, n_header=4):\n",
    "    with open(filename) as txtfile:\n",
    "        head = [\n",
    "            [x.strip('\"') for x in next(txtfile).strip().split(\",\")]\n",
    "            for x in range(n_header)\n",
    "        ]\n",
    "\n",
    "    return head\n",
    "\n",
    "\n",
    "def get_toa5_data(filename, header, n_header=4):\n",
    "    \"\"\"cs logger file\"\"\"\n",
    "    col_names = header[1]\n",
    "    data = pd.read_csv(\n",
    "        filename,\n",
    "        skiprows=n_header,\n",
    "        engine=\"python\",\n",
    "        # warn_bad_lines=False, # until pandas 1.3\n",
    "        # error_bad_lines=False, # until pandas 1.3\n",
    "        on_bad_lines=\"skip\",  # from pandas 1.3\n",
    "        names=col_names,\n",
    "        na_values=[\"NAN\"],\n",
    "    )\n",
    "\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        # dtype coercion\n",
    "        d_k = []\n",
    "        for n, k in enumerate(header[1]):\n",
    "            # time\n",
    "            if header[2][n] == \"TS\":\n",
    "                data[k] = pd.to_datetime(data[k], origin=\"unix\", errors=\"coerce\",format='ISO8601')\n",
    "                d_k.append(k)\n",
    "            # integer\n",
    "            if header[2][n] == \"RN\":\n",
    "                data[k] = pd.to_numeric(data[k], downcast=\"integer\", errors=\"coerce\")\n",
    "                d_k.append(k)\n",
    "            # numeric values\n",
    "            if header[3][n] not in [\"TS\", \"RN\"] and not k in [\n",
    "                \"SYS_CV50_Meta\"\n",
    "            ]:  # [\"Smp\", \"Tot\"]\n",
    "                data[k] = pd.to_numeric(data[k], errors=\"coerce\")\n",
    "                # d_k.append(k)\n",
    "\n",
    "        # filter for valid values on selected (critical) columns\n",
    "        data.dropna(axis=0, subset=d_k, how=\"any\", inplace=True)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_toa5_metadata_attributes(header):\n",
    "    res = OrderedDict(\n",
    "        [\n",
    "            (\"system_id\", header[0][3]),\n",
    "            (\"system_desc\", header[0]),\n",
    "            (\"channel_id\", header[0][-1]),\n",
    "            (\"cell_name\", header[1]),\n",
    "            (\"cell_units\", header[2]),\n",
    "            (\"cell_methods\", header[3]),\n",
    "            # (\"time\", pd.to_datetime(datetime(**{**d_date, **d_time}))),\n",
    "            (\n",
    "                \"data_vars_1\",\n",
    "                [\n",
    "                    dict(\n",
    "                        zip(\n",
    "                            [\"name\", \"units\", \"original_name\"],\n",
    "                            [n[0].strip(), n[1], n[0].strip()],\n",
    "                        )\n",
    "                    )\n",
    "                    for n in list(zip(header[1], header[2]))\n",
    "                ],\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_toa5_cell_methods(aggr, cell_dim=\"time\"):\n",
    "    lut = {\n",
    "        \"Max\": \"maximum\",\n",
    "        \"Min\": \"minimum\",\n",
    "        \"Avg\": \"mean\",\n",
    "        \"Std\": \"standard_deviation\",\n",
    "        \"Vec\": \"mean\",\n",
    "        \"Tot\": \"sum\",\n",
    "        \"latest\": \"last\",\n",
    "    }\n",
    "    if aggr in [\n",
    "        \"sum\",\n",
    "        \"mean\",\n",
    "        \"maximum\",\n",
    "        \"minimum\",\n",
    "        \"mid_range\",\n",
    "        \"standard_deviation\",\n",
    "        \"variance\",\n",
    "        \"mode\",\n",
    "        \"median\",\n",
    "    ]:\n",
    "        res = OrderedDict(\n",
    "            cell_methods=\"{}: {}\".format(cell_dim, aggr),\n",
    "            cell_bounds_domain={\"{}_{}\".format(cell_dim, \"bounds\"): [0, 1]},\n",
    "        )\n",
    "    elif aggr in lut.keys():\n",
    "        res = OrderedDict(\n",
    "            cell_methods=\"{}: {}\".format(cell_dim, lut[aggr]),\n",
    "            cell_bounds_domain={\"{}_{}\".format(cell_dim, \"bounds\"): [0, 1]},\n",
    "        )\n",
    "    elif aggr in [\"point\"]:\n",
    "        res = OrderedDict(\n",
    "            cell_methods=\"{}: {}\".format(cell_dim, aggr),\n",
    "            cell_bounds_domain={\"{}_{}\".format(cell_dim, \"bounds\"): [0, 0]},\n",
    "        )\n",
    "    elif aggr in [\"last\"]:\n",
    "        res = OrderedDict(\n",
    "            cell_methods=\"{}: {}\".format(cell_dim, \"point\"),\n",
    "            cell_bounds_domain={\"{}_{}\".format(cell_dim, \"bounds\"): [1, 1]},\n",
    "        )\n",
    "    else:\n",
    "        res = OrderedDict()\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_toa5_query(filename):\n",
    "    header = get_toa5_header(filename)\n",
    "    d_attr = get_toa5_metadata_attributes(header)\n",
    "\n",
    "    if (header[0][0] == \"TOA5\") and (header[1][0] == \"TIMESTAMP\"):\n",
    "        data = get_toa5_data(filename, header)\n",
    "    else:\n",
    "        data = None\n",
    "\n",
    "    return {\n",
    "        \"header\": header,\n",
    "        \"attributes\": d_attr,\n",
    "        \"data\": data,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9f12ff-6ae8-4401-8260-7b16b9668f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_toa5_datasets(\n",
    "    metadb, data, header, attributes \n",
    "):  # , responses, meta_stations, meta_locations\n",
    "    def evaluate_attributes(d):\n",
    "        \"\"\"Encoding of list and dicts to json.\"\"\"\n",
    "        import json\n",
    "\n",
    "        res = {\n",
    "            \"attributes_id\": [],\n",
    "            \"attributes_key\": [],\n",
    "            \"attributes_type\": [],\n",
    "            \"attributes_val\": [],\n",
    "        }\n",
    "        for i, j in d.items():\n",
    "            for k, v in j.items():\n",
    "                res[\"attributes_id\"].append(i)\n",
    "                res[\"attributes_key\"].append(k)\n",
    "                res[\"attributes_type\"].append(str(type(v).__name__))\n",
    "                if isinstance(v, list) or isinstance(v, dict):\n",
    "                    v = [\n",
    "                        n if (isinstance(n, float) or isinstance(n, int)) else str(n)\n",
    "                        for n in v\n",
    "                    ]\n",
    "                    v = json.dumps(v)\n",
    "                else:\n",
    "                    v = str(v)\n",
    "                    v = json.dumps(v)\n",
    "\n",
    "                res[\"attributes_val\"].append(v)\n",
    "        return res\n",
    "\n",
    "    def unexplode(df):\n",
    "        \"\"\"Compress columns into single rows\"\"\"\n",
    "        dd = []\n",
    "        for i in list(df):\n",
    "            m_list = df[i].tolist()\n",
    "            m_uniq = list(set(m_list))\n",
    "            if not isinstance(m_uniq, list):\n",
    "                m = m_list\n",
    "            elif len(m_uniq) > 1:\n",
    "                m = [m_list]\n",
    "            else:\n",
    "                m = m_uniq\n",
    "            dd.append((i, m))\n",
    "\n",
    "        res = pd.DataFrame(dict(dd))\n",
    "        return res\n",
    "\n",
    "    ds_dict = OrderedDict()\n",
    "\n",
    "    # processing identifiers\n",
    "    system_id = attributes[\"system_id\"]\n",
    "\n",
    "    \"\"\"\n",
    "    res_meta = metadb\n",
    "    res_meta = (\n",
    "        get_cslogger_metadb()\n",
    "        .set_index((\"id\", \"system_id\"))\n",
    "        .loc[[system_id]]\n",
    "        .reset_index()\n",
    "        .iloc[0]\n",
    "        .copy()\n",
    "    )\n",
    "    \"\"\"\n",
    "\n",
    "    # res_meta = metadb[\"id\"].set_index(\"system_id\")\n",
    "    # res_meta = res_meta.loc[  # get_streamline_metadb(meta_data_subset)\n",
    "    #    attributes[\"system_id\"], :\n",
    "    # ].to_dict()\n",
    "\n",
    "    # res_meta = metadb.iloc[0]\n",
    "    res_meta = unexplode(metadb).iloc[0]\n",
    "    res_refs = get_global_reference()\n",
    "    res_glob = get_global_attributes()\n",
    "\n",
    "    # attributes\n",
    "    dattr_system = OrderedDict(\n",
    "        {\n",
    "            **res_meta[\"system\"].to_dict(),\n",
    "            **{k: v for k, v in attributes.items() if not k.startswith(\"data_var\")},\n",
    "        }\n",
    "    )\n",
    "    dattr = OrderedDict(\n",
    "        [\n",
    "            (\"global\", OrderedDict(res_refs)),\n",
    "            (\"station\", OrderedDict(res_meta[\"station\"].to_dict())),\n",
    "            (\"system\", dattr_system),\n",
    "            (\"sensor\", OrderedDict(res_meta[\"sensor\"].to_dict())),\n",
    "            (\"channel\", OrderedDict()),\n",
    "            (\"cell\", OrderedDict(res_meta[\"configuration\"].to_dict())),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # flatten attributes, convert values to json\n",
    "    ddattr = evaluate_attributes(dattr)\n",
    "\n",
    "    attributes_values = (\n",
    "        \"attributes_values\".format(\"\"),\n",
    "        (\n",
    "            [\"station\", \"system\", \"sensor\", \"channel\", \"cell\", \"attributes\"],\n",
    "            [[[[[ddattr[\"attributes_val\"]] * 1]]]],\n",
    "            {\n",
    "                \"description\": \"Attributes combined from meta data and input-file headers.\"\n",
    "            },\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    def get_data_vars(k_dv, d_dv, l_dl, a_dv={}, dim_names=[\"time\"]):\n",
    "        var_name = d_dv[\"name\"].replace(\" \", \"_\")\n",
    "        var_long_name = d_dv[\"name\"]\n",
    "        var_units = d_dv[\"units\"]\n",
    "\n",
    "        # var = [dv[k_dv] for k, dv in l_dl.items()]\n",
    "        # var = pd.to_numeric(l_dl[var_name].values.tolist(),errors='coerce').tolist()\n",
    "        var = l_dl[var_name].values.tolist()\n",
    "        var_dim = 1\n",
    "\n",
    "        # stack 1-dimensions\n",
    "        for n in range(len(dim_names) - var_dim):\n",
    "            var = [var]\n",
    "\n",
    "        data_var = (\n",
    "            \"{}\".format(var_name),\n",
    "            (\n",
    "                dim_names,\n",
    "                var,\n",
    "                {**{\"long_name\": var_long_name, \"units\": var_units}, **a_dv},\n",
    "            ),\n",
    "        )\n",
    "        return data_var\n",
    "\n",
    "    coord_time = pd.to_datetime(data[\"TIMESTAMP\"])\n",
    "    coord_cell_methods = [\n",
    "        n[\"cell_methods\"] if \"cell_methods\" in n.keys() else \"\"\n",
    "        for n in [get_toa5_cell_methods(m) for m in attributes[\"cell_methods\"]]\n",
    "    ]\n",
    "\n",
    "    # return(attributes)\n",
    "    # Data variables, filters (2)\n",
    "    data_vars = []\n",
    "    for k_dv, d_dv in enumerate(attributes[\"data_vars_1\"]):\n",
    "        data_var = get_data_vars(\n",
    "            k_dv,\n",
    "            d_dv,\n",
    "            data,\n",
    "            {\"cell_methods\": coord_cell_methods[k_dv]},\n",
    "            dim_names=[\"station\", \"system\", \"channel\", \"time\"],\n",
    "        )\n",
    "        data_vars.append(data_var)\n",
    "\n",
    "    coords = [\n",
    "        (\n",
    "            \"time\",\n",
    "            (\n",
    "                [\"time\"],\n",
    "                coord_time,\n",
    "                {\n",
    "                    \"long_name\": \"time\",\n",
    "                    \"standard_name\": \"time\",\n",
    "                    # \"calendar\": \"proleptic_gregorian\",  # xarray error\n",
    "                    # \"units\": \"microseconds since 1970-01-01 00:00:00 +0000\", # xarray error\n",
    "                },\n",
    "            ),\n",
    "        ),\n",
    "        (\n",
    "            \"station_id\",\n",
    "            ([\"station\"], [res_meta[(\"id\", \"station_id\")]], OrderedDict()),\n",
    "        ),\n",
    "        (\"station_name\", ([\"station\"], [res_meta[(\"id\", \"station_name\")]], {})),\n",
    "        (\"station_lat\", ([\"station\"], [float(res_meta[(\"id\", \"station_lat\")])], {})),\n",
    "        (\"station_lon\", ([\"station\"], [float(res_meta[(\"id\", \"station_lon\")])], {})),\n",
    "        (\n",
    "            \"station_height\",\n",
    "            ([\"station\"], [float(res_meta[(\"id\", \"station_height\")])], {}),\n",
    "        ),\n",
    "        (\"system_id\", ([\"system\"], [attributes[\"system_id\"]], {})),\n",
    "        (\"system_name\", ([\"system\"], [str(res_meta[(\"id\", \"system_name\")])], {})),        \n",
    "        (\"system_group\", ([\"system\"], [str(\"AWS\")], {})),\n",
    "        (\"sensor_id\", ([\"sensor\"], [\"\"], {})),\n",
    "        (\"channel_id\", ([\"channel\"], [attributes[\"channel_id\"]], {})),\n",
    "        # (\"cell_id\", ([\"cell\"], coord_cell, {})),\n",
    "        (\"attributes_id\", ([\"attributes\"], ddattr[\"attributes_key\"], {})),\n",
    "        (\"attributes_group\", ([\"attributes\"], ddattr[\"attributes_id\"], {})),\n",
    "        (\"attributes_type\", ([\"attributes\"], ddattr[\"attributes_type\"], {})),\n",
    "    ]\n",
    "\n",
    "    # add attributes_values as coord or data_vars?\n",
    "    coords.append(attributes_values)\n",
    "\n",
    "    # Conversion to dict\n",
    "    data_vars = OrderedDict(data_vars)\n",
    "    coords = OrderedDict(coords)\n",
    "\n",
    "    # Dataset (xarray)\n",
    "    ds = xr.Dataset(\n",
    "        data_vars=data_vars,\n",
    "        coords=coords,\n",
    "        attrs=res_glob,\n",
    "    )\n",
    "\n",
    "    # Dimensions sort order (xarray)\n",
    "    ds = ds.transpose(\n",
    "        \"station\",\n",
    "        \"system\",\n",
    "        \"sensor\",\n",
    "        \"channel\",\n",
    "        \"time\",\n",
    "        \"cell\",\n",
    "        \"attributes\",\n",
    "        missing_dims=\"ignore\",\n",
    "    )\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ba0050-2688-4fad-9d47-f390afe5b028",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets_groups(\n",
    "    dx, group_keys=[\"station_id\", \"system_id\", \"channel_id\", \"attributes_id\"]\n",
    "):\n",
    "    dsi = []\n",
    "    for n, ds in enumerate(dx):\n",
    "        dsi.append(\n",
    "            OrderedDict(\n",
    "                [\n",
    "                    (\"index\", n),\n",
    "                    (\"station_id\", ds[\"station_id\"].values.tolist()),\n",
    "                    (\"system_id\", ds[\"system_id\"].values.tolist()),\n",
    "                    (\"channel_id\", ds[\"channel_id\"].values.tolist()),\n",
    "                    (\n",
    "                        \"attributes_id\",\n",
    "                        [\n",
    "                            ds[\"attributes_values\"]\n",
    "                            .sel(attributes=ds[\"attributes_id\"] == [\"system_desc\"])\n",
    "                            .squeeze()\n",
    "                            .values.tolist()\n",
    "                        ],\n",
    "                    ),\n",
    "                    (\"variables\", list(ds.keys())),\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "    dsg = {}\n",
    "    for si in dsi:\n",
    "        k = [(k, n) for k, v in si.items() if k in group_keys for n in v]\n",
    "        k = tuple(k)\n",
    "        if not k in list(dsg.keys()):\n",
    "            dsg[k] = []\n",
    "\n",
    "        dsg[k].append(si[\"index\"])\n",
    "    return (dsi, dsg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e999f997-7095-4b74-9caa-2d699fb526ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_creation_time(d=datetime.utcnow()):\n",
    "    d_pat = \"[%Y%m%dT%H%M%S+0000] %a, %d %b %Y %H:%M:%S GMT\"\n",
    "    d_str = d.strftime(d_pat)\n",
    "    return d_str\n",
    "\n",
    "\n",
    "def get_time_bounds(query_range):\n",
    "    tr = query_range.strftime(\"%Y%m%dT%H%M%S%z\").tolist()\n",
    "    res = tr[0] if tr[0] == tr[1] else \"{}_{}\".format(*tr)\n",
    "    return res\n",
    "\n",
    "\n",
    "def get_global_attributes():\n",
    "    return get_gattrs(gattrs, data_subset)\n",
    "\n",
    "\n",
    "def get_global_reference():\n",
    "    crs = OrderedDict(\n",
    "        origin_time=\"microseconds since 1970-01-01 00:00:00 +0000\",\n",
    "        origin_lon=0.0,\n",
    "        origin_lat=0.0,\n",
    "        origin_utm_x=0.0,\n",
    "        origin_utm_y=0.0,\n",
    "        origin_x=0.0,\n",
    "        origin_y=0.0,\n",
    "        origin_z=0.0,  # or \"station: h\"\n",
    "        origin_h=\"meters above mean sea level\",  # or \"meters above Normaal Amsterdams Peil\"\n",
    "        origin_azimuth=0.0,\n",
    "    )\n",
    "    return crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b46104-b809-4f5e-993d-5c484d4090fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_datasets(\n",
    "    dx,\n",
    "    output_file,\n",
    "    encoding={},\n",
    "    mode=\"w\",\n",
    "    reduced_size=False,\n",
    "    verbose=False,\n",
    "):\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "\n",
    "    if output_file:\n",
    "        fn = Path(output_file)\n",
    "\n",
    "        if isinstance(dx, xr.Dataset):\n",
    "            try:\n",
    "                fn.parent.mkdir(parents=True, exist_ok=True)\n",
    "                dx.to_netcdf(output_file, encoding=encoding, mode=mode)\n",
    "                logging.info(\"File '{}' was written.\".format(fn))\n",
    "            except:\n",
    "                logging.info(\"Warning: File '{}' was not written.\".format(fn))\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f06c355-14fe-4759-8a2e-58b1f6cdd621",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_datasets_attrgroups(dsg):\n",
    "    k = pd.DataFrame([dict(x) for x in dsg.keys()])\n",
    "    v = pd.DataFrame([dict([(\"dataset_id\", x)]) for x in dsg.values()])\n",
    "\n",
    "    df = pd.concat([k, v], axis=1)\n",
    "    df[\"attributes_id\"] = df[\"attributes_id\"].map(lambda x: json.loads(x)[0:7])\n",
    "    df = df.reset_index()\n",
    "    df.index = pd.MultiIndex.from_tuples(df[\"attributes_id\"].values.tolist())\n",
    "    df = df.sort_index()\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_merged_attrgroups(dc, dxg):\n",
    "    def dim_set(dc):\n",
    "        dd = (\n",
    "            dc.loc[:, [\"channel_id\", \"system_id\", \"station_id\"]]\n",
    "            .reset_index(drop=True)\n",
    "            .to_dict(orient=\"list\")\n",
    "        )\n",
    "        return {k: list(set(v)) for k, v in dd.items()}\n",
    "\n",
    "    def dxg_set(d, reset=False, lut={}):\n",
    "        if reset:\n",
    "            return d.reset_index(list(lut.keys())).rename_vars(lut)\n",
    "        else:\n",
    "            return d.set_index(**lut)\n",
    "\n",
    "    dim_lut = {k.split(\"_\")[0]: k for k, v in dim_set(dc).items() if len(v) > 1}\n",
    "\n",
    "    dxga = xr.combine_by_coords([dxg_set(d, lut=dim_lut) for d in dxg])\n",
    "    dxgb = dxg_set(dxga, lut=dim_lut, reset=True)\n",
    "\n",
    "    return dxgb\n",
    "\n",
    "\n",
    "def export_merged_attrgroups(dxi, nc_mode=\"w\"):\n",
    "    time_bounds = pd.Index(\n",
    "        [\n",
    "            query_range[0]\n",
    "            if np.any((dxi[\"time\"] < query_range[0]).any().values)\n",
    "            else pd.to_datetime(dxi[\"time\"].min().values,format='ISO8601'),\n",
    "            query_range[1]\n",
    "            if np.any((dxi[\"time\"] > query_range[1]).any().values)\n",
    "            else pd.to_datetime(dxi[\"time\"].max().values,format='ISO8601'),\n",
    "        ]\n",
    "    )\n",
    "    fn, fs = get_output_files(\n",
    "        dxi,\n",
    "        custom_subset={\"time_bounds\": get_time_bounds(time_bounds)},\n",
    "    )\n",
    "\n",
    "    export_datasets(\n",
    "        dxi,\n",
    "        encoding={},\n",
    "        mode=nc_mode,\n",
    "        output_file=fn,\n",
    "        reduced_size=True,\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        fn_log_src = Path(os.path.join(log_path, log_file))\n",
    "        fn_log_dst = Path(fn).with_suffix(\".log\")\n",
    "        shutil.copy(fn_log_src, fn_log_dst)\n",
    "        logging.info(\"Log file saved.\")\n",
    "        logging.info(\"File '%s' was written.\", str(fn_log_dst))\n",
    "        shutil.copy(fn_log_src, fn_log_dst)\n",
    "    except:\n",
    "        logging.info(\"Log file not saved.\")\n",
    "\n",
    "\n",
    "def get_output_files(dx, default_subset={}, custom_subset={}):\n",
    "    # dataset lookup\n",
    "    s_id = list(dx[\"station_id\"].values)\n",
    "    i_id = list(dx[\"system_id\"].values)\n",
    "\n",
    "    # modify output subset dict, based on input settings\n",
    "    output_subset = get_dictlist_flatten(input_subset)\n",
    "    output_subset[\"station_id\"] = \"\".join(s_id) if len(s_id) == 1 else \"\"\n",
    "    output_subset[\"system_id\"] = \"+\".join(i_id)\n",
    "    output_subset[\"version_id\"] = version[\"id\"]\n",
    "    output_subset[\"time_bounds\"] = \"_\".join(\n",
    "        [\n",
    "            x.strftime(\"%Y%m%dT%H%M%S\")\n",
    "            for x in [data_subset[\"time\"].start, data_subset[\"time\"].stop]\n",
    "        ]\n",
    "    )\n",
    "    output_subset = {**output_subset, **default_subset}\n",
    "    output_subset = {**output_subset, **custom_subset}\n",
    "\n",
    "    #\n",
    "    output_file = os.path.join(output_files[\"path\"], output_files[\"file\"]).format(\n",
    "        **output_subset\n",
    "    )\n",
    "    return output_file, output_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c005fb3-4307-487a-88d3-74649564b734",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main(\n",
    "    metadb_df,\n",
    "    combine_dims=[\"time\"],\n",
    "    export=True,\n",
    "    verbose=True,\n",
    "):\n",
    "    for gb_ind, metadb in metadb_df.groupby(\n",
    "        [(\"id\", \"station_id\"), (\"id\", \"system_id\")]\n",
    "    ):\n",
    "        logging.info(\"MetaDataDB `index`: %s\", gb_ind)\n",
    "        logging.info(\"MetaDataDB `station_id`: %s\", metadb[\"id\"][\"station_id\"].iloc[0])\n",
    "        logging.info(\"MetaDataDB `system_id`: %s\", metadb[\"id\"][\"system_id\"].iloc[0])\n",
    "\n",
    "        # data related helper dict:\n",
    "        global data_subset\n",
    "        data_subset = {\n",
    "            \"station\": {\"station_id\": metadb[\"id\"][\"station_id\"].iloc[0]},\n",
    "            \"system\": {\n",
    "                k: metadb[\"id\"][k].iloc[0]\n",
    "                for k in [\"system_name\", \"system_id\", \"sensor_id\"]\n",
    "            },  # metadb_systems[0],  # e.g., 0 for \"175\",\n",
    "            \"time\": slice(*query_range),\n",
    "            \"time_range\": slice(*(query_range + pd.to_timedelta([\"-1D\", \"1D\"]))),\n",
    "        }\n",
    "\n",
    "        # update ioconfig\n",
    "        ioconf[\"gattrs\"] = get_gattrs(gattrs, data_subset)\n",
    "\n",
    "        dx = []\n",
    "        dxcache = []\n",
    "        dxg = []\n",
    "        nc_mode = \"w\"\n",
    "\n",
    "        # read source file list\n",
    "        p_list = get_input_files(input_subset, data_subset)\n",
    "\n",
    "        # read files\n",
    "        if p_list:\n",
    "            for ind, fns in p_list.items():\n",
    "                for filename in tqdm(fns):\n",
    "                    filename = os.path.expanduser(filename)\n",
    "\n",
    "                    # read information from file\n",
    "                    try:\n",
    "                        logging.info(\"Read input file '%s'.\", filename)\n",
    "                        data = get_toa5_query(filename)\n",
    "                    except:\n",
    "                        logging.info(\"Warning: File '%s' could not be read.\", filename)\n",
    "                        continue\n",
    "\n",
    "                    # convert information to xarray dataset (netcdf)\n",
    "                    try:\n",
    "                        logging.info(\"Convert data to dataset.\")\n",
    "                        logging.info(\"Dataset `channel_id` '%s'.\", ind)\n",
    "                        ds = get_toa5_datasets(metadb, **data)\n",
    "                    except:\n",
    "                        logging.info(\n",
    "                            \"Warning: File '%s' could not be converted to a dataset.\",\n",
    "                            filename,\n",
    "                        )\n",
    "                        continue\n",
    "\n",
    "                    # export, for debugging\n",
    "                    if export and (None in combine_dims):\n",
    "                        export_datasets(\n",
    "                            ds,\n",
    "                            encoding={},\n",
    "                            reduced_size=True,\n",
    "                            output_file=fp,\n",
    "                            verbose=verbose,\n",
    "                        )\n",
    "\n",
    "                    dx.append(ds)\n",
    "                    logging.info(\"Dataset `dataset_id` '%s'.\", str(len(dx)))\n",
    "\n",
    "            # define cache (not implemented)\n",
    "            dxcache = dx\n",
    "\n",
    "            # return(dx)\n",
    "            # continue with groups of data, in case multiple tables were read\n",
    "            dsi, dsg = get_datasets_groups(dxcache)\n",
    "            dsc = get_datasets_attrgroups(dsg)\n",
    "\n",
    "            if \"time\" in combine_dims:\n",
    "                for ic, dc in dsc.groupby(dsc.index):\n",
    "                    logging.info(\"Attributes group: %s\", \" ; \".join(ic))\n",
    "                    idc = dc[\"dataset_id\"].values.tolist()\n",
    "                    logging.info(\"Attributes group `dataset_id`: %s\", json.dumps(idc))\n",
    "\n",
    "                    dxg = []\n",
    "                    for idx in idc:\n",
    "                        # select variables\n",
    "                        vnl = [list(dx[idx[0]].keys())]\n",
    "                        for vn in vnl:\n",
    "                            dxi = xr.concat(\n",
    "                                [\n",
    "                                    dxcache[n]\n",
    "                                    .copy()\n",
    "                                    .drop_vars(\n",
    "                                        [x for x in dxcache[n].keys() if not x in vn]\n",
    "                                    )\n",
    "                                    for n in idx\n",
    "                                ],\n",
    "                                dim=\"time\",\n",
    "                            )\n",
    "\n",
    "                            # evaluate and filter merge results (dim: time)\n",
    "                            dxi = dxi.sel(time=dxi[\"time\"].notnull())  # NaT\n",
    "                            dxi = dxi.drop_duplicates(\"time\", keep=\"first\")\n",
    "                            \n",
    "                            if dxi[\"time\"].shape[0] == 0:\n",
    "                                logging.info(\"Skipping empty datasets.\")\n",
    "                                continue                            \n",
    "                            \n",
    "                            # slice to match query, in case of dupes\n",
    "                            dxi = dxi.sortby(['time'])\n",
    "                            dxi = dxi.sel(time=slice(query_range[0],query_range[1]))\n",
    "                            \n",
    "                            if dxi[\"time\"].shape[0] == 0:\n",
    "                                logging.info(\"Skipping empty datasets.\")\n",
    "                                continue\n",
    "\n",
    "                            dxg.append(dxi.copy())\n",
    "                            logging.info(\"Add variables: %s\", \" ; \".join(vn))\n",
    "                            \n",
    "                    if dxg:\n",
    "                        # merge group (e.g, by channel)\n",
    "                        dxm = get_merged_attrgroups(dc, dxg)\n",
    "\n",
    "                        # export xarray dataset to netcdf\n",
    "                        if export:\n",
    "                            export_merged_attrgroups(dxm)\n",
    "\n",
    "            # if dxi:\n",
    "            #    logging.info(\"Returning merged dataset.\")\n",
    "            #    return dxi\n",
    "            # if dx:\n",
    "            #    return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74edb85f-719d-412b-9c61-df1e010c2e8d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Configuration\n",
    "## Static Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e497dc8a-71a1-4efb-ab9b-4ca20416b6f4",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# history\n",
    "version = {\n",
    "    \"id\": \"000_prerelease\",\n",
    "    \"time\": \"2022-07-25\",\n",
    "}  # prerelease.\n",
    "version = {\n",
    "    \"id\": \"v1.0.0\",\n",
    "    \"time\": \"2022-10-10\",\n",
    "}  # first version.\n",
    "version = {\n",
    "    \"id\": \"v1.0.1\",\n",
    "    \"time\": \"2022-10-11\",\n",
    "}  # first version.\n",
    "# - [x] update system group, CR -> AWS\n",
    "# - [x] use of TOML configuration\n",
    "\n",
    "# Configuration file for input / output files\n",
    "ioconfig_name = \"cslogger_metadata\"\n",
    "try:\n",
    "    import ipynbname\n",
    "\n",
    "    ioconfig_file = \"{}.toml\".format(ipynbname.name())\n",
    "except:\n",
    "    ioconfig_file = \"{}.toml\".format(ioconfig_name)\n",
    "\n",
    "# ----- Papermill injection below this cell -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8469e0-9d8f-4d86-8c58-2c9daa888478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input/output config\n",
    "ioconf = parse_config(ioconfig_file, ioconfig_name, version)\n",
    "\n",
    "# validate config (to do)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a5c8bf-734e-41a8-a5bc-59b892ce8dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ioconf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc3a77d-b732-408a-a854-b9f2d770bc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Note: the approach to set global helper variables should be revised. \n",
    "But was/is used in combination with papermill automation.\n",
    "\"\"\"\n",
    "\n",
    "# set global variables\n",
    "query_from = ioconf[\"query\"][\"start\"]\n",
    "query_to = None\n",
    "query_period = ioconf[\"query\"][\"period\"]\n",
    "query_index = (\n",
    "    ioconf[\"query\"][\"system_index\"] if ioconf[\"query\"][\"system_index\"] != \"\" else None\n",
    ")\n",
    "query_city = ioconf[\"query\"][\"city\"]\n",
    "query_tasks = ioconf[\"query\"][\"tasks\"]\n",
    "query_cache = ioconf[\"query\"][\"cache\"]\n",
    "input_subset = ioconf[\"input\"][\"subset\"]\n",
    "input_path_base = ioconf[\"input\"][\"path_base\"]\n",
    "input_path = ioconf[\"input\"][\"path\"]\n",
    "input_file = ioconf[\"input\"][\"file\"]\n",
    "output_path_base = ioconf[\"output\"][\"path_base\"]\n",
    "output_path = ioconf[\"output\"][\"path\"]\n",
    "output_file = ioconf[\"output\"][\"file\"]\n",
    "\n",
    "log_path = ioconf[\"logging\"][\"path\"]\n",
    "log_file = ioconf[\"logging\"][\"file\"]\n",
    "log_format = ioconf[\"logging\"][\"format\"]\n",
    "\n",
    "gattrs = ioconf[\"gattrs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9756f7-e4cc-4f1e-be60-1240fedc24c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1327e7c-bbda-4515-8773-61091503be45",
   "metadata": {},
   "source": [
    "## Logging Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343ef108-9296-4517-90f0-6f9a113e4711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create logger\n",
    "import logging\n",
    "import logging.handlers\n",
    "from pprint import pformat\n",
    "\n",
    "logging.basicConfig(\n",
    "    # encoding=\"utf-8\",\n",
    "    format=log_format,\n",
    "    level=logging.INFO,\n",
    "    # Declare handlers\n",
    "    handlers=[\n",
    "        logging.FileHandler(\n",
    "            os.path.join(log_path, log_file), \"w\"\n",
    "        ),  # overwrite, instad of append 'w+'\n",
    "        logging.StreamHandler(sys.stdout),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbc8c26-0c65-4f21-b9ed-10f242cd615c",
   "metadata": {},
   "source": [
    "## Dynamic Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b8d473-4424-4901-ab85-a1e6ee37bff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# time related helper variables\n",
    "if not query_to and query_period:\n",
    "    if query_period.endswith(\"M\"):\n",
    "        query_to = pd.to_datetime(query_from,format='ISO8601') + pd.DateOffset(\n",
    "            months=int(query_period[:-1])\n",
    "        )\n",
    "    else:\n",
    "        query_to = pd.to_datetime(query_from,format='ISO8601') + pd.to_timedelta(query_period)\n",
    "    query_to = query_to.strftime(\"%Y-%m-%d\")\n",
    "query_range = pd.to_datetime([query_from, query_to],format='ISO8601')\n",
    "\n",
    "# metadb (online)\n",
    "meta_data_subset = {\n",
    "    \"station\": {\"y_code\": [query_city]},\n",
    "    \"system\": {\n",
    "        \"i_description\": [\"CR logger\", \"Data logger\"],\n",
    "    },\n",
    "    \"time\": {\"start\": query_from, \"end\": query_to},\n",
    "}\n",
    "query_index, metadb_systems, metadb = input_args(query_index)\n",
    "\n",
    "# output files\n",
    "output_files = {\n",
    "    \"path\": os.path.join(\n",
    "        output_path_base,\n",
    "        output_path,\n",
    "    ),\n",
    "    \"file\": output_file,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04931879-b6f9-461e-92f0-c488c10edfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize\n",
    "logging.info(\"`ioconf` file: %s\", ioconfig_file)\n",
    "logging.info(\n",
    "    \"`ioconf` dict:\\n# start of item\\n%s\\n# end of item\\n\",\n",
    "    pformat(ioconf, sort_dicts=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e817013a-fc66-48bf-9074-6c1fec532f45",
   "metadata": {},
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac851766-18e0-4f9d-8ee9-41cde1ebd1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dx = main(metadb, export=True, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f3d5c2-aaf3-4dd3-936b-9c22d4c669c3",
   "metadata": {},
   "source": [
    "# DEV snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5744e7-552a-4dce-93f3-4cfbe3375893",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # quick evaluation of TOA5 files\n",
    "    def read_cs_toa5(input_stations, input_files):\n",
    "        \"\"\"Read Campbell Scientific TOA5 data table files.\"\"\"\n",
    "        h_dict = {}\n",
    "        d_dict = {}\n",
    "        d_list = {}\n",
    "        for station in input_stations:\n",
    "            s_id = station[\"system_id\"]\n",
    "            h_dict[s_id] = {}\n",
    "            d_dict[s_id] = {}\n",
    "            d_list[s_id] = {}\n",
    "\n",
    "            # read tables\n",
    "            for filegroup in input_files:\n",
    "                # local definitions\n",
    "                fp = filegroup[\"file_path\"].format(**station)\n",
    "                fn = filegroup[\"file_name\"].format(**station)\n",
    "                ft = filegroup[\"file_type\"].format(**station)\n",
    "\n",
    "                # list files\n",
    "                fi_list = sorted(glob.glob(os.path.join(fp, fn)))\n",
    "                if len(fi_list) == 0:\n",
    "                    continue\n",
    "\n",
    "                # read TOA5 files\n",
    "                h_dict[s_id][ft] = []  # initialize list, header info, for DEBUG only\n",
    "                d_dict[s_id][ft] = []  # initialize list, data table\n",
    "                for fi in tqdm(fi_list):\n",
    "                    # retrieve timestamp from the filename\n",
    "                    t_pat = \".*?_(\\d{4}-\\d{2}-\\d{2})+\\.csv$\"\n",
    "                    t_str = re.findall(t_pat, fi)\n",
    "                    fd = pd.to_datetime(t_str, format=\"%Y-%m-%d\")\n",
    "\n",
    "                    # MZ add: if needed, filter input files by timestamp here\n",
    "\n",
    "                    # read TOA5 header information\n",
    "                    h_df = pd.read_csv(fi, skiprows=1, nrows=2)\n",
    "\n",
    "                    # skip files without header information\n",
    "                    if not h_df.columns[0] in [\"TIMESTAMP\"]:\n",
    "                        continue\n",
    "\n",
    "                    d_df = pd.read_csv(\n",
    "                        fi,\n",
    "                        skiprows=4,\n",
    "                        engine=\"python\",\n",
    "                        # warn_bad_lines=False, # until pandas 1.3\n",
    "                        # error_bad_lines=False, # until pandas 1.3\n",
    "                        on_bad_lines=\"skip\",  # from pandas 1.3\n",
    "                        names=h_df.columns.to_list(),\n",
    "                        na_values=[\"NAN\"],\n",
    "                    )\n",
    "\n",
    "                    # MZ add: dtype coercion here\n",
    "                    d_k = []\n",
    "                    for k in h_df.columns:\n",
    "                        # time\n",
    "                        if h_df.loc[0, k] == \"TS\":\n",
    "                            d_df.loc[:, k] = pd.to_datetime(\n",
    "                                d_df.loc[:, k], origin=\"unix\", errors=\"coerce\", format='ISO8601'\n",
    "                            )\n",
    "                            d_k.append(k)\n",
    "                        # integer\n",
    "                        if h_df.loc[0, k] == \"RN\":\n",
    "                            d_df.loc[:, k] = pd.to_numeric(\n",
    "                                d_df.loc[:, k], downcast=\"integer\", errors=\"coerce\", \n",
    "                            )\n",
    "                            d_k.append(k)\n",
    "                        # numeric values\n",
    "                        if h_df.loc[1, k] in [\"Smp\", \"Tot\"] and not k in [\n",
    "                            \"SYS_CV50_Meta\"\n",
    "                        ]:\n",
    "                            d_df.loc[:, k] = pd.to_numeric(\n",
    "                                d_df.loc[:, k], errors=\"coerce\"\n",
    "                            )\n",
    "                            # dk.append(k)\n",
    "\n",
    "                    # filter for valid values on selected (critical) columns\n",
    "                    d_df.dropna(axis=0, subset=d_k, how=\"any\", inplace=True)\n",
    "\n",
    "                    if isinstance(d_df, pd.DataFrame):\n",
    "                        # append data tables to the list\n",
    "                        h_dict[s_id][ft].append(h_df)\n",
    "                        d_dict[s_id][ft].append(d_df)\n",
    "\n",
    "            # aggregate tables, per file type\n",
    "            for ft, df_list in list(d_dict[s_id].items()):\n",
    "                d_list[s_id][ft] = pd.concat(df_list, axis=0).rename(\n",
    "                    columns={\"RECORD\": f\"{ft}_RECORD\"}\n",
    "                )\n",
    "\n",
    "        return (d_list, h_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
